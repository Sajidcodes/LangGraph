{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22417295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "351ef8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389\n",
      "page_content='Chip Huyen \n",
      "Designing  \n",
      "Machine Learning  \n",
      "Systems\n",
      "An Iterative Process  \n",
      "for Production-Ready  \n",
      "Applications' metadata={'producer': 'Antenna House PDF Output Library 7.1.1639', 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)', 'creationdate': '2022-05-17T13:53:06+00:00', 'author': 'Huyen, Chip;', 'moddate': '2022-05-17T10:40:04-04:00', 'title': 'Designing Machine Learning Systems', 'ebx_publisher': \"O'Reilly Media, Incorporated\", 'source': '/Users/sajidhussain/Desktop/Agentic AI/rag/Designing MLS.pdf', 'total_pages': 389, 'page': 0, 'page_label': 'Cover'}\n"
     ]
    }
   ],
   "source": [
    "# loading text\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# returns a document object\n",
    "loader = PyPDFLoader('/Users/sajidhussain/Desktop/Agentic AI/rag/Designing MLS.pdf')\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c0922fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Patterns are different from distributions. We know the distribution of the outcomes of a fair die, but there are\n",
      "no patterns in the way the outcomes are generated.\n",
      "5 Andrej Karpathy, “Software 2.0, ” Medium, November 11, 2017, https://oreil.ly/yHZrE.\n",
      "2. Complex patterns: there are patterns to learn, and they are complex\n",
      "ML solutions are only useful when there are patterns to learn. Sane people\n",
      "don’t invest money into building an ML system to predict the next outcome\n",
      "of a fair die because there’s no pattern in how these outcomes are generated. 4\n",
      "However, there are patterns in how stocks are priced, and therefore companies\n",
      "have invested billions of dollars in building ML systems to learn those patterns.\n",
      "Whether a pattern exists might not be obvious, or if patterns exist, your dataset\n",
      "or ML algorithms might not be sufficient to capture them. For example, there\n",
      "might be a pattern in how Elon Musk’s tweets affect cryptocurrency prices. How‐\n",
      "ever, you wouldn’t know until you’ve rigorously trained and evaluated your ML\n",
      "models on his tweets. Even if all your models fail to make reasonable predictions\n",
      "of cryptocurrency prices, it doesn’t mean there’s no pattern.\n",
      "Consider a website like Airbnb with a lot of house listings; each listing comes\n",
      "with a zip code. If you want to sort listings into the states they are located in,\n",
      "you wouldn’t need an ML system. Since the pattern is simple—each zip code\n",
      "corresponds to a known state—you can just use a lookup table.\n",
      "The relationship between a rental price and all its characteristics follows a much\n",
      "more complex pattern, which would be very challenging to manually specify. ML\n",
      "is a good solution for this. Instead of telling your system how to calculate the\n",
      "price from a list of characteristics, you can provide prices and characteristics, and\n",
      "let your ML system figure out the pattern. The difference between ML solutions\n",
      "and the lookup table solution as well as general traditional software solutions is\n",
      "shown in Figure 1-2. For this reason, ML is also called Software 2.0.5\n",
      "ML has been very successful with tasks with complex patterns such as object\n",
      "detection and speech recognition. What is complex to machines is different from\n",
      "what is complex to humans. Many tasks that are hard for humans to do are\n",
      "easy for machines—for example, raising a number of the power of 10. On the\n",
      "other hand, many tasks that are easy for humans can be hard for machines—for\n",
      "example, deciding whether there’s a cat in a picture.\n",
      "4 | Chapter 1: Overview of Machine Learning Systems\n"
     ]
    }
   ],
   "source": [
    "print(docs[23].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0261127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Huyen, Chip;',\n",
      " 'creationdate': '2022-05-17T13:53:06+00:00',\n",
      " 'creator': 'AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 '\n",
      "            '(2021-04-26T09:47+09)',\n",
      " 'ebx_publisher': \"O'Reilly Media, Incorporated\",\n",
      " 'moddate': '2022-05-17T10:40:04-04:00',\n",
      " 'page': 12,\n",
      " 'page_label': 'ix',\n",
      " 'producer': 'Antenna House PDF Output Library 7.1.1639',\n",
      " 'source': '/Users/sajidhussain/Desktop/Agentic AI/rag/Designing MLS.pdf',\n",
      " 'title': 'Designing Machine Learning Systems',\n",
      " 'total_pages': 389}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(docs[12].metadata) # key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "921e3132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"producer\": \"Antenna House PDF Output Library 7.1.1639\",\n",
      "    \"creator\": \"AH CSS Formatter V7.1 MR2 for Linux64 : 7.1.3.50324 (2021-04-26T09:47+09)\",\n",
      "    \"creationdate\": \"2022-05-17T13:53:06+00:00\",\n",
      "    \"author\": \"Huyen, Chip;\",\n",
      "    \"moddate\": \"2022-05-17T10:40:04-04:00\",\n",
      "    \"title\": \"Designing Machine Learning Systems\",\n",
      "    \"ebx_publisher\": \"O'Reilly Media, Incorporated\",\n",
      "    \"source\": \"/Users/sajidhussain/Desktop/Agentic AI/rag/Designing MLS.pdf\",\n",
      "    \"total_pages\": 389,\n",
      "    \"page\": 12,\n",
      "    \"page_label\": \"ix\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Converts the dictionary to a JSON string\\nPreserves the insertion order exactly as the dictionary was created.\\n\\nKeys are printed in the order they exist in the dictionary, not “sorted” or rearranged.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print(json.dumps(docs[12].metadata, indent=4))\n",
    "\"\"\"Converts the dictionary to a JSON string\n",
    "Preserves the insertion order exactly as the dictionary was created.\n",
    "\n",
    "Keys are printed in the order they exist in the dictionary, not “sorted” or rearranged.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e094c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
      "\n",
      "\n",
      "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n"
     ]
    }
   ],
   "source": [
    "# text splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "sample = \"\"\"\n",
    "Farmers were working hard in the fields, preparing the soil and planting seeds for the next season. The sun was bright, and the air smelled of earth and fresh grass. The Indian Premier League (IPL) is the biggest cricket league in the world. People all over the world watch the matches and cheer for their favourite teams.\n",
    "\n",
    "\n",
    "Terrorism is a big danger to peace and safety. It causes harm to people and creates fear in cities and villages. When such attacks happen, they leave behind pain and sadness. To fight terrorism, we need strong laws, alert security forces, and support from people who care about peace and safety.\n",
    "\"\"\"\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN,\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=300,\n",
    ")\n",
    "\n",
    "# Perform the split\n",
    "chunkss = splitter.split_text(sample)\n",
    "\n",
    "print(len(chunkss))\n",
    "for chunk in chunkss:\n",
    "    print(chunk)\n",
    "# print(chunkss[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b3f42a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chip Huyen \\nDesigning  \\nMachine Learning  \\nSystems\\nAn Iterative Process  \\nfor Production-Ready  \\nApplications', 'MACHINE LEARNING\\n“This is, simply, the \\nvery best book you \\ncan read about how \\nto build, deploy, and \\nscale machine learning \\nmodels at a company for \\nmaximum impact.”\\n—Josh Wills\\nSoftware Engineer at WeaveGrid and \\nformer Director of Data Engineering, Slack\\n“In a blooming but chaotic \\necosystem, this principled \\nview on end-to-end ML is \\nboth your map and your \\ncompass: a must-read for \\npractitioners inside and \\noutside of Big Tech.”  \\n—Jacopo Tagliabue\\nDirector of AI, Coveo\\nDesigning Machine Learning Systems\\nUS $59.99  CAN $74.99\\nISBN: 978-1-098-10796-3\\nTwitter: @oreillymedia\\nlinkedin.com/company/oreilly-media\\nyoutube.com/oreillymedia \\nMachine learning systems are both complex and unique. \\nComplex because they consist of many different components \\nand involve many different stakeholders. Unique because \\nthey’re data dependent, with data varying wildly from one \\nuse case to the next. In this book, you’ll learn a holistic \\napproach to designing ML systems that are reliable, scalable, \\nmaintainable, and adaptive to changing environments and \\nbusiness requirements.\\nAuthor Chip Huyen, co-founder of Claypot AI, considers each \\ndesign decision—such as how to process and create training \\ndata, which features to use, how often to retrain models, \\nand what to monitor—in the context of how it can help \\nyour system as a whole achieve its objectives. The iterative \\nframework in this book uses actual case studies backed by \\nample references.\\nThis book will help you tackle scenarios such as:\\n• Engineering data and choosing the right metrics to solve a \\nbusiness problem\\n• Automating the process for continually developing, \\nevaluating, deploying, and updating models\\n• Developing a monitoring system to quickly detect and \\naddress issues your models might encounter in production\\n• Architecting an ML platform that serves across use cases\\n• Developing responsible ML systems\\nChip Huyen is co-founder of Claypot AI,  \\na platform for real-time machine learning. \\nThrough her work at NVIDIA, Netflix, \\nand Snorkel AI, she’s helped some of the \\nworld’s largest organizations develop and \\ndeploy ML systems. Chip based this book \\non her lectures for CS 329S: Machine \\nLearning Systems Design, a course she \\nteaches at Stanford University.', 'Praise for Designing Machine Learning Systems\\nThere is so much information one needs to know to be an effective machine learning\\nengineer. It’s hard to cut through the chaff to get the most relevant information, but Chip\\nhas done that admirably with this book. If you are serious about ML in production, and\\ncare about how to design and implement ML systems end to end, this book is essential.\\n—Laurence Moroney, AI and ML Lead, Google\\nOne of the best resources that focuses on the first principles behind designing ML\\nsystems for production. A must-read to navigate the ephemeral landscape of tooling\\nand platform options.\\n—Goku Mohandas, Founder of Made With ML\\nChip’s manual is the book we deserve and the one we need right now. In a blooming but\\nchaotic ecosystem, this principled view on end-to-end ML is both your map and your\\ncompass: a must-read for practitioners inside and outside of Big Tech—especially those\\nworking at “reasonable scale. ” This book will also appeal to data leaders looking for best\\npractices on how to deploy, manage, and monitor systems in the wild.\\n—Jacopo Tagliabue, Director of AI, Coveo;\\nAdj. Professor of MLSys, NYU\\nThis is, simply, the very best book you can read about how to build, deploy, and scale\\nmachine learning models at a company for maximum impact. Chip is a masterful teacher,\\nand the breadth and depth of her knowledge is unparalleled.\\n—Josh Wills, Software Engineer at WeaveGrid and former\\nDirector of Data Engineering, Slack\\nThis is the book I wish I had read when I started as an ML engineer.\\n—Shreya Shankar, MLOps PhD Student', 'Designing Machine Learning Systems is a welcome addition to the field of applied machine\\nlearning. The book provides a detailed guide for people building end-to-end machine\\nlearning systems. Chip Huyen writes from her extensive, hands-on experience building\\nreal-world machine learning applications.\\n—Brian Spiering, Data Science Instructor at Metis\\nChip is truly a world-class expert on machine learning systems, as well as a brilliant\\nwriter. Both are evident in this book, which is a fantastic resource for anyone looking to\\nlearn about this topic.\\n—Andrey Kurenkov, PhD Candidate at the Stanford AI Lab\\nChip Huyen has produced an important addition to the canon of machine learning\\nliterature—one that is deeply literate in ML fundamentals, but has a much more\\nconcrete and practical approach than most. The focus on business requirements alone\\nis uncommon and valuable. This book will resonate with engineers getting started with\\nML and with others in any part of the organization trying to understand how ML works.\\n—Todd Underwood, Senior Engineering Director for ML SRE,\\nGoogle, and Coauthor of Reliable Machine Learning', 'Chip Huyen\\nDesigning Machine\\nLearning Systems\\nAn Iterative Process for\\nProduction-Ready Applications\\nBoston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing', '978-1-098-10796-3\\n[LSI]\\nDesigning Machine Learning Systems\\nby Chip Huyen\\nCopyright © 2022 Huyen Thi Khanh Nguyen. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\\nalso available for most titles (http://oreilly.com). For more information, contact our corporate/institutional\\nsales department: 800-998-9938 or corporate@oreilly.com.\\nAcquisitions Editor: Nicole Butterfield\\nDevelopment Editor: Jill Leonard\\nProduction Editor: Gregory Hyman\\nCopyeditor: nSight, Inc.\\nProofreader: Piper Editorial Consulting, LLC\\nIndexer: nSight, Inc.\\nInterior Designer: David Futato\\nCover Designer: Karen Montgomery\\nIllustrator: Kate Dullea\\nMay 2022:  First Edition\\nRevision History for the First Edition\\n2022-05-17: First Release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098107963 for release details.\\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Designing Machine Learning Systems,\\nthe cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\\nThe views expressed in this work are those of the author, and do not represent the publisher’s views.\\nWhile the publisher and the author have used good faith efforts to ensure that the information and\\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\\nof or reliance on this work. Use of the information and instructions contained in this work is at your\\nown risk. If any code samples or other technology this work contains or describes is subject to open\\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\\nthereof complies with such licenses and/or rights.', 'Table of Contents\\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ix\\n1. Overview of Machine Learning Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhen to Use Machine Learning                                                                                      3\\nMachine Learning Use Cases                                                                                        9\\nUnderstanding Machine Learning Systems                                                                 12\\nMachine Learning in Research Versus in Production                                            12\\nMachine Learning Systems Versus Traditional Software                                       22\\nSummary                                                                                                                           23\\n2. Introduction to Machine Learning Systems Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\\nBusiness and ML Objectives                                                                                           26\\nRequirements for ML Systems                                                                                       29\\nReliability                                                                                                                       29\\nScalability                                                                                                                       30\\nMaintainability                                                                                                             31\\nAdaptability                                                                                                                   31\\nIterative Process                                                                                                               32\\nFraming ML Problems                                                                                                    35\\nTypes of ML Tasks                                                                                                        36\\nObjective Functions                                                                                                     40\\nMind Versus Data                                                                                                            43', 'Objective Functions                                                                                                     40\\nMind Versus Data                                                                                                            43\\nSummary                                                                                                                           46\\n3. Data Engineering Fundamentals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  49\\nData Sources                                                                                                                     50\\nData Formats                                                                                                                    53\\nJSON                                                                                                                               54\\niii', 'Row-Major Versus Column-Major Format                                                              54\\nText Versus Binary Format                                                                                         57\\nData Models                                                                                                                      58\\nRelational Model                                                                                                          59\\nNoSQL                                                                                                                           63\\nStructured Versus Unstructured Data                                                                       66\\nData Storage Engines and Processing                                                                           67\\nTransactional and Analytical Processing                                                                  67\\nETL: Extract, Transform, and Load                                                                           70\\nModes of Dataflow                                                                                                           72\\nData Passing Through Databases                                                                               72\\nData Passing Through Services                                                                                  73\\nData Passing Through Real-Time Transport                                                           74\\nBatch Processing Versus Stream Processing                                                                78\\nSummary                                                                                                                           79\\n4. Training Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  81\\nSampling                                                                                                                            82\\nNonprobability Sampling                                                                                            83\\nSimple Random Sampling                                                                                           84\\nStratified Sampling                                                                                                       84', 'Simple Random Sampling                                                                                           84\\nStratified Sampling                                                                                                       84\\nWeighted Sampling                                                                                                      85\\nReservoir Sampling                                                                                                      86\\nImportance Sampling                                                                                                  87\\nLabeling                                                                                                                             88\\nHand Labels                                                                                                                   88\\nNatural Labels                                                                                                               91\\nHandling the Lack of Labels                                                                                       94\\nClass Imbalance                                                                                                             102\\nChallenges of Class Imbalance                                                                                 103\\nHandling Class Imbalance                                                                                        105\\nData Augmentation                                                                                                       113\\nSimple Label-Preserving Transformations                                                             114\\nPerturbation                                                                                                                114\\nData Synthesis                                                                                                             116\\nSummary                                                                                                                         118\\n5. Feature Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  119\\nLearned Features Versus Engineered Features                                                          120\\nCommon Feature Engineering Operations                                                                123', 'Learned Features Versus Engineered Features                                                          120\\nCommon Feature Engineering Operations                                                                123\\nHandling Missing Values                                                                                          123\\nScaling                                                                                                                          126\\niv | Table of Contents', 'Discretization                                                                                                              128\\nEncoding Categorical Features                                                                                 129\\nFeature Crossing                                                                                                         132\\nDiscrete and Continuous Positional Embeddings                                                 133\\nData Leakage                                                                                                                   135\\nCommon Causes for Data Leakage                                                                         137\\nDetecting Data Leakage                                                                                             140\\nEngineering Good Features                                                                                          141\\nFeature Importance                                                                                                    142\\nFeature Generalization                                                                                              144\\nSummary                                                                                                                         146\\n6. Model Development and Offline Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  149\\nModel Development and Training                                                                              150\\nEvaluating ML Models                                                                                               150\\nEnsembles                                                                                                                    156\\nExperiment Tracking and Versioning                                                                     162\\nDistributed Training                                                                                                  168\\nAutoML                                                                                                                       172\\nModel Offline Evaluation                                                                                             178\\nBaselines                                                                                                                      179', 'Model Offline Evaluation                                                                                             178\\nBaselines                                                                                                                      179\\nEvaluation Methods                                                                                                   181\\nSummary                                                                                                                         188\\n7. Model Deployment and Prediction Service. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\\nMachine Learning Deployment Myths                                                                       194\\nMyth 1: Y ou Only Deploy One or Two ML Models at a Time                             194\\nMyth 2: If We Don’t Do Anything, Model Performance Remains the Same     195\\nMyth 3: Y ou Won’t Need to Update Y our Models as Much                                  196\\nMyth 4: Most ML Engineers Don’t Need to Worry About Scale                         196\\nBatch Prediction Versus Online Prediction                                                               197\\nFrom Batch Prediction to Online Prediction                                                         201\\nUnifying Batch Pipeline and Streaming Pipeline                                                  203\\nModel Compression                                                                                                      206\\nLow-Rank Factorization                                                                                            206\\nKnowledge Distillation                                                                                              208\\nPruning                                                                                                                        208\\nQuantization                                                                                                               209\\nML on the Cloud and on the Edge                                                                              212\\nCompiling and Optimizing Models for Edge Devices                                          214\\nML in Browsers                                                                                                          222\\nSummary                                                                                                                         223\\nTable of Contents | v', '8. Data Distribution Shifts and Monitoring. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  225\\nCauses of ML System Failures                                                                                      226\\nSoftware System Failures                                                                                           227\\nML-Specific Failures                                                                                                  229\\nData Distribution Shifts                                                                                                237\\nTypes of Data Distribution Shifts                                                                             237\\nGeneral Data Distribution Shifts                                                                             241\\nDetecting Data Distribution Shifts                                                                          242\\nAddressing Data Distribution Shifts                                                                       248\\nMonitoring and Observability                                                                                     250\\nML-Specific Metrics                                                                                                   251\\nMonitoring Toolbox                                                                                                   256\\nObservability                                                                                                               259\\nSummary                                                                                                                         261\\n9. Continual Learning and Test in Production. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  263\\nContinual Learning                                                                                                        264\\nStateless Retraining Versus Stateful Training                                                         265\\nWhy Continual Learning?                                                                                         268\\nContinual Learning Challenges                                                                                270\\nFour Stages of Continual Learning                                                                          274\\nHow Often to Update Y our Models                                                                         279', 'Four Stages of Continual Learning                                                                          274\\nHow Often to Update Y our Models                                                                         279\\nTest in Production                                                                                                         281\\nShadow Deployment                                                                                                  282\\nA/B Testing                                                                                                                  283\\nCanary Release                                                                                                            285\\nInterleaving Experiments                                                                                          285\\nBandits                                                                                                                         287\\nSummary                                                                                                                         291\\n10. Infrastructure and Tooling for MLOps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  293\\nStorage and Compute                                                                                                    297\\nPublic Cloud Versus Private Data Centers                                                             300\\nDevelopment Environment                                                                                          302\\nDev Environment Setup                                                                                            303\\nStandardizing Dev Environments                                                                            306\\nFrom Dev to Prod: Containers                                                                                 308\\nResource Management                                                                                                  311\\nCron, Schedulers, and Orchestrators                                                                      311\\nData Science Workflow Management                                                                     314\\nML Platform                                                                                                                   319\\nModel Deployment                                                                                                    320', 'ML Platform                                                                                                                   319\\nModel Deployment                                                                                                    320\\nvi | Table of Contents', 'Model Store                                                                                                                 321\\nFeature Store                                                                                                               325\\nBuild Versus Buy                                                                                                            327\\nSummary                                                                                                                         329\\n11. The Human Side of Machine Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  331\\nUser Experience                                                                                                             331\\nEnsuring User Experience Consistency                                                                  332\\nCombatting “Mostly Correct” Predictions                                                             332\\nSmooth Failing                                                                                                            334\\nTeam Structure                                                                                                               334\\nCross-functional Teams Collaboration                                                                   335\\nEnd-to-End Data Scientists                                                                                       335\\nResponsible AI                                                                                                               339\\nIrresponsible AI: Case Studies                                                                                  341\\nA Framework for Responsible AI                                                                            347\\nSummary                                                                                                                         353\\nEpilogue. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  355\\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\\nTable of Contents | vii', 'Preface\\nEver since the first machine learning course I taught at Stanford in 2017, many people\\nhave asked me for advice on how to deploy ML models at their organizations. These\\nquestions can be generic, such as “What model should I use?” “How often should I\\nretrain my model?” “How can I detect data distribution shifts?” “How do I ensure\\nthat the features used during training are consistent with the features used during\\ninference?”\\nThese questions can also be specific, such as “I’m convinced that switching from\\nbatch prediction to online prediction will give our model a performance boost, but\\nhow do I convince my manager to let me do so?” or “I’m the most senior data\\nscientist at my company and I’ve recently been tasked with setting up our first\\nmachine learning platform; where do I start?”\\nMy short answer to all these questions is always: “It depends. ” My long answers often\\ninvolve hours of discussion to understand where the questioner comes from, what\\nthey’re actually trying to achieve, and the pros and cons of different approaches for\\ntheir specific use case.\\nML systems are both complex and unique. They are complex because they consist of\\nmany different components (ML algorithms, data, business logic, evaluation metrics,\\nunderlying infrastructure, etc.) and involve many different stakeholders (data scien‐\\ntists, ML engineers, business leaders, users, even society at large). ML systems are\\nunique because they are data dependent, and data varies wildly from one use case to\\nthe next.\\nFor example, two companies might be in the same domain (ecommerce) and have the\\nsame problem that they want ML to solve (recommender system), but their resulting\\nML systems can have different model architecture, use different sets of features, be\\nevaluated on different metrics, and bring different returns on investment.\\nix', 'Many blog posts and tutorials on ML production focus on answering one specific\\nquestion. While the focus helps get the point across, they can create the impression\\nthat it’s possible to consider each of these questions in isolation. In reality, changes\\nin one component will likely affect other components. Therefore, it’s necessary to\\nconsider the system as a whole while attempting to make any design decision.\\nThis book takes a holistic approach to ML systems. It takes into account different\\ncomponents of the system and the objectives of different stakeholders involved. The\\ncontent in this book is illustrated using actual case studies, many of which I’ve\\npersonally worked on, backed by ample references, and reviewed by ML practitioners\\nin both academia and industry. Sections that require in-depth knowledge of a certain\\ntopic—e.g., batch processing versus stream processing, infrastructure for storage and\\ncompute, and responsible AI—are further reviewed by experts whose work focuses\\non that one topic. In other words, this book is an attempt to give nuanced answers to\\nthe aforementioned questions and more.\\nWhen I first wrote the lecture notes that laid the foundation for this book, I thought\\nI wrote them for my students to prepare them for the demands of their future jobs\\nas data scientists and ML engineers. However, I soon realized that I also learned\\ntremendously through the process. The initial drafts I shared with early readers\\nsparked many conversations that tested my assumptions, forced me to consider\\ndifferent perspectives, and introduced me to new problems and new approaches.\\nI hope that this learning process will continue for me now that the book is in your\\nhand, as you have experiences and perspectives that are unique to you. Please feel free\\nto share with me any feedback you might have for this book, via the MLOps Discord\\nserver that I run (where you can also find other readers of this book), Twitter,\\nLinkedIn, or other channels that you can find on my website.\\nWho This Book Is For\\nThis book is for anyone who wants to leverage ML to solve real-world problems.\\nML in this book refers to both deep learning and classical algorithms, with a leaning\\ntoward ML systems at scale, such as those seen at medium to large enterprises and\\nfast-growing startups. Systems at a smaller scale tend to be less complex and might\\nbenefit less from the comprehensive approach laid out in this book.', 'toward ML systems at scale, such as those seen at medium to large enterprises and\\nfast-growing startups. Systems at a smaller scale tend to be less complex and might\\nbenefit less from the comprehensive approach laid out in this book.\\nBecause my background is engineering, the language of this book is geared toward\\nengineers, including ML engineers, data scientists, data engineers, ML platform engi‐\\nneers, and engineering managers. Y ou might be able to relate to one of the following\\nscenarios:\\nx | Preface', '• Y ou have been given a business problem and a lot of raw data. Y ou want to•\\nengineer this data and choose the right metrics to solve this problem.\\n• Y our initial models perform well in offline experiments and you want to deploy•\\nthem.\\n• Y ou have little feedback on how your models are performing after your models•\\nare deployed, and you want to figure out a way to quickly detect, debug, and\\naddress any issue your models might run into in production.\\n• The process of developing, evaluating, deploying, and updating models for your•\\nteam has been mostly manual, slow, and error-prone. Y ou want to automate and\\nimprove this process.\\n• Each ML use case in your organization has been deployed using its own work‐•\\nflow, and you want to lay down the foundation (e.g., model store, feature store,\\nmonitoring tools) that can be shared and reused across use cases.\\n• Y ou’re worried that there might be biases in your ML systems and you want to•\\nmake your systems responsible!\\nY ou can also benefit from the book if you belong to one of the following groups:\\n• Tool developers who want to identify underserved areas in ML production and•\\nfigure out how to position your tools in the ecosystem.\\n• Individuals looking for ML-related roles in the industry.•\\n• Technical and business leaders who are considering adopting ML solutions to•\\nimprove your products and/or business processes. Readers without strong tech‐\\nnical backgrounds might benefit the most from Chapters 1, 2, and 11.\\nWhat This Book Is Not\\nThis book is not an introduction to ML. There are many books, courses, and\\nresources available for ML theories, and therefore, this book shies away from these\\nconcepts to focus on the practical aspects of ML. To be specific, the book assumes\\nthat readers have a basic understanding of the following topics:\\n• ML models such as clustering, logistic regression, decision trees, collaborative fil‐•\\ntering, and various neural network architectures including feed-forward, recur‐\\nrent, convolutional, and transformer\\n• ML techniques such as supervised versus unsupervised, gradient descent, objec‐•\\ntive/loss function, regularization, generalization, and hyperparameter tuning\\n• Metrics such as accuracy, F1, precision, recall, ROC, mean squared error, and•\\nlog-likelihood\\nPreface | xi', '• Statistical concepts  such as variance, probability, and normal/long-tail•\\ndistribution\\n• Common ML tasks such as language modeling, anomaly detection, object classifi‐•\\ncation, and machine translation\\nY ou don’t have to know these topics inside out—for concepts whose exact definitions\\ncan take some effort to remember, e.g., F1 score, we include short notes as refer‐\\nences—but you should have a rough sense of what they mean going in.\\nWhile this book mentions current tools to illustrate certain concepts and solutions,\\nit’s not a tutorial book. Technologies evolve over time. Tools go in and out of style\\nquickly, but fundamental approaches to problem solving should last a bit longer. This\\nbook provides a framework for you to evaluate the tool that works best for your\\nuse cases. When there’s a tool you want to use, it’s usually straightforward to find\\ntutorials for it online. As a result, this book has few code snippets and instead focuses\\non providing a lot of discussion around trade-offs, pros and cons, and concrete\\nexamples.\\nNavigating This Book\\nThe chapters in this book are organized to reflect the problems data scientists might\\nencounter as they progress through the lifecycle of an ML project. The first two\\nchapters lay down the groundwork to set an ML project up for success, starting from\\nthe most basic question: does your project need ML? It also covers choosing the\\nobjectives for your project and how to frame your problem in a way that makes for\\nsimpler solutions. If you’re already familiar with these considerations and impatient\\nto get to the technical solutions, feel free to skip the first two chapters.\\nChapters 4 to 6 cover the pre-deployment phase of an ML project: from creating the\\ntraining data and engineering features to developing and evaluating your models in\\na development environment. This is the phase where expertise in both ML and the\\nproblem domain are especially needed.\\nChapters 7 to 9 cover the deployment and post-deployment phase of an ML project.\\nWe’ll learn through a story many readers might be able to relate to that having\\na model deployed isn’t the end of the deployment process. The deployed model\\nwill need to be monitored and continually updated to changing environments and\\nbusiness requirements.\\nChapters 3 and 10 focus on the infrastructure needed to enable stakeholders from\\ndifferent backgrounds to work together to deliver successful ML systems. Chapter 3', 'will need to be monitored and continually updated to changing environments and\\nbusiness requirements.\\nChapters 3 and 10 focus on the infrastructure needed to enable stakeholders from\\ndifferent backgrounds to work together to deliver successful ML systems. Chapter 3\\nfocuses on data systems, whereas Chapter 10 focuses on compute infrastructure and\\nML platforms. I debated for a long time on how deep to go into data systems and\\nwhere to introduce it in the book. Data systems, including databases, data formats,\\nxii | Preface', 'data movements, and data processing engines, tend to be sparsely covered in ML\\ncoursework, and therefore many data scientists might think of them as low level or\\nirrelevant. After consulting with many of my colleagues, I decided that because ML\\nsystems depend on data, covering the basics of data systems early will help us get on\\nthe same page to discuss data matters in the rest of the book.\\nWhile we cover many technical aspects of an ML system in this book, ML systems are\\nbuilt by people, for people, and can have outsized impact on the life of many. It’ d be\\nremiss to write a book on ML production without a chapter on the human side of it,\\nwhich is the focus of Chapter 11, the last chapter.\\nNote that “data scientist” is a role that has evolved a lot in the last few years, and\\nthere have been many discussions to determine what this role should entail—we’ll go\\ninto some of these discussions in Chapter 10 . In this book, we use “data scientist”\\nas an umbrella term to include anyone who works developing and deploying ML\\nmodels, including people whose job titles might be ML engineers, data engineers,\\ndata analysts, etc.\\nGitHub Repository and Community\\nThis book is accompanied by a GitHub repository that contains:\\n• A review of basic ML concepts•\\n• A list of references used in this book and other advanced, updated resources•\\n• Code snippets used in this book•\\n• A list of tools you can use for certain problems you might encounter in your•\\nworkflows\\nI also run a Discord server on MLOps  where you’re encouraged to discuss and ask\\nquestions about the book.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program\\nelements such as variable or function names, databases, data types, environment\\nvariables, statements, and keywords.\\nPreface | xiii', 'This element signifies a general note.\\nThis element indicates a warning or caution.\\nUsing Code Examples\\nAs mentioned, supplemental material (code examples, exercises, etc.) is available for\\ndownload at https://oreil.ly/designing-machine-learning-systems-code.\\nIf you have a technical question or a problem using the code examples, please send\\nemail to bookquestions@oreilly.com.\\nThis book is here to help you get your job done. In general, if example code is\\noffered with this book, you may use it in your programs and documentation. Y ou\\ndo not need to contact us for permission unless you’re reproducing a significant\\nportion of the code. For example, writing a program that uses several chunks of code\\nfrom this book does not require permission. Selling or distributing examples from\\nO’Reilly books does require permission. Answering a question by citing this book\\nand quoting example code does not require permission. Incorporating a significant\\namount of example code from this book into your product’s documentation does\\nrequire permission.\\nWe appreciate, but generally do not require, attribution. An attribution usually\\nincludes the title, author, publisher, and ISBN. For example: “ Designing Machine\\nLearning Systems  by Chip Huyen (O’Reilly). Copyright 2022 Huyen Thi Khanh\\nNguyen, 978-1-098-10796-3. ”\\nIf you feel your use of code examples falls outside fair use or the permission given\\nabove, feel free to contact us at permissions@oreilly.com.\\nO’Reilly Online Learning\\nFor more than 40 years, O’Reilly Media has provided technol‐\\nogy and business training, knowledge, and insight to help\\ncompanies succeed.\\nxiv | Preface', 'Our unique network of experts and innovators share their knowledge and expertise\\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\\nplatform gives you on-demand access to live training courses, in-depth learning\\npaths, interactive coding environments, and a vast collection of text and video from\\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com.\\nHow to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. Y ou can access this page at https://oreil.ly/designing-machine-learning-\\nsystems.\\nEmail bookquestions@oreilly.com to comment or ask technical questions about this\\nbook.\\nFor news and information about our books and courses, visit https://oreilly.com.\\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media\\nFollow us on Twitter: https://twitter.com/oreillymedia\\nWatch us on Y ouTube: https://youtube.com/oreillymedia\\nAcknowledgments\\nThis book took two years to write, and many more years beforehand to prepare.\\nLooking back, I’m equally amazed and grateful for the enormous amount of help\\nI received in writing this book. I tried my best to include the names of everyone\\nwho has helped me here, but due to the inherent faultiness of human memory, I\\nundoubtedly neglected to mention many. If I forgot to include your name, please\\nknow that it wasn’t because I don’t appreciate your contribution and please kindly\\nremind me so that I can rectify as soon as possible!\\nFirst and foremost, I’ d like to thank the course staff who helped me develop the\\ncourse and materials this book was based on: Michael Cooper, Xi Yin, Chloe He,\\nKinbert Chou, Megan Leszczynski, Karan Goel, and Michele Catasta. I’ d like to\\nPreface | xv', 'thank my professors, Christopher Ré and Mehran Sahami, without whom the course\\nwouldn’t exist in the first place.\\nI’ d like to thank a long list of reviewers who not only gave encouragement but\\nalso improved the book by many orders of magnitude: Eugene Y an, Josh Wills,\\nHan-chung Lee, Thomas Dietterich, Irene Tematelewo, Goku Mohandas, Jacopo\\nTagliabue, Andrey Kurenkov, Zach Nussbaum, Jay Chia, Laurens Geffert, Brian Spier‐\\ning, Erin Ledell, Rosanne Liu, Chin Ling, Shreya Shankar, and Sara Hooker.\\nI’ d like to thank all the readers who read the early release version of the book and gave\\nme ideas on how to improve the book, including Charles Frye, Xintong Yu, Jordan\\nZhang, Jonathon Belotti, and Cynthia Yu.\\nOf course, the book wouldn’t have been possible with the team at O’Reilly, especially\\nmy development editor, Jill Leonard, and my production editors, Kristen Brown,\\nSharon Tripp, and Gregory Hyman. I’ d like to thank Laurence Moroney, Hannes\\nHapke, and Rebecca Novack, who helped me get this book from an idea to a proposal.\\nThis book, after all, is an accumulation of invaluable lessons I learned throughout my\\ncareer to date. I owe these lessons to my extremely competent and patient coworkers\\nand former coworkers at Claypot AI, Primer AI, Netflix, NVIDIA, and Snorkel AI.\\nEvery person I’ve worked with has taught me something new about bringing ML into\\nthe world.\\nA special thanks to my cofounder Zhenzhong Xu for putting out the fires at our\\nstartup and allowing me to spend time on this book. Thank you, Luke, for always\\nbeing so supportive of everything that I want to do, no matter how ambitious it is.\\nxvi | Preface', '1 Mike Schuster, Melvin Johnson, and Nikhil Thorat, “Zero-Shot Translation with Google’s Multilingual Neural\\nMachine Translation System, ” Google AI Blog, November 22, 2016, https://oreil.ly/2R1CB.\\n2 Larry Hardesty, “ A Method to Image Black Holes, ” MIT News, June 6, 2016, https://oreil.ly/HpL2F.\\nCHAPTER 1\\nOverview of Machine Learning Systems\\nIn November 2016, Google announced that it had incorporated its multilingual\\nneural machine translation system into Google Translate, marking one of the first\\nsuccess stories of deep artificial neural networks in production at scale. 1 According\\nto Google, with this update, the quality of translation improved more in a single leap\\nthan they had seen in the previous 10 years combined.\\nThis success of deep learning renewed the interest in machine learning (ML) at large.\\nSince then, more and more companies have turned toward ML for solutions to their\\nmost challenging problems. In just five years, ML has found its way into almost\\nevery aspect of our lives: how we access information, how we communicate, how we\\nwork, how we find love. The spread of ML has been so rapid that it’s already hard to\\nimagine life without it. Y et there are still many more use cases for ML waiting to be\\nexplored in fields such as health care, transportation, farming, and even in helping us\\nunderstand the universe.2\\nMany people, when they hear “machine learning system, ” think of just the ML algo‐\\nrithms being used such as logistic regression or different types of neural networks.\\nHowever, the algorithm is only a small part of an ML system in production. The\\nsystem also includes the business requirements that gave birth to the ML project in\\nthe first place, the interface where users and developers interact with your system, the\\ndata stack, and the logic for developing, monitoring, and updating your models, as\\nwell as the infrastructure that enables the delivery of that logic. Figure 1-1 shows you\\nthe different components of an ML system and in which chapters of this book they\\nwill be covered.\\n1', 'The Relationship Between MLOps and ML Systems Design\\nOps in MLOps comes from DevOps, short for Developments and\\nOperations. To operationalize something means to bring it into\\nproduction, which includes deploying, monitoring, and maintain‐\\ning it. MLOps is a set of tools and best practices for bringing ML\\ninto production.\\nML systems design takes a system approach to MLOps, which\\nmeans that it considers an ML system holistically to ensure that all\\nthe components and their stakeholders can work together to satisfy\\nthe specified objectives and requirements.\\nFigure 1-1. Different components of an ML system. “ML algorithms” is usually what\\npeople think of when they say machine learning, but it’s only a small part of the entire\\nsystem.\\nThere are many excellent books about various ML algorithms. This book doesn’t\\ncover any specific algorithms in detail but rather helps readers understand the entire\\nML system as a whole. In other words, this book’s goal is to provide you with a frame‐\\nwork to develop a solution that best works for your problem, regardless of which\\nalgorithm you might end up using. Algorithms might become outdated quickly as\\nnew algorithms are constantly being developed, but the framework proposed in this\\nbook should still work with new algorithms.\\nThe first chapter of the book aims to give you an overview of what it takes to bring\\nan ML model to production. Before discussing how to develop an ML system, it’s\\n2 | Chapter 1: Overview of Machine Learning Systems', '3 I didn’t ask whether ML is sufficient because the answer is always no.\\nimportant to ask a fundamental question of when and when not to use ML. We’ll\\ncover some of the popular use cases of ML to illustrate this point.\\nAfter the use cases, we’ll move on to the challenges of deploying ML systems, and\\nwe’ll do so by comparing ML in production to ML in research as well as to traditional\\nsoftware. If you’ve been in the trenches of developing applied ML systems, you might\\nalready be familiar with what’s written in this chapter. However, if you have only had\\nexperience with ML in an academic setting, this chapter will give an honest view of\\nML in the real world and set your first application up for success.\\nWhen to Use Machine Learning\\nAs its adoption in the industry quickly grows, ML has proven to be a powerful tool\\nfor a wide range of problems. Despite an incredible amount of excitement and hype\\ngenerated by people both inside and outside the field, ML is not a magic tool that can\\nsolve all problems. Even for problems that ML can solve, ML solutions might not be\\nthe optimal solutions. Before starting an ML project, you might want to ask whether\\nML is necessary or cost-effective.3\\nTo understand what ML can do, let’s examine what ML solutions generally do:\\nMachine learning is an approach to (1) learn (2) complex patterns from (3) existing\\ndata and use these patterns to make (4) predictions on (5) unseen data.\\nWe’ll look at each of the italicized keyphrases in the above framing to understand its\\nimplications to the problems ML can solve:\\n1. Learn: the system has the capacity to learn\\nA relational database isn’t an ML system because it doesn’t have the capacity\\nto learn. Y ou can explicitly state the relationship between two columns in a rela‐\\ntional database, but it’s unlikely to have the capacity to figure out the relationship\\nbetween these two columns by itself.\\nFor an ML system to learn, there must be something for it to learn from. In most\\ncases, ML systems learn from data. In supervised learning, based on example\\ninput and output pairs, ML systems learn how to generate outputs for arbitrary\\ninputs. For example, if you want to build an ML system to learn to predict\\nthe rental price for Airbnb listings, you need to provide a dataset where each\\ninput is a listing with relevant characteristics (square footage, number of rooms,\\nneighborhood, amenities, rating of that listing, etc.) and the associated output is', 'the rental price for Airbnb listings, you need to provide a dataset where each\\ninput is a listing with relevant characteristics (square footage, number of rooms,\\nneighborhood, amenities, rating of that listing, etc.) and the associated output is\\nthe rental price of that listing. Once learned, this ML system should be able to\\npredict the price of a new listing given its characteristics.\\nWhen to Use Machine Learning | 3', '4 Patterns are different from distributions. We know the distribution of the outcomes of a fair die, but there are\\nno patterns in the way the outcomes are generated.\\n5 Andrej Karpathy, “Software 2.0, ” Medium, November 11, 2017, https://oreil.ly/yHZrE.\\n2. Complex patterns: there are patterns to learn, and they are complex\\nML solutions are only useful when there are patterns to learn. Sane people\\ndon’t invest money into building an ML system to predict the next outcome\\nof a fair die because there’s no pattern in how these outcomes are generated. 4\\nHowever, there are patterns in how stocks are priced, and therefore companies\\nhave invested billions of dollars in building ML systems to learn those patterns.\\nWhether a pattern exists might not be obvious, or if patterns exist, your dataset\\nor ML algorithms might not be sufficient to capture them. For example, there\\nmight be a pattern in how Elon Musk’s tweets affect cryptocurrency prices. How‐\\never, you wouldn’t know until you’ve rigorously trained and evaluated your ML\\nmodels on his tweets. Even if all your models fail to make reasonable predictions\\nof cryptocurrency prices, it doesn’t mean there’s no pattern.\\nConsider a website like Airbnb with a lot of house listings; each listing comes\\nwith a zip code. If you want to sort listings into the states they are located in,\\nyou wouldn’t need an ML system. Since the pattern is simple—each zip code\\ncorresponds to a known state—you can just use a lookup table.\\nThe relationship between a rental price and all its characteristics follows a much\\nmore complex pattern, which would be very challenging to manually specify. ML\\nis a good solution for this. Instead of telling your system how to calculate the\\nprice from a list of characteristics, you can provide prices and characteristics, and\\nlet your ML system figure out the pattern. The difference between ML solutions\\nand the lookup table solution as well as general traditional software solutions is\\nshown in Figure 1-2. For this reason, ML is also called Software 2.0.5\\nML has been very successful with tasks with complex patterns such as object\\ndetection and speech recognition. What is complex to machines is different from\\nwhat is complex to humans. Many tasks that are hard for humans to do are\\neasy for machines—for example, raising a number of the power of 10. On the\\nother hand, many tasks that are easy for humans can be hard for machines—for\\nexample, deciding whether there’s a cat in a picture.', 'what is complex to humans. Many tasks that are hard for humans to do are\\neasy for machines—for example, raising a number of the power of 10. On the\\nother hand, many tasks that are easy for humans can be hard for machines—for\\nexample, deciding whether there’s a cat in a picture.\\n4 | Chapter 1: Overview of Machine Learning Systems', '6 We’ll go over online learning in Chapter 9.\\nFigure 1-2. Instead of requiring hand-specified patterns to calculate outputs, ML\\nsolutions learn patterns from inputs and outputs\\n3. Existing data: data is available, or it’s possible to collect data\\nBecause ML learns from data, there must be data for it to learn from. It’s amusing\\nto think about building a model to predict how much tax a person should pay a\\nyear, but it’s not possible unless you have access to tax and income data of a large\\npopulation.\\nIn the zero-shot learning (sometimes known as zero-data learning) context, it’s\\npossible for an ML system to make good predictions for a task without having\\nbeen trained on data for that task. However, this ML system was previously\\ntrained on data for other tasks, often related to the task in consideration. So even\\nthough the system doesn’t require data for the task at hand to learn from, it still\\nrequires data to learn.\\nIt’s also possible to launch an ML system without data. For example, in the\\ncontext of continual learning, ML models can be deployed without having been\\ntrained on any data, but they will learn from incoming data in production. 6\\nHowever, serving insufficiently trained models to users comes with certain risks,\\nsuch as poor customer experience.\\nWithout data and without continual learning, many companies follow a “fake-it-\\ntil-you make it” approach: launching a product that serves predictions made by\\nhumans, instead of ML models, with the hope of using the generated data to train\\nML models later.\\nWhen to Use Machine Learning | 5', '7 Steke Bako, Thijs Vogels, Brian McWilliams, Mark Meyer, Jan Novák, Alex Harvill, Pradeep Sen, Tony Derose,\\nand Fabrice Rousselle, “Kernel-Predicting Convolutional Networks for Denoising Monte Carlo Renderings, ”\\nACM Transactions on Graphics 36, no. 4 (2017): 97, https://oreil.ly/EeI3j; Oliver Nalbach, Elena Arabadzhiy‐\\nska, Dushyant Mehta, Hans-Peter Seidel, and Tobias Ritschel, “Deep Shading: Convolutional Neural Networks\\nfor Screen-Space Shading, ” arXiv, 2016, https://oreil.ly/dSspz.\\n4. Predictions: it’s a predictive problem\\nML models make predictions, so they can only solve problems that require\\npredictive answers. ML can be especially appealing when you can benefit from a\\nlarge quantity of cheap but approximate predictions. In English, “predict” means\\n“estimate a value in the future. ” For example, what will the weather be like\\ntomorrow? Who will win the Super Bowl this year? What movie will a user want\\nto watch next?\\nAs predictive machines (e.g., ML models) are becoming more effective, more and\\nmore problems are being reframed as predictive problems. Whatever question\\nyou might have, you can always frame it as: “What would the answer to this\\nquestion be?” regardless of whether this question is about something in the\\nfuture, the present, or even the past.\\nCompute-intensive problems are one class of problems that have been very\\nsuccessfully reframed as predictive. Instead of computing the exact outcome of a\\nprocess, which might be even more computationally costly and time-consuming\\nthan ML, you can frame the problem as: “What would the outcome of this\\nprocess look like?” and approximate it using an ML model. The output will be an\\napproximation of the exact output, but often, it’s good enough. Y ou can see a lot\\nof it in graphic renderings, such as image denoising and screen-space shading.7\\n5. Unseen data: unseen data shares patterns with the training data\\nThe patterns your model learns from existing data are only useful if unseen data\\nalso share these patterns. A model to predict whether an app will get downloaded\\non Christmas 2020 won’t perform very well if it’s trained on data from 2008,\\nwhen the most popular app on the App Store was Koi Pond. What’s Koi Pond?\\nExactly.\\nIn technical terms, it means your unseen data and training data should come\\nfrom similar distributions. Y ou might ask: “If the data is unseen, how do we\\nknow what distribution it comes from?” We don’t, but we can make assump‐', 'Exactly.\\nIn technical terms, it means your unseen data and training data should come\\nfrom similar distributions. Y ou might ask: “If the data is unseen, how do we\\nknow what distribution it comes from?” We don’t, but we can make assump‐\\ntions—such as we can assume that users’ behaviors tomorrow won’t be too\\ndifferent from users’ behaviors today—and hope that our assumptions hold. If\\nthey don’t, we’ll have a model that performs poorly, which we might be able to\\nfind out with monitoring, as covered in Chapter 8 , and test in production, as\\ncovered in Chapter 9.\\n6 | Chapter 1: Overview of Machine Learning Systems', 'Due to the way most ML algorithms today learn, ML solutions will especially shine if\\nyour problem has these additional following characteristics:\\n6. It’s repetitive\\nHumans are great at few-shot learning: you can show kids a few pictures of cats\\nand most of them will recognize a cat the next time they see one. Despite exciting\\nprogress in few-shot learning research, most ML algorithms still require many\\nexamples to learn a pattern. When a task is repetitive, each pattern is repeated\\nmultiple times, which makes it easier for machines to learn it.\\n7. The cost of wrong predictions is cheap\\nUnless your ML model’s performance is 100% all the time, which is highly\\nunlikely for any meaningful tasks, your model is going to make mistakes. ML is\\nespecially suitable when the cost of a wrong prediction is low. For example, one\\nof the biggest use cases of ML today is in recommender systems because with\\nrecommender systems, a bad recommendation is usually forgiving—the user just\\nwon’t click on the recommendation.\\nIf one prediction mistake can have catastrophic consequences, ML might still be\\na suitable solution if, on average, the benefits of correct predictions outweigh the\\ncost of wrong predictions. Developing self-driving cars is challenging because an\\nalgorithmic mistake can lead to death. However, many companies still want to\\ndevelop self-driving cars because they have the potential to save many lives once\\nself-driving cars are statistically safer than human drivers.\\n8. It’s at scale\\nML solutions often require nontrivial up-front investment on data, compute,\\ninfrastructure, and talent, so it’ d make sense if we can use these solutions a lot.\\n“ At scale” means different things for different tasks, but, in general, it means\\nmaking a lot of predictions. Examples include sorting through millions of emails\\na year or predicting which departments thousands of support tickets should be\\nrouted to a day.\\nA problem might appear to be a singular prediction, but it’s actually a series of\\npredictions. For example, a model that predicts who will win a US presidential\\nelection seems like it only makes one prediction every four years, but it might\\nactually be making a prediction every hour or even more frequently because that\\nprediction has to be continually updated to incorporate new information.\\nHaving a problem at scale also means that there’s a lot of data for you to collect,\\nwhich is useful for training ML models.\\nWhen to Use Machine Learning | 7', '9. The patterns are constantly changing\\nCultures change. Tastes change. Technologies change. What’s trendy today might\\nbe old news tomorrow. Consider the task of email spam classification. Today\\nan indication of a spam email is a Nigerian prince, but tomorrow it might be a\\ndistraught Vietnamese writer.\\nIf your problem involves one or more constantly changing patterns, hardcoded\\nsolutions such as handwritten rules can become outdated quickly. Figuring\\nhow your problem has changed so that you can update your handwritten rules\\naccordingly can be too expensive or impossible. Because ML learns from data,\\nyou can update your ML model with new data without having to figure out how\\nthe data has changed. It’s also possible to set up your system to adapt to the\\nchanging data distributions, an approach we’ll discuss in the section “Continual\\nLearning” on page 264.\\nThe list of use cases can go on and on, and it’ll grow even longer as ML adoption\\nmatures in the industry. Even though ML can solve a subset of problems very well,\\nit can’t solve and/or shouldn’t be used for a lot of problems. Most of today’s ML\\nalgorithms shouldn’t be used under any of the following conditions:\\n• It’s unethical. We’ll go over one case study where the use of ML algorithms can•\\nbe argued as unethical in the section “Case study I: Automated grader’s biases” on\\npage 341.\\n• Simpler solutions do the trick. In Chapter 6, we’ll cover the four phases of ML•\\nmodel development where the first phase should be non-ML solutions.\\n• It’s not cost-effective.•\\nHowever, even if ML can’t solve your problem, it might be possible to break your\\nproblem into smaller components, and use ML to solve some of them. For example,\\nif you can’t build a chatbot to answer all your customers’ queries, it might be possible\\nto build an ML model to predict whether a query matches one of the frequently asked\\nquestions. If yes, direct the customer to the answer. If not, direct them to customer\\nservice.\\nI’ d also want to caution against dismissing a new technology because it’s not as cost-\\neffective as the existing technologies at the moment. Most technological advances are\\nincremental. A type of technology might not be efficient now, but it might be over\\ntime with more investments. If you wait for the technology to prove its worth to the\\nrest of the industry before jumping in, you might end up years or decades behind\\nyour competitors.\\n8 | Chapter 1: Overview of Machine Learning Systems', 'Machine Learning Use Cases\\nML has found increasing usage in both enterprise and consumer applications. Since\\nthe mid-2010s, there has been an explosion of applications that leverage ML to\\ndeliver superior or previously impossible services to consumers.\\nWith the explosion of information and services, it would have been very challenging\\nfor us to find what we want without the help of ML, manifested in either a search\\nengine or a recommender system. When you visit a website like Amazon or Netflix,\\nyou’re recommended items that are predicted to best match your taste. If you don’t\\nlike any of your recommendations, you might want to search for specific items, and\\nyour search results are likely powered by ML.\\nIf you have a smartphone, ML is likely already assisting you in many of your daily\\nactivities. Typing on your phone is made easier with predictive typing, an ML system\\nthat gives you suggestions on what you might want to say next. An ML system might\\nrun in your photo editing app to suggest how best to enhance your photos. Y ou might\\nauthenticate your phone using your fingerprint or your face, which requires an ML\\nsystem to predict whether a fingerprint or a face matches yours.\\nThe ML use case that drew me into the field was machine translation, automatically\\ntranslating from one language to another. It has the potential to allow people from\\ndifferent cultures to communicate with each other, erasing the language barrier. My\\nparents don’t speak English, but thanks to Google Translate, now they can read my\\nwriting and talk to my friends who don’t speak Vietnamese.\\nML is increasingly present in our homes with smart personal assistants such as Alexa\\nand Google Assistant. Smart security cameras can let you know when your pets leave\\nhome or if you have an uninvited guest. A friend of mine was worried about his aging\\nmother living by herself—if she falls, no one is there to help her get up—so he relied\\non an at-home health monitoring system that predicts whether someone has fallen in\\nthe house.\\nEven though the market for consumer ML applications is booming, the majority of\\nML use cases are still in the enterprise world. Enterprise ML applications tend to\\nhave vastly different requirements and considerations from consumer applications.\\nThere are many exceptions, but for most cases, enterprise applications might have\\nstricter accuracy requirements but be more forgiving with latency requirements. For', 'have vastly different requirements and considerations from consumer applications.\\nThere are many exceptions, but for most cases, enterprise applications might have\\nstricter accuracy requirements but be more forgiving with latency requirements. For\\nexample, improving a speech recognition system’s accuracy from 95% to 95.5% might\\nnot be noticeable to most consumers, but improving a resource allocation system’s\\nefficiency by just 0.1% can help a corporation like Google or General Motors save\\nmillions of dollars. At the same time, latency of a second might get a consumer\\ndistracted and opening something else, but enterprise users might be more tolerant\\nof high latency. For people interested in building companies out of ML applications,\\nWhen to Use Machine Learning | 9', '8 “2020 State of Enterprise Machine Learning, ” Algorithmia, 2020, https://oreil.ly/wKMZB.\\nconsumer apps might be easier to distribute but much harder to monetize. However,\\nmost enterprise use cases aren’t obvious unless you’ve encountered them yourself.\\nAccording to Algorithmia’s 2020 state of enterprise machine learning survey, ML\\napplications in enterprises are diverse, serving both internal use cases (reducing costs,\\ngenerating customer insights and intelligence, internal processing automation) and\\nexternal use cases (improving customer experience, retaining customers, interacting\\nwith customers) as shown in Figure 1-3.8\\nFigure 1-3. 2020 state of enterprise machine learning. Source: Adapted from an image by\\nAlgorithmia\\n10 | Chapter 1: Overview of Machine Learning Systems', '9 “ Average Mobile App User Acquisition Costs Worldwide from September 2018 to August 2019, by User\\nAction and Operating System, ” Statista, 2019, https://oreil.ly/2pTCH.\\n10 Jeff Henriksen, “Valuing Lyft Requires a Deep Look into Unit Economics, ” Forbes, May 17, 2019,\\nhttps://oreil.ly/VeSt4.\\n11 David Skok, “Startup Killer: The Cost of Customer Acquisition, ” For Entrepreneurs, 2018,\\nhttps://oreil.ly/L3tQ7.\\n12 Amy Gallo, “The Value of Keeping the Right Customers, ” Harvard Business Review, October 29, 2014,\\nhttps://oreil.ly/OlNkl.\\nFraud detection is among the oldest applications of ML in the enterprise world. If\\nyour product or service involves transactions of any value, it’ll be susceptible to\\nfraud. By leveraging ML solutions for anomaly detection, you can have systems that\\nlearn from historical fraud transactions and predict whether a future transaction is\\nfraudulent.\\nDeciding how much to charge for your product or service is probably one of the\\nhardest business decisions; why not let ML do it for you? Price optimization is the\\nprocess of estimating a price at a certain time period to maximize a defined objective\\nfunction, such as the company’s margin, revenue, or growth rate. ML-based pricing\\noptimization is most suitable for cases with a large number of transactions where\\ndemand fluctuates and consumers are willing to pay a dynamic price—for example,\\ninternet ads, flight tickets, accommodation bookings, ride-sharing, and events.\\nTo run a business, it’s important to be able to forecast customer demand so that you\\ncan prepare a budget, stock inventory, allocate resources, and update pricing strategy.\\nFor example, if you run a grocery store, you want to stock enough so that customers\\nfind what they’re looking for, but you don’t want to overstock, because if you do, your\\ngroceries might go bad and you lose money.\\nAcquiring a new user is expensive. As of 2019, the average cost for an app to acquire\\na user who’ll make an in-app purchase is $86.61. 9 The acquisition cost for Lyft is\\nestimated at $158/rider.10 This cost is so much higher for enterprise customers. Cus‐\\ntomer acquisition cost is hailed by investors as a startup killer. 11 Reducing customer\\nacquisition costs by a small amount can result in a large increase in profit. This can\\nbe done through better identifying potential customers, showing better-targeted ads,\\ngiving discounts at the right time, etc.—all of which are suitable tasks for ML.', 'acquisition costs by a small amount can result in a large increase in profit. This can\\nbe done through better identifying potential customers, showing better-targeted ads,\\ngiving discounts at the right time, etc.—all of which are suitable tasks for ML.\\nAfter you’ve spent so much money acquiring a customer, it’ d be a shame if they\\nleave. The cost of acquiring a new user is approximated to be 5 to 25 times more\\nexpensive than retaining an existing one. 12 Churn prediction is predicting when a\\nspecific customer is about to stop using your products or services so that you can\\ntake appropriate actions to win them back. Churn prediction can be used not only for\\ncustomers but also for employees.\\nWhen to Use Machine Learning | 11', '13 Marty Swant, “The World’s 20 Most Valuable Brands, ” Forbes, 2020, https://oreil.ly/4uS5i.\\nTo prevent customers from leaving, it’s important to keep them happy by addressing\\ntheir concerns as soon as they arise. Automated support ticket classification can help\\nwith that. Previously, when a customer opened a support ticket or sent an email,\\nit needed to first be processed then passed around to different departments until it\\narrived at the inbox of someone who could address it. An ML system can analyze the\\nticket content and predict where it should go, which can shorten the response time\\nand improve customer satisfaction. It can also be used to classify internal IT tickets.\\nAnother popular use case of ML in enterprise is brand monitoring. The brand is\\na valuable asset of a business. 13 It’s important to monitor how the public and your\\ncustomers perceive your brand. Y ou might want to know when/where/how it’s men‐\\ntioned, both explicitly (e.g., when someone mentions “Google”) or implicitly (e.g.,\\nwhen someone says “the search giant”), as well as the sentiment associated with it.\\nIf there’s suddenly a surge of negative sentiment in your brand mentions, you might\\nwant to address it as soon as possible. Sentiment analysis is a typical ML task.\\nA set of ML use cases that has generated much excitement recently is in health care.\\nThere are ML systems that can detect skin cancer and diagnose diabetes. Even though\\nmany health-care applications are geared toward consumers, because of their strict\\nrequirements with accuracy and privacy, they are usually provided through a health-\\ncare provider such as a hospital or used to assist doctors in providing diagnosis.\\nUnderstanding Machine Learning Systems\\nUnderstanding ML systems will be helpful in designing and developing them. In this\\nsection, we’ll go over how ML systems are different from both ML in research (or\\nas often taught in school) and traditional software, which motivates the need for this\\nbook.\\nMachine Learning in Research Versus in Production\\nAs ML usage in the industry is still fairly new, most people with ML expertise have\\ngained it through academia: taking courses, doing research, reading academic papers.\\nIf that describes your background, it might be a steep learning curve for you to\\nunderstand the challenges of deploying ML systems in the wild and navigate an\\noverwhelming set of solutions to these challenges. ML in production is very different', 'If that describes your background, it might be a steep learning curve for you to\\nunderstand the challenges of deploying ML systems in the wild and navigate an\\noverwhelming set of solutions to these challenges. ML in production is very different\\nfrom ML in research. Table 1-1 shows five of the major differences.\\n12 | Chapter 1: Overview of Machine Learning Systems', 'Table 1-1. Key differences between ML in research and ML in production\\n Research Production\\nRequirements State-of-the-art model performance on\\nbenchmark datasets\\nDifferent stakeholders have different\\nrequirements\\nComputational priority Fast training, high throughput Fast inference, low latency\\nData Statica Constantly shifting\\nFairness Often not a focus Must be considered\\nInterpretability Often not a focus Must be considered\\na A subfield of research focuses on continual learning: developing models to work with changing data distributions. We’ll\\ncover continual learning in Chapter 9.\\nDifferent stakeholders and requirements\\nPeople involved in a research and leaderboard project often align on one single\\nobjective. The most common objective is model performance—develop a model that\\nachieves the state-of-the-art results on benchmark datasets. To edge out a small\\nimprovement in performance, researchers often resort to techniques that make mod‐\\nels too complex to be useful.\\nThere are many stakeholders involved in bringing an ML system into production.\\nEach stakeholder has their own requirements. Having different, often conflicting,\\nrequirements can make it difficult to design, develop, and select an ML model that\\nsatisfies all the requirements.\\nConsider a mobile app that recommends restaurants to users. The app makes money\\nby charging restaurants a 10% service fee on each order. This means that expensive\\norders give the app more money than cheap orders. The project involves ML engi‐\\nneers, salespeople, product managers, infrastructure engineers, and a manager:\\nML engineers\\nWant a model that recommends restaurants that users will most likely order\\nfrom, and they believe they can do so by using a more complex model with more\\ndata.\\nSales team\\nWants a model that recommends the more expensive restaurants since these\\nrestaurants bring in more service fees.\\nProduct team\\nNotices that every increase in latency leads to a drop in orders through the\\nservice, so they want a model that can return the recommended restaurants in\\nless than 100 milliseconds.\\nUnderstanding Machine Learning Systems | 13', '14 It’s not unusual for the ML and data science teams to be among the first to go during a company’s mass\\nlayoff, as has been reported at IBM, Uber, Airbnb. See also Sejuti Das’s analysis “How Data Scientists Are Also\\nSusceptible to the Layoffs Amid Crisis, ” Analytics India Magazine, May 21, 2020, https://oreil.ly/jobmz.\\n15 Wikipedia, s.v. “Ensemble learning, ” https://oreil.ly/5qkgp.\\nML platform team\\nAs the traffic grows, this team has been woken up in the middle of the night\\nbecause of problems with scaling their existing system, so they want to hold off\\non model updates to prioritize improving the ML platform.\\nManager\\nWants to maximize the margin, and one way to achieve this might be to let go of\\nthe ML team.14\\n“Recommending the restaurants that users are most likely to click on” and “recom‐\\nmending the restaurants that will bring in the most money for the app” are two\\ndifferent objectives, and in the section “Decoupling objectives” on page 41, we’ll\\ndiscuss how to develop an ML system that satisfies different objectives. Spoiler: we’ll\\ndevelop one model for each objective and combine their predictions.\\nLet’s imagine for now that we have two different models. Model A is the model that\\nrecommends the restaurants that users are most likely to click on, and model B is\\nthe model that recommends the restaurants that will bring in the most money for the\\napp. A and B might be very different models. Which model should be deployed to the\\nusers? To make the decision more difficult, neither A nor B satisfies the requirement\\nset forth by the product team: they can’t return restaurant recommendations in less\\nthan 100 milliseconds.\\nWhen developing an ML project, it’s important for ML engineers to understand\\nrequirements from all stakeholders involved and how strict these requirements are.\\nFor example, if being able to return recommendations within 100 milliseconds is a\\nmust-have requirement—the company finds that if your model takes over 100 milli‐\\nseconds to recommend restaurants, 10% of users would lose patience and close the\\napp—then neither model A nor model B will work. However, if it’s just a nice-to-have\\nrequirement, you might still want to consider model A or model B.\\nProduction having different requirements from research is one of the reasons why\\nsuccessful research projects might not always be used in production. For example,\\nensembling is a technique popular among the winners of many ML competitions,', 'Production having different requirements from research is one of the reasons why\\nsuccessful research projects might not always be used in production. For example,\\nensembling is a technique popular among the winners of many ML competitions,\\nincluding the famed $1 million Netflix Prize, and yet it’s not widely used in produc‐\\ntion. Ensembling combines “multiple learning algorithms to obtain better predictive\\nperformance than could be obtained from any of the constituent learning algorithms\\nalone. ”15 While it can give your ML system a small performance improvement,\\nensembling tends to make a system too complex to be useful in production, e.g.,\\n14 | Chapter 1: Overview of Machine Learning Systems', '16 Julia Evans, “Machine Learning Isn’t Kaggle Competitions, ” 2014, https://oreil.ly/p8mZq.\\n17 Lauren Oakden-Rayner, “ AI Competitions Don’t Produce Useful Models, ” September 19, 2019,\\nhttps://oreil.ly/X6RlT.\\n18 Kawin Ethayarajh and Dan Jurafsky, “Utility Is in the Eye of the User: A Critique of NLP Leaderboards, ”\\nEMNLP , 2020, https://oreil.ly/4Ud8P.\\nslower to make predictions or harder to interpret the results. We’ll discuss ensem‐\\nbling further in the section “Ensembles” on page 156.\\nFor many tasks, a small improvement in performance can result in a huge boost\\nin revenue or cost savings. For example, a 0.2% improvement in the click-through\\nrate for a product recommender system can result in millions of dollars increase\\nin revenue for an ecommerce site. However, for many tasks, a small improvement\\nmight not be noticeable for users. For the second type of task, if a simple model can\\ndo a reasonable job, complex models must perform significantly better to justify the\\ncomplexity.\\nCriticism of ML Leaderboards\\nIn recent years, there have been many critics of ML leaderboards, both competitions\\nsuch as Kaggle and research leaderboards such as ImageNet or GLUE.\\nAn obvious argument is that in these competitions many of the hard steps needed for\\nbuilding ML systems are already done for you.16\\nA less obvious argument is that due to the multiple-hypothesis testing scenario that\\nhappens when you have multiple teams testing on the same hold-out test set, a model\\ncan do better than the rest just by chance.17\\nThe misalignment of interests between research and production has been noticed\\nby researchers. In an EMNLP 2020 paper, Ethayarajh and Jurafsky argued that\\nbenchmarks have helped drive advances in natural language processing (NLP) by\\nincentivizing the creation of more accurate models at the expense of other qualities\\nvalued by practitioners such as compactness, fairness, and energy efficiency.18\\nComputational priorities\\nWhen designing an ML system, people who haven’t deployed an ML system often\\nmake the mistake of focusing too much on the model development part and not\\nenough on the model deployment and maintenance part.\\nDuring the model development process, you might train many different models, and\\neach model does multiple passes over the training data. Each trained model then\\ngenerates predictions on the validation data once to report the scores. The validation\\ndata is usually much smaller than the training data. During model development,', 'each model does multiple passes over the training data. Each trained model then\\ngenerates predictions on the validation data once to report the scores. The validation\\ndata is usually much smaller than the training data. During model development,\\nUnderstanding Machine Learning Systems | 15', '19 Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA: O’Reilly, 2017).\\ntraining is the bottleneck. Once the model has been deployed, however, its job is to\\ngenerate predictions, so inference is the bottleneck. Research usually prioritizes fast\\ntraining, whereas production usually prioritizes fast inference.\\nOne corollary of this is that research prioritizes high throughput whereas production\\nprioritizes low latency. In case you need a refresh, latency refers to the time it takes\\nfrom receiving a query to returning the result. Throughput refers to how many\\nqueries are processed within a specific period of time.\\nTerminology Clash\\nSome books make the distinction between latency and response\\ntime. According to Martin Kleppmann in his book Designing Data-\\nIntensive Applications, “The response time is what the client sees:\\nbesides the actual time to process the request (the service time),\\nit includes network delays and queueing delays. Latency is the\\nduration that a request is waiting to be handled—during which it is\\nlatent, awaiting service. ”19\\nIn this book, to simplify the discussion and to be consistent with\\nthe terminology used in the ML community, we use latency to refer\\nto the response time, so the latency of a request measures the time\\nfrom when the request is sent to the time a response is received.\\nFor example, the average latency of Google Translate is the average time it takes from\\nwhen a user clicks Translate to when the translation is shown, and the throughput is\\nhow many queries it processes and serves a second.\\nIf your system always processes one query at a time, higher latency means lower\\nthroughput. If the average latency is 10 ms, which means it takes 10 ms to process\\na query, the throughput is 100 queries/second. If the average latency is 100 ms, the\\nthroughput is 10 queries/second.\\nHowever, because most modern distributed systems batch queries to process them\\ntogether, often concurrently, higher latency might also mean higher throughput. If you\\nprocess 10 queries at a time and it takes 10 ms to run a batch, the average latency is\\nstill 10 ms but the throughput is now 10 times higher—1,000 queries/second. If you\\nprocess 50 queries at a time and it takes 20 ms to run a batch, the average latency now\\nis 20 ms and the throughput is 2,500 queries/second. Both latency and throughput\\nhave increased! The difference in latency and throughput trade-off for processing', 'process 50 queries at a time and it takes 20 ms to run a batch, the average latency now\\nis 20 ms and the throughput is 2,500 queries/second. Both latency and throughput\\nhave increased! The difference in latency and throughput trade-off for processing\\nqueries one at a time and processing queries in batches is illustrated in Figure 1-4.\\n16 | Chapter 1: Overview of Machine Learning Systems', '20 Akamai Technologies, Akamai Online Retail Performance Report: Milliseconds Are Critical, April 19, 2017,\\nhttps://oreil.ly/bEtRu.\\n21 Lucas Bernardi, Themis Mavridis, and Pablo Estevez, “150 Successful Machine Learning Models: 6 Lessons\\nLearned at Booking.com, ” KDD ’19, August 4–8, 2019, Anchorage, AK, https://oreil.ly/G5QNA.\\n22 “Consumer Insights, ” Think with Google, https://oreil.ly/JCp6Z.\\nFigure 1-4. When processing queries one at a time, higher latency means lower through‐\\nput. When processing queries in batches, however, higher latency might also mean higher\\nthroughput.\\nThis is even more complicated if you want to batch online queries. Batching requires\\nyour system to wait for enough queries to arrive in a batch before processing them,\\nwhich further increases latency.\\nIn research, you care more about how many samples you can process in a second\\n(throughput) and less about how long it takes for each sample to be processed\\n(latency). Y ou’re willing to increase latency to increase throughput, for example, with\\naggressive batching.\\nHowever, once you deploy your model into the real world, latency matters a lot. In\\n2017, an Akamai study found that a 100 ms delay can hurt conversion rates by 7%. 20\\nIn 2019, Booking.com found that an increase of about 30% in latency cost about 0.5%\\nin conversion rates—“a relevant cost for our business. ” 21 In 2016, Google found that\\nmore than half of mobile users will leave a page if it takes more than three seconds to\\nload.22 Users today are even less patient.\\nUnderstanding Machine Learning Systems | 17', '23 Kleppmann, Designing Data-Intensive Applications.\\nTo reduce latency in production, you might have to reduce the number of queries you\\ncan process on the same hardware at a time. If your hardware is capable of processing\\nmany more queries at a time, using it to process fewer queries means underutilizing\\nyour hardware, increasing the cost of processing each query.\\nWhen thinking about latency, it’s important to keep in mind that latency is not an\\nindividual number but a distribution. It’s tempting to simplify this distribution by\\nusing a single number like the average (arithmetic mean) latency of all the requests\\nwithin a time window, but this number can be misleading. Imagine you have 10\\nrequests whose latencies are 100 ms, 102 ms, 100 ms, 100 ms, 99 ms, 104 ms, 110\\nms, 90 ms, 3,000 ms, 95 ms. The average latency is 390 ms, which makes your\\nsystem seem slower than it actually is. What might have happened is that there was\\na network error that made one request much slower than others, and you should\\ninvestigate that troublesome request.\\nIt’s usually better to think in percentiles, as they tell you something about a certain\\npercentage of your requests. The most common percentile is the 50th percentile,\\nabbreviated as p50. It’s also known as the median. If the median is 100 ms, half of the\\nrequests take longer than 100 ms, and half of the requests take less than 100 ms.\\nHigher percentiles also help you discover outliers, which might be symptoms of\\nsomething wrong. Typically, the percentiles you’ll want to look at are p90, p95, and\\np99. The 90th percentile (p90) for the 10 requests above is 3,000 ms, which is an\\noutlier.\\nHigher percentiles are important to look at because even though they account for\\na small percentage of your users, sometimes they can be the most important users.\\nFor example, on the Amazon website, the customers with the slowest requests are\\noften those who have the most data on their accounts because they have made many\\npurchases—that is, they’re the most valuable customers.23\\nIt’s a common practice to use high percentiles to specify the performance require‐\\nments for your system; for example, a product manager might specify that the 90th\\npercentile or 99.9th percentile latency of a system must be below a certain number.\\nData\\nDuring the research phase, the datasets you work with are often clean and well-\\nformatted, freeing you to focus on developing models. They are static by nature so', 'percentile or 99.9th percentile latency of a system must be below a certain number.\\nData\\nDuring the research phase, the datasets you work with are often clean and well-\\nformatted, freeing you to focus on developing models. They are static by nature so\\nthat the community can use them to benchmark new architectures and techniques.\\nThis means that many people might have used and discussed the same datasets, and\\nquirks of the dataset are known. Y ou might even find open source scripts to process\\nand feed the data directly into your models.\\n18 | Chapter 1: Overview of Machine Learning Systems', '24 Andrej Karpathy, “Building the Software 2.0 Stack, ” Spark+AI Summit 2018, video, 17:54,\\nhttps://oreil.ly/Z21Oz.\\nIn production, data, if available, is a lot more messy. It’s noisy, possibly unstructured,\\nconstantly shifting. It’s likely biased, and you likely don’t know how it’s biased. Labels,\\nif there are any, might be sparse, imbalanced, or incorrect. Changing project or\\nbusiness requirements might require updating some or all of your existing labels. If\\nyou work with users’ data, you’ll also have to worry about privacy and regulatory\\nconcerns. We’ll discuss a case study where users’ data is inadequately handled in the\\nsection “Case study II: The danger of “anonymized” data” on page 344.\\nIn research, you mostly work with historical data, e.g., data that already exists and is\\nstored somewhere. In production, most likely you’ll also have to work with data that\\nis being constantly generated by users, systems, and third-party data.\\nFigure 1-5 has been adapted from a great graphic by Andrej Karpathy, director of AI\\nat Tesla, that illustrates the data problems he encountered during his PhD compared\\nto his time at Tesla.\\nFigure 1-5. Data in research versus data in production. Source: Adapted from an image\\nby Andrej Karpathy24\\nFairness\\nDuring the research phase, a model is not yet used on people, so it’s easy for research‐\\ners to put off fairness as an afterthought: “Let’s try to get state of the art first and\\nworry about fairness when we get to production. ” When it gets to production, it’s\\ntoo late. If you optimize your models for better accuracy or lower latency, you can\\nshow that your models beat state of the art. But, as of writing this book, there’s no\\nequivalent state of the art for fairness metrics.\\nUnderstanding Machine Learning Systems | 19', '25 Khristopher J. Brooks, “Disparity in Home Lending Costs Minorities Millions, Researchers Find, ” CBS News,\\nNovember 15, 2019, https://oreil.ly/UiHUB.\\n26 Cathy O’Neil, Weapons of Math Destruction (New Y ork: Crown Books, 2016).\\n27 Stanford University Human-Centered Artificial Intelligence (HAI), The 2019 AI Index Report, 2019,\\nhttps://oreil.ly/xs8mG.\\nY ou or someone in your life might already be a victim of biased mathematical\\nalgorithms without knowing it. Y our loan application might be rejected because\\nthe ML algorithm picks on your zip code, which embodies biases about one’s soci‐\\noeconomic background. Y our resume might be ranked lower because the ranking\\nsystem employers use picks on the spelling of your name. Y our mortgage might\\nget a higher interest rate because it relies partially on credit scores, which favor the\\nrich and punish the poor. Other examples of ML biases in the real world are in\\npredictive policing algorithms, personality tests administered by potential employers,\\nand college rankings.\\nIn 2019, “Berkeley researchers found that both face-to-face and online lenders\\nrejected a total of 1.3 million creditworthy Black and Latino applicants between\\n2008 and 2015. ” When the researchers “used the income and credit scores of the\\nrejected applications but deleted the race identifiers, the mortgage application was\\naccepted. ”25 For even more galling examples, I recommend Cathy O’Neil’s Weapons of\\nMath Destruction.26\\nML algorithms don’t predict the future, but encode the past, thus perpetuating the\\nbiases in the data and more. When ML algorithms are deployed at scale, they can\\ndiscriminate against people at scale. If a human operator might only make sweeping\\njudgments about a few individuals at a time, an ML algorithm can make sweeping\\njudgments about millions in split seconds. This can especially hurt members of\\nminority groups because misclassification on them could only have a minor effect on\\nmodels’ overall performance metrics.\\nIf an algorithm can already make correct predictions on 98% of the population,\\nand improving the predictions on the other 2% would incur multiples of cost, some\\ncompanies might, unfortunately, choose not to do it. During a McKinsey & Company\\nresearch study in 2019, only 13% of the large companies surveyed said they are\\ntaking steps to mitigate risks to equity and fairness, such as algorithmic bias and\\ndiscrimination.27 However, this is changing rapidly. We’ll cover fairness and other', 'research study in 2019, only 13% of the large companies surveyed said they are\\ntaking steps to mitigate risks to equity and fairness, such as algorithmic bias and\\ndiscrimination.27 However, this is changing rapidly. We’ll cover fairness and other\\naspects of responsible AI in Chapter 11.\\nInterpretability\\nIn early 2020, the Turing Award winner Professor Geoffrey Hinton proposed a\\nheatedly debated question about the importance of interpretability in ML systems.\\n“Suppose you have cancer and you have to choose between a black box AI surgeon\\n20 | Chapter 1: Overview of Machine Learning Systems', '28 Tweet by Geoffrey Hinton (@geoffreyhinton), February 20, 2020, https://oreil.ly/KdfD8.\\n29 For certain use cases in certain countries, users have a “right to explanation”: a right to be given an explana‐\\ntion for an output of the algorithm.\\n30 Stanford HAI, The 2019 AI Index Report.\\nthat cannot explain how it works but has a 90% cure rate and a human surgeon with\\nan 80% cure rate. Do you want the AI surgeon to be illegal?”28\\nA couple of weeks later, when I asked this question to a group of 30 technology\\nexecutives at public nontech companies, only half of them would want the highly\\neffective but unable-to-explain AI surgeon to operate on them. The other half wanted\\nthe human surgeon.\\nWhile most of us are comfortable with using a microwave without understanding\\nhow it works, many don’t feel the same way about AI yet, especially if that AI makes\\nimportant decisions about their lives.\\nSince most ML research is still evaluated on a single objective, model performance,\\nresearchers aren’t incentivized to work on model interpretability. However, interpret‐\\nability isn’t just optional for most ML use cases in the industry, but a requirement.\\nFirst, interpretability is important for users, both business leaders and end users, to\\nunderstand why a decision is made so that they can trust a model and detect potential\\nbiases mentioned previously. 29 Second, it’s important for developers to be able to\\ndebug and improve a model.\\nJust because interpretability is a requirement doesn’t mean everyone is doing it. As of\\n2019, only 19% of large companies are working to improve the explainability of their\\nalgorithms.30\\nDiscussion\\nSome might argue that it’s OK to know only the academic side of ML because there\\nare plenty of jobs in research. The first part—it’s OK to know only the academic side\\nof ML—is true. The second part is false.\\nWhile it’s important to pursue pure research, most companies can’t afford it unless it\\nleads to short-term business applications. This is especially true now that the research\\ncommunity took the “bigger, better” approach. Oftentimes, new models require a\\nmassive amount of data and tens of millions of dollars in compute alone.\\nAs ML research and off-the-shelf models become more accessible, more people and\\norganizations would want to find applications for them, which increases the demand\\nfor ML in production.\\nThe vast majority of ML-related jobs will be, and already are, in productionizing ML.\\nUnderstanding Machine Learning Systems | 21', '31 Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song, “Targeted Backdoor Attacks on Deep\\nLearning Systems Using Data Poisoning, ” arXiv, December 15, 2017, https://oreil.ly/OkAjb.\\n32 We’ll cover edge devices in Chapter 7.\\nMachine Learning Systems Versus Traditional Software\\nSince ML is part of software engineering (SWE), and software has been successfully\\nused in production for more than half a century, some might wonder why we don’t\\njust take tried-and-true best practices in software engineering and apply them to ML.\\nThat’s an excellent idea. In fact, ML production would be a much better place if ML\\nexperts were better software engineers. Many traditional SWE tools can be used to\\ndevelop and deploy ML applications.\\nHowever, many challenges are unique to ML applications and require their own tools.\\nIn SWE, there’s an underlying assumption that code and data are separated. In fact, in\\nSWE, we want to keep things as modular and separate as possible (see the Wikipedia\\npage on separation of concerns).\\nOn the contrary, ML systems are part code, part data, and part artifacts created\\nfrom the two. The trend in the last decade shows that applications developed with\\nthe most/best data win. Instead of focusing on improving ML algorithms, most\\ncompanies will focus on improving their data. Because data can change quickly, ML\\napplications need to be adaptive to the changing environment, which might require\\nfaster development and deployment cycles.\\nIn traditional SWE, you only need to focus on testing and versioning your code. With\\nML, we have to test and version our data too, and that’s the hard part. How to version\\nlarge datasets? How to know if a data sample is good or bad for your system? Not\\nall data samples are equal—some are more valuable to your model than others. For\\nexample, if your model has already trained on one million scans of normal lungs and\\nonly one thousand scans of cancerous lungs, a scan of a cancerous lung is much more\\nvaluable than a scan of a normal lung. Indiscriminately accepting all available data\\nmight hurt your model’s performance and even make it susceptible to data poisoning\\nattacks.31\\nThe size of ML models is another challenge. As of 2022, it’s common for ML models\\nto have hundreds of millions, if not billions, of parameters, which requires gigabytes\\nof random-access memory (RAM) to load them into memory. A few years from now,\\na billion parameters might seem quaint—like, “Can you believe the computer that', 'to have hundreds of millions, if not billions, of parameters, which requires gigabytes\\nof random-access memory (RAM) to load them into memory. A few years from now,\\na billion parameters might seem quaint—like, “Can you believe the computer that\\nsent men to the moon only had 32 MB of RAM?”\\nHowever, for now, getting these large models into production, especially on edge\\ndevices,32 is a massive engineering challenge. Then there is the question of how to get\\nthese models to run fast enough to be useful. An autocompletion model is useless if\\n22 | Chapter 1: Overview of Machine Learning Systems', '33 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of Deep Bidirec‐\\ntional Transformers for Language Understanding, ” arXiv, October 11, 2018, https://oreil.ly/TG3ZW.\\n34 Google Search On, 2020, https://oreil.ly/M7YjM.\\nthe time it takes to suggest the next character is longer than the time it takes for you\\nto type.\\nMonitoring and debugging these models in production is also nontrivial. As ML\\nmodels get more complex, coupled with the lack of visibility into their work, it’s hard\\nto figure out what went wrong or be alerted quickly enough when things go wrong.\\nThe good news is that these engineering challenges are being tackled at a breakneck\\npace. Back in 2018, when the Bidirectional Encoder Representations from Transform‐\\ners (BERT) paper first came out, people were talking about how BERT was too big,\\ntoo complex, and too slow to be practical. The pretrained large BERT model has\\n340 million parameters and is 1.35 GB. 33 Fast-forward two years later, BERT and its\\nvariants were already used in almost every English search on Google.34\\nSummary\\nThis opening chapter aimed to give readers an understanding of what it takes to bring\\nML into the real world. We started with a tour of the wide range of use cases of ML\\nin production today. While most people are familiar with ML in consumer-facing\\napplications, the majority of ML use cases are for enterprise. We also discussed when\\nML solutions would be appropriate. Even though ML can solve many problems\\nvery well, it can’t solve all the problems and it’s certainly not appropriate for all the\\nproblems. However, for problems that ML can’t solve, it’s possible that ML can be one\\npart of the solution.\\nThis chapter also highlighted the differences between ML in research and ML in pro‐\\nduction. The differences include the stakeholder involvement, computational priority,\\nthe properties of data used, the gravity of fairness issues, and the requirements for\\ninterpretability. This section is the most helpful to those coming to ML production\\nfrom academia. We also discussed how ML systems differ from traditional software\\nsystems, which motivated the need for this book.\\nML systems are complex, consisting of many different components. Data scientists\\nand ML engineers working with ML systems in production will likely find that\\nfocusing only on the ML algorithms part is far from enough. It’s important to know\\nabout other aspects of the system, including the data stack, deployment, monitoring,', 'and ML engineers working with ML systems in production will likely find that\\nfocusing only on the ML algorithms part is far from enough. It’s important to know\\nabout other aspects of the system, including the data stack, deployment, monitoring,\\nmaintenance, infrastructure, etc. This book takes a system approach to developing\\nML systems, which means that we’ll consider all components of a system holistically\\ninstead of just looking at ML algorithms. We’ll provide detail on what this holistic\\napproach means in the next chapter.\\nSummary | 23', 'CHAPTER 2\\nIntroduction to Machine Learning\\nSystems Design\\nNow that we’ve walked through an overview of ML systems in the real world, we\\ncan get to the fun part of actually designing an ML system. To reiterate from the\\nfirst chapter, ML systems design takes a system approach to MLOps, which means\\nthat we’ll consider an ML system holistically to ensure that all the components—the\\nbusiness requirements, the data stack, infrastructure, deployment, monitoring, etc.—\\nand their stakeholders can work together to satisfy the specified objectives and\\nrequirements.\\nWe’ll start the chapter with a discussion on objectives. Before we develop an ML\\nsystem, we must understand why this system is needed. If this system is built for a\\nbusiness, it must be driven by business objectives, which will need to be translated\\ninto ML objectives to guide the development of ML models.\\nOnce everyone is on board with the objectives for our ML system, we’ll need to set\\nout some requirements to guide the development of this system. In this book, we’ll\\nconsider the four requirements: reliability, scalability, maintainability, and adaptabil‐\\nity. We will then introduce the iterative process for designing systems to meet those\\nrequirements.\\nY ou might wonder: with all these objectives, requirements, and processes in place,\\ncan I finally start building my ML model yet? Not so soon! Before using ML algo‐\\nrithms to solve your problem, you first need to frame your problem into a task that\\nML can solve. We’ll continue this chapter with how to frame your ML problems. The\\ndifficulty of your job can change significantly depending on how you frame your\\nproblem.\\n25', '1 Eugene Y an has a great post on how data scientists can understand the business intent and context of the\\nprojects they work on.\\n2 Milton Friedman, “ A Friedman Doctrine—The Social Responsibility of Business Is to Increase Its Profits, ”\\nNew York Times Magazine, September 13, 1970, https://oreil.ly/Fmbem.\\nBecause ML is a data-driven approach, a book on ML systems design will be amiss if\\nit fails to discuss the importance of data in ML systems. The last part of this chapter\\ntouches on a debate that has consumed much of the ML literature in recent years:\\nwhich is more important—data or intelligent algorithms?\\nLet’s get started!\\nBusiness and ML Objectives\\nWe first need to consider the objectives of the proposed ML projects. When working\\non an ML project, data scientists tend to care about the ML objectives: the metrics\\nthey can measure about the performance of their ML models such as accuracy, F1\\nscore, inference latency, etc. They get excited about improving their model’s accuracy\\nfrom 94% to 94.2% and might spend a ton of resources—data, compute, and engi‐\\nneering time—to achieve that.\\nBut the truth is: most companies don’t care about the fancy ML metrics. They don’t\\ncare about increasing a model’s accuracy from 94% to 94.2% unless it moves some\\nbusiness metrics. A pattern I see in many short-lived ML projects is that the data\\nscientists become too focused on hacking ML metrics without paying attention to\\nbusiness metrics. Their managers, however, only care about business metrics and,\\nafter failing to see how an ML project can help push their business metrics, kill the\\nprojects prematurely (and possibly let go of the data science team involved).1\\nSo what metrics do companies care about? While most companies want to convince\\nyou otherwise, the sole purpose of businesses, according to the Nobel-winning econ‐\\nomist Milton Friedman, is to maximize profits for shareholders.2\\nThe ultimate goal of any project within a business is, therefore, to increase profits,\\neither directly or indirectly: directly such as increasing sales (conversion rates) and\\ncutting costs; indirectly such as higher customer satisfaction and increasing time\\nspent on a website.\\nFor an ML project to succeed within a business organization, it’s crucial to tie the\\nperformance of an ML system to the overall business performance. What business\\nperformance metrics is the new ML system supposed to influence, e.g., the amount of\\nads revenue, the number of monthly active users?', 'For an ML project to succeed within a business organization, it’s crucial to tie the\\nperformance of an ML system to the overall business performance. What business\\nperformance metrics is the new ML system supposed to influence, e.g., the amount of\\nads revenue, the number of monthly active users?\\n26 | Chapter 2: Introduction to Machine Learning Systems Design', '3 We’ll cover batch prediction and online prediction in Chapter 7.\\n4 Ashok Chandrashekar, Fernando Amat, Justin Basilico, and Tony Jebara, “ Artwork Personalization at Netflix, ”\\nNetflix Technology Blog, December 7, 2017, https://oreil.ly/UEDmw.\\n5 Carlos A. Gomez-Uribe and Neil Hunt, “The Netflix Recommender System: Algorithms, Business Value,\\nand Innovation, ” ACM Transactions on Management Information Systems 6, no. 4 (January 2016): 13,\\nhttps://oreil.ly/JkEPB.\\nImagine that you work for an ecommerce site that cares about purchase-through rate\\nand you want to move your recommender system from batch prediction to online\\nprediction.3 Y ou might reason that online prediction will enable recommendations\\nmore relevant to users right now, which can lead to a higher purchase-through rate.\\nY ou can even do an experiment to show that online prediction can improve your\\nrecommender system’s predictive accuracy by X% and, historically on your site, each\\npercent increase in the recommender system’s predictive accuracy led to a certain\\nincrease in purchase-through rate.\\nOne of the reasons why predicting ad click-through rates and fraud detection are\\namong the most popular use cases for ML today is that it’s easy to map ML models’\\nperformance to business metrics: every increase in click-through rate results in actual\\nad revenue, and every fraudulent transaction stopped results in actual money saved.\\nMany companies create their own metrics to map business metrics to ML metrics.\\nFor example, Netflix measures the performance of their recommender system using\\ntake-rate: the number of quality plays divided by the number of recommendations\\na user sees. 4 The higher the take-rate, the better the recommender system. Netflix\\nalso put a recommender system’s take-rate in the context of their other business\\nmetrics like total streaming hours and subscription cancellation rate. They found that\\na higher take-rate also results in higher total streaming hours and lower subscription\\ncancellation rates.5\\nThe effect of an ML project on business objectives can be hard to reason about. For\\nexample, an ML model that gives customers more personalized solutions can make\\nthem happier, which makes them spend more money on your services. The same ML\\nmodel can also solve their problems faster, which makes them spend less money on\\nyour services.\\nTo gain a definite answer on the question of how ML metrics influence business', 'them happier, which makes them spend more money on your services. The same ML\\nmodel can also solve their problems faster, which makes them spend less money on\\nyour services.\\nTo gain a definite answer on the question of how ML metrics influence business\\nmetrics, experiments are often needed. Many companies do that with experiments\\nlike A/B testing and choose the model that leads to better business metrics, regardless\\nof whether this model has better ML metrics.\\nBusiness and ML Objectives | 27', '6 Parmy Olson, “Nearly Half of All ‘ AI Startups’ Are Cashing In on Hype, ” Forbes, March 4, 2019,\\nhttps://oreil.ly/w5kOr.\\n7 “2020 State of Enterprise Machine Learning, ” Algorithmia, 2020, https://oreil.ly/FlIV1.\\nY et, even rigorous experiments might not be sufficient to understand the relationship\\nbetween an ML model’s outputs and business metrics. Imagine you work for a\\ncybersecurity company that detects and stops security threats, and ML is just a\\ncomponent in their complex process. An ML model is used to detect anomalies in\\nthe traffic pattern. These anomalies then go through a logic set (e.g., a series of\\nif-else statements) that categorizes whether they constitute potential threats. These\\npotential threats are then reviewed by security experts to determine whether they are\\nactual threats. Actual threats will then go through another, different process aimed\\nat stopping them. When this process fails to stop a threat, it might be impossible to\\nfigure out whether the ML component has anything to do with it.\\nMany companies like to say that they use ML in their systems because “being AI-\\npowered” alone already helps them attract customers, regardless of whether the AI\\npart actually does anything useful.6\\nWhen evaluating ML solutions through the business lens, it’s important to be realistic\\nabout the expected returns. Due to all the hype surrounding ML, generated both\\nby the media and by practitioners with a vested interest in ML adoption, some\\ncompanies might have the notion that ML can magically transform their businesses\\novernight.\\nMagically: possible. Overnight: no.\\nThere are many companies that have seen payoffs from ML. For example, ML has\\nhelped Google search better, sell more ads at higher prices, improve translation qual‐\\nity, and build better Android applications. But this gain hardly happened overnight.\\nGoogle has been investing in ML for decades.\\nReturns on investment in ML depend a lot on the maturity stage of adoption. The\\nlonger you’ve adopted ML, the more efficient your pipeline will run, the faster your\\ndevelopment cycle will be, the less engineering time you’ll need, and the lower your\\ncloud bills will be, which all lead to higher returns. According to a 2020 survey by\\nAlgorithmia, among companies that are more sophisticated in their ML adoption\\n(having had models in production for over five years), almost 75% can deploy a\\nmodel in under 30 days. Among those just getting started with their ML pipeline,', 'Algorithmia, among companies that are more sophisticated in their ML adoption\\n(having had models in production for over five years), almost 75% can deploy a\\nmodel in under 30 days. Among those just getting started with their ML pipeline,\\n60% take over 30 days to deploy a model (see Figure 2-1).7\\n28 | Chapter 2: Introduction to Machine Learning Systems Design', 'Figure 2-1. How long it takes for a company to bring a model to production is propor‐\\ntional to how long it has used ML. Source: Adapted from an image by Algorithmia\\nRequirements for ML Systems\\nWe can’t say that we’ve successfully built an ML system without knowing what\\nrequirements the system has to satisfy. The specified requirements for an ML system\\nvary from use case to use case. However, most systems should have these four charac‐\\nteristics: reliability, scalability, maintainability, and adaptability. We’ll walk through\\neach of these concepts in detail. Let’s take a closer look at reliability first.\\nReliability\\nThe system should continue to perform the correct function at the desired level of\\nperformance even in the face of adversity (hardware or software faults, and even\\nhuman error).\\n“Correctness” might be difficult to determine for ML systems. For example, your\\nsystem might call the predict function—e.g., model.predict()—correctly, but the\\npredictions are wrong. How do we know if a prediction is wrong if we don’t have\\nground truth labels to compare it with?\\nWith traditional software systems, you often get a warning, such as a system crash\\nor runtime error or 404. However, ML systems can fail silently. End users don’t even\\nknow that the system has failed and might have kept on using it as if it were working.\\nFor example, if you use Google Translate to translate a sentence into a language you\\ndon’t know, it might be very hard for you to tell even if the translation is wrong. We’ll\\ndiscuss how ML systems fail in production in Chapter 8.\\nRequirements for ML Systems | 29', '8 Up-scaling and down-scaling are two aspects of “scaling out, ” which is different from “scaling up. ” Scaling out\\nis adding more equivalently functional components in parallel to spread out a load. Scaling up is making a\\ncomponent larger or faster to handle a greater load (Leah Schoeb, “Cloud Scalability: Scale Up vs Scale Out, ”\\nTurbonomic Blog, March 15, 2018, https://oreil.ly/CFPtb).\\n9 Sean Wolfe, “ Amazon’s One Hour of Downtime on Prime Day May Have Cost It up to $100 Million in Lost\\nSales, ” Business Insider, July 19, 2018, https://oreil.ly/VBezI.\\nScalability\\nThere are multiple ways an ML system can grow. It can grow in complexity. Last year\\nyou used a logistic regression model that fit into an Amazon Web Services (AWS)\\nfree tier instance with 1 GB of RAM, but this year, you switched to a 100-million-\\nparameter neural network that requires 16 GB of RAM to generate predictions.\\nY our ML system can grow in traffic volume. When you started deploying an ML\\nsystem, you only served 10,000 prediction requests daily. However, as your company’s\\nuser base grows, the number of prediction requests your ML system serves daily\\nfluctuates between 1 million and 10 million.\\nAn ML system might grow in ML model count. Initially, you might have only one\\nmodel for one use case, such as detecting the trending hashtags on a social network\\nsite like Twitter. However, over time, you want to add more features to this use\\ncase, so you’ll add one more to filter out NSFW (not safe for work) content and\\nanother model to filter out tweets generated by bots. This growth pattern is especially\\ncommon in ML systems that target enterprise use cases. Initially, a startup might\\nserve only one enterprise customer, which means this startup only has one model.\\nHowever, as this startup gains more customers, they might have one model for each\\ncustomer. A startup I worked with had 8,000 models in production for their 8,000\\nenterprise customers.\\nWhichever way your system grows, there should be reasonable ways of dealing with\\nthat growth. When talking about scalability most people think of resource scaling,\\nwhich consists of up-scaling (expanding the resources to handle growth) and down-\\nscaling (reducing the resources when not needed).8\\nFor example, at peak, your system might require 100 GPUs (graphics processing\\nunits). However, most of the time, it needs only 10 GPUs. Keeping 100 GPUs up all\\nthe time can be costly, so your system should be able to scale down to 10 GPUs.', 'scaling (reducing the resources when not needed).8\\nFor example, at peak, your system might require 100 GPUs (graphics processing\\nunits). However, most of the time, it needs only 10 GPUs. Keeping 100 GPUs up all\\nthe time can be costly, so your system should be able to scale down to 10 GPUs.\\nAn indispensable feature in many cloud services is autoscaling: automatically scaling\\nup and down the number of machines depending on usage. This feature can be tricky\\nto implement. Even Amazon fell victim to this when their autoscaling feature failed\\non Prime Day, causing their system to crash. An hour of downtime was estimated to\\ncost Amazon between $72 million and $99 million.9\\n30 | Chapter 2: Introduction to Machine Learning Systems Design', 'However, handling growth isn’t just resource scaling, but also artifact management.\\nManaging one hundred models is very different from managing one model. With one\\nmodel, you can, perhaps, manually monitor this model’s performance and manually\\nupdate the model with new data. Since there’s only one model, you can just have a file\\nthat helps you reproduce this model whenever needed. However, with one hundred\\nmodels, both the monitoring and retraining aspect will need to be automated. Y ou’ll\\nneed a way to manage the code generation so that you can adequately reproduce a\\nmodel when you need to.\\nBecause scalability is such an important topic throughout the ML project workflow,\\nwe’ll discuss it in different parts of the book. Specifically, we’ll touch on the resource\\nscaling aspect in the section “Distributed Training” on page 168, the section “Model\\noptimization” on page 216, and the section “Resource Management” on page 311.\\nWe’ll discuss the artifact management aspect in the section “Experiment Tracking and\\nVersioning” on page 162 and the section “Development Environment” on page 302.\\nMaintainability\\nThere are many people who will work on an ML system. They are ML engineers,\\nDevOps engineers, and subject matter experts (SMEs). They might come from very\\ndifferent backgrounds, with very different programming languages and tools, and\\nmight own different parts of the process.\\nIt’s important to structure your workloads and set up your infrastructure in such\\na way that different contributors can work using tools that they are comfortable\\nwith, instead of one group of contributors forcing their tools onto other groups.\\nCode should be documented. Code, data, and artifacts should be versioned. Models\\nshould be sufficiently reproducible so that even when the original authors are not\\naround, other contributors can have sufficient contexts to build on their work. When\\na problem occurs, different contributors should be able to work together to identify\\nthe problem and implement a solution without finger-pointing.\\nWe’ll go more into this in the section “Team Structure” on page 334.\\nAdaptability\\nTo adapt to shifting data distributions and business requirements, the system should\\nhave some capacity for both discovering aspects for performance improvement and\\nallowing updates without service interruption.\\nRequirements for ML Systems | 31', '10 Which, as an early reviewer pointed out, is a property of traditional software.\\n11 Praying and crying not featured, but present through the entire process.\\nBecause ML systems are part code, part data, and data can change quickly, ML\\nsystems need to be able to evolve quickly. This is tightly linked to maintainability.\\nWe’ll discuss changing data distributions in the section “Data Distribution Shifts” on\\npage 237, and how to continually update your model with new data in the section\\n“Continual Learning” on page 264.\\nIterative Process\\nDeveloping an ML system is an iterative and, in most cases, never-ending process. 10\\nOnce a system is put into production, it’ll need to be continually monitored and\\nupdated.\\nBefore deploying my first ML system, I thought the process would be linear and\\nstraightforward. I thought all I had to do was to collect data, train a model, deploy\\nthat model, and be done. However, I soon realized that the process looks more like a\\ncycle with a lot of back and forth between different steps.\\nFor example, here is one workflow that you might encounter when building an ML\\nmodel to predict whether an ad should be shown when users enter a search query:11\\n1. Choose a metric to optimize. For example, you might want to optimize for1.\\nimpressions—the number of times an ad is shown.\\n2. Collect data and obtain labels.2.\\n3. Engineer features.3.\\n4. Train models.4.\\n5. During error analysis, you realize that errors are caused by the wrong labels, so5.\\nyou relabel the data.\\n6. Train the model again.6.\\n7. During error analysis, you realize that your model always predicts that an ad7.\\nshouldn’t be shown, and the reason is because 99.99% of the data you have have\\nNEGATIVE labels (ads that shouldn’t be shown). So you have to collect more\\ndata of ads that should be shown.\\n32 | Chapter 2: Introduction to Machine Learning Systems Design', '8. Train the model again.8.\\n9. The model performs well on your existing test data, which is by now two months9.\\nold. However, it performs poorly on the data from yesterday. Y our model is now\\nstale, so you need to update it on more recent data.\\n10. Train the model again.10.\\n11. Deploy the model.11.\\n12. The model seems to be performing well, but then the businesspeople come12.\\nknocking on your door asking why the revenue is decreasing. It turns out the\\nads are being shown, but few people click on them. So you want to change your\\nmodel to optimize for ad click-through rate instead.\\n13. Go to step 1.13.\\nFigure 2-2  shows an oversimplified representation of what the iterative process for\\ndeveloping ML systems in production looks like from the perspective of a data\\nscientist or an ML engineer. This process looks different from the perspective of\\nan ML platform engineer or a DevOps engineer, as they might not have as much\\ncontext into model development and might spend a lot more time on setting up\\ninfrastructure.\\nFigure 2-2. The process of developing an ML system looks more like a cycle with a lot of\\nback and forth between steps\\nIterative Process | 33', 'Later chapters will dive deeper into what each of these steps requires in practice.\\nHere, let’s take a brief look at what they mean:\\nStep 1. Project scoping\\nA project starts with scoping the project, laying out goals, objectives, and con‐\\nstraints. Stakeholders should be identified and involved. Resources should be\\nestimated and allocated. We already discussed different stakeholders and some of\\nthe foci for ML projects in production in Chapter 1. We also already discussed\\nhow to scope an ML project in the context of a business earlier in this chapter.\\nWe’ll discuss how to organize teams to ensure the success of an ML project in\\nChapter 11.\\nStep 2. Data engineering\\nA vast majority of ML models today learn from data, so developing ML models\\nstarts with engineering data. In Chapter 3, we’ll discuss the fundamentals of data\\nengineering, which covers handling data from different sources and formats.\\nWith access to raw data, we’ll want to curate training data out of it by sampling\\nand generating labels, which is discussed in Chapter 4.\\nStep 3. ML model development\\nWith the initial set of training data, we’ll need to extract features and develop\\ninitial models leveraging these features. This is the stage that requires the most\\nML knowledge and is most often covered in ML courses. In Chapter 5 , we’ll\\ndiscuss feature engineering. In Chapter 6, we’ll discuss model selection, training,\\nand evaluation.\\nStep 4. Deployment\\nAfter a model is developed, it needs to be made accessible to users. Developing\\nan ML system is like writing—you will never reach the point when your system\\nis done. But you do reach the point when you have to put your system out there.\\nWe’ll discuss different ways to deploy an ML model in Chapter 7.\\n34 | Chapter 2: Introduction to Machine Learning Systems Design', 'Step 5. Monitoring and continual learning\\nOnce in production, models need to be monitored for performance decay and\\nmaintained to be adaptive to changing environments and changing requirements.\\nThis step will be discussed in Chapters 8 and 9.\\nStep 6. Business analysis\\nModel performance needs to be evaluated against business goals and analyzed\\nto generate business insights. These insights can then be used to eliminate unpro‐\\nductive projects or scope out new projects. This step is closely related to the first\\nstep.\\nFraming ML Problems\\nImagine you’re an ML engineering tech lead at a bank that targets millennial users.\\nOne day, your boss hears about a rival bank that uses ML to speed up their customer\\nservice support that supposedly helps the rival bank process their customer requests\\ntwo times faster. He orders your team to look into using ML to speed up your\\ncustomer service support too.\\nSlow customer support is a problem, but it’s not an ML problem. An ML problem\\nis defined by inputs, outputs, and the objective function that guides the learning\\nprocess—none of these three components are obvious from your boss’s request. It’s\\nyour job, as a seasoned ML engineer, to use your knowledge of what problems ML\\ncan solve to frame this request as an ML problem.\\nUpon investigation, you discover that the bottleneck in responding to customer\\nrequests lies in routing customer requests to the right department among four\\ndepartments: accounting, inventory, HR (human resources), and IT. Y ou can alleviate\\nthis bottleneck by developing an ML model to predict which of these four depart‐\\nments a request should go to. This makes it a classification problem. The input is\\nthe customer request. The output is the department the request should go to. The\\nobjective function is to minimize the difference between the predicted department\\nand the actual department.\\nWe’ll discuss extensively how to extract features from raw data to input into your ML\\nmodel in Chapter 5. In this section, we’ll focus on two aspects: the output of your\\nmodel and the objective function that guides the learning process.\\nFraming ML Problems | 35', 'Types of ML Tasks\\nThe output of your model dictates the task type of your ML problem. The most\\ngeneral types of ML tasks are classification and regression. Within classification, there\\nare more subtypes, as shown in Figure 2-3. We’ll go over each of these task types.\\nFigure 2-3. Common task types in ML\\nClassification versus regression\\nClassification models classify inputs into different categories. For example, you want\\nto classify each email to be either spam or not spam. Regression models output a\\ncontinuous value. An example is a house prediction model that outputs the price of a\\ngiven house.\\nA regression model can easily be framed as a classification model and vice versa. For\\nexample, house prediction can become a classification task if we quantize the house\\nprices into buckets such as under $100,000, $100,000–$200,000, $200,000–$500,000,\\nand so forth and predict the bucket the house should be in.\\nThe email classification model can become a regression model if we make it output\\nvalues between 0 and 1, and decide on a threshold to determine which values should\\nbe SPAM (for example, if the value is above 0.5, the email is spam), as shown in\\nFigure 2-4.\\n36 | Chapter 2: Introduction to Machine Learning Systems Design', 'Figure 2-4. The email classification task can also be framed as a regression task\\nBinary versus multiclass classification\\nWithin classification problems, the fewer classes there are to classify, the simpler the\\nproblem is. The simplest is binary classification, where there are only two possible\\nclasses. Examples of binary classification include classifying whether a comment is\\ntoxic, whether a lung scan shows signs of cancer, whether a transaction is fraudulent.\\nIt’s unclear whether this type of problem is common in the industry because they\\nare common in nature or simply because ML practitioners are most comfortable\\nhandling them.\\nWhen there are more than two classes, the problem becomes multiclass classification.\\nDealing with binary classification problems is much easier than dealing with multi‐\\nclass problems. For example, calculating F1 and visualizing confusion matrices are a\\nlot more intuitive when there are only two classes.\\nWhen the number of classes is high, such as disease diagnosis where the number\\nof diseases can go up to thousands or product classifications where the number of\\nproducts can go up to tens of thousands, we say the classification task has high\\ncardinality. High cardinality problems can be very challenging. The first challenge is\\nin data collection. In my experience, ML models typically need at least 100 examples\\nfor each class to learn to classify that class. So if you have 1,000 classes, you already\\nneed at least 100,000 examples. The data collection can be especially difficult for rare\\nclasses. When you have thousands of classes, it’s likely that some of them are rare.\\nFraming ML Problems | 37', 'When the number of classes is large, hierarchical classification might be useful. In\\nhierarchical classification, you have a classifier to first classify each example into one\\nof the large groups. Then you have another classifier to classify this example into one\\nof the subgroups. For example, for product classification, you can first classify each\\nproduct into one of the four main categories: electronics, home and kitchen, fashion,\\nor pet supplies. After a product has been classified into a category, say fashion, you\\ncan use another classifier to put this product into one of the subgroups: shoes, shirts,\\njeans, or accessories.\\nMulticlass versus multilabel classification\\nIn both binary and multiclass classification, each example belongs to exactly one\\nclass. When an example can belong to multiple classes, we have a multilabel classifi‐\\ncation problem. For example, when building a model to classify articles into four\\ntopics—tech, entertainment, finance, and politics—an article can be in both tech and\\nfinance.\\nThere are two major approaches to multilabel classification problems. The first is to\\ntreat it as you would a multiclass classification. In multiclass classification, if there\\nare four possible classes [tech, entertainment, finance, politics] and the label for an\\nexample is entertainment, you represent this label with the vector [0, 1, 0, 0]. In\\nmultilabel classification, if an example has both labels entertainment and finance, its\\nlabel will be represented as [0, 1, 1, 0].\\nThe second approach is to turn it into a set of binary classification problems. For the\\narticle classification problem, you can have four models corresponding to four topics,\\neach model outputting whether an article is in that topic or not.\\nOut of all task types, multilabel classification is usually the one that I’ve seen compa‐\\nnies having the most problems with. Multilabel means that the number of classes\\nan example can have varies from example to example. First, this makes it difficult\\nfor label annotation since it increases the label multiplicity problem that we discuss\\nin Chapter 4 . For example, an annotator might believe an example belongs to two\\nclasses while another annotator might believe the same example to belong in only one\\nclass, and it might be difficult resolving their disagreements.\\nSecond, this varying number of classes makes it hard to extract predictions from raw\\nprobability. Consider the same task of classifying articles into four topics. Imagine', 'class, and it might be difficult resolving their disagreements.\\nSecond, this varying number of classes makes it hard to extract predictions from raw\\nprobability. Consider the same task of classifying articles into four topics. Imagine\\nthat, given an article, your model outputs this raw probability distribution: [0.45, 0.2,\\n0.02, 0.33]. In the multiclass setting, when you know that an example can belong to\\nonly one category, you simply pick the category with the highest probability, which\\nis 0.45 in this case. In the multilabel setting, because you don’t know how many\\ncategories an example can belong to, you might pick the two highest probability\\ncategories (corresponding to 0.45 and 0.33) or three highest probability categories\\n(corresponding to 0.45, 0.2, and 0.33).\\n38 | Chapter 2: Introduction to Machine Learning Systems Design', 'Multiple ways to frame a problem\\nChanging the way you frame your problem might make your problem significantly\\nharder or easier. Consider the task of predicting what app a phone user wants to use\\nnext. A naive setup would be to frame this as a multiclass classification task—use\\nthe user’s and environment’s features (user demographic information, time, location,\\nprevious apps used) as input, and output a probability distribution for every single\\napp on the user’s phone. Let N be the number of apps you want to consider recom‐\\nmending to a user. In this framing, for a given user at a given time, there is only\\none prediction to make, and the prediction is a vector of the size N. This setup is\\nvisualized in Figure 2-5.\\nFigure 2-5. Given the problem of predicting the app a user will most likely open next,\\nyou can frame it as a classification problem. The input is the user’s features and environ‐\\nment’s features. The output is a distribution over all apps on the phone.\\nThis is a bad approach because whenever a new app is added, you might have to\\nretrain your model from scratch, or at least retrain all the components of your model\\nwhose number of parameters depends on N. A better approach is to frame this as a\\nregression task. The input is the user’s, the environment’s, and the app’s features. The\\noutput is a single value between 0 and 1; the higher the value, the more likely the\\nuser will open the app given the context. In this framing, for a given user at a given\\ntime, there are N predictions to make, one for each app, but each prediction is just a\\nnumber. This improved setup is visualized in Figure 2-6.\\nFraming ML Problems | 39', '12 Note that objective functions are mathematical functions, which are different from the business and ML\\nobjectives we discussed earlier in this chapter.\\nFigure 2-6. Given the problem of predicting the app a user will most likely open next,\\nyou can frame it as a regression problem. The input is the user’s features, environment’s\\nfeatures, and an app’s features. The output is a single value between 0 and 1 denoting\\nhow likely the user will be to open the app given the context.\\nIn this new framing, whenever there’s a new app you want to consider recommending\\nto a user, you simply need to use new inputs with this new app’s feature instead of\\nhaving to retrain your model or part of your model from scratch.\\nObjective Functions\\nTo learn, an ML model needs an objective function to guide the learning process. 12\\nAn objective function is also called a loss function, because the objective of the learn‐\\ning process is usually to minimize (or optimize) the loss caused by wrong predictions.\\nFor supervised ML, this loss can be computed by comparing the model’s outputs with\\nthe ground truth labels using a measurement like root mean squared error (RMSE) or\\ncross entropy.\\nTo illustrate this point, let’s again go back to the previous task of classifying articles\\ninto four topics [tech, entertainment, finance, politics]. Consider an article that\\nbelongs to the politics class, e.g., its ground truth label is [0, 0, 0, 1]. Imagine that,\\ngiven this article, your model outputs this raw probability distribution: [0.45, 0.2,\\n0.02, 0.33]. The cross entropy loss of this model, given this example, is the cross\\nentropy of [0.45, 0.2, 0.02, 0.33] relative to [0, 0, 0, 1]. In Python, you can calculate\\ncross entropy with the following code:\\n40 | Chapter 2: Introduction to Machine Learning Systems Design', '13 Joe Kukura, “Facebook Employee Raises Powered by ‘Really Dangerous’ Algorithm That Favors Angry Posts, ”\\nSFist, September 24, 2019, https://oreil.ly/PXtGi; Kevin Roose, “The Making of a Y ouTube Radical, ” New York\\nTimes, June 8, 2019, https://oreil.ly/KYqzF.\\nimport numpy as np\\ndef cross_entropy(p, q):\\nreturn -sum([p[i] * np.log(q[i]) for i in range(len(p))])\\np = [0, 0, 0, 1]\\nq = [0.45, 0.2, 0.02, 0.33]\\ncross_entropy(p, q)\\nChoosing an objective function is usually straightforward, though not because objec‐\\ntive functions are easy. Coming up with meaningful objective functions requires\\nalgebra knowledge, so most ML engineers just use common loss functions like RMSE\\nor MAE (mean absolute error) for regression, logistic loss (also log loss) for binary\\nclassification, and cross entropy for multiclass classification.\\nDecoupling objectives\\nFraming ML problems can be tricky when you want to minimize multiple objective\\nfunctions. Imagine you’re building a system to rank items on users’ newsfeeds. Y our\\noriginal goal is to maximize users’ engagement. Y ou want to achieve this goal through\\nthe following three objectives:\\n• Filter out spam•\\n• Filter out NSFW content•\\n• Rank posts by engagement: how likely users will click on it•\\nHowever, you quickly learned that optimizing for users’ engagement alone can lead to\\nquestionable ethical concerns. Because extreme posts tend to get more engagements,\\nyour algorithm learned to prioritize extreme content. 13 Y ou want to create a more\\nwholesome newsfeed. So you have a new goal: maximize users’ engagement while\\nminimizing the spread of extreme views and misinformation. To obtain this goal, you\\nadd two new objectives to your original plan:\\n• Filter out spam•\\n• Filter out NSFW content•\\n• Filter out misinformation•\\n• Rank posts by quality•\\n• Rank posts by engagement: how likely users will click on it•\\nFraming ML Problems | 41', '14 For simplicity, let’s pretend for now that we know how to measure a post’s quality.\\n15 Wikipedia, s.v. “Pareto optimization, ” https://oreil.ly/NdApy. While you’re at it, you might also want to read\\nJin and Sendhoff ’s great paper on applying Pareto optimization for ML, in which the authors claimed that\\n“machine learning is inherently a multiobjective task” (Y aochu Jin and Bernhard Sendhoff, “Pareto-Based\\nMultiobjective Machine Learning: An Overview and Case Studies, ” IEEE Transactions on Systems, Man, and\\nCybernetics—Part C: Applications and Reviews 38, no. 3 [May 2008], https://oreil.ly/f1aKk).\\nNow two objectives are in conflict with each other. If a post is engaging but it’s of\\nquestionable quality, should that post rank high or low?\\nAn objective is represented by an objective function. To rank posts by quality, you\\nfirst need to predict posts’ quality, and you want posts’ predicted quality to be as close\\nto their actual quality as possible. Essentially, you want to minimize quality_loss: the\\ndifference between each post’s predicted quality and its true quality.14\\nSimilarly, to rank posts by engagement, you first need to predict the number of clicks\\neach post will get. Y ou want to minimize engagement_loss: the difference between\\neach post’s predicted clicks and its actual number of clicks.\\nOne approach is to combine these two losses into one loss and train one model to\\nminimize that loss:\\nloss = ɑ quality_loss + β engagement_loss\\nY ou can randomly test out different values of α and β to find the values that work\\nbest. If you want to be more systematic about tuning these values, you can check out\\nPareto optimization, “an area of multiple criteria decision making that is concerned\\nwith mathematical optimization problems involving more than one objective func‐\\ntion to be optimized simultaneously. ”15\\nA problem with this approach is that each time you tune α and β—for example, if the\\nquality of your users’ newsfeeds goes up but users’ engagement goes down, you might\\nwant to decrease α and increase β—you’ll have to retrain your model.\\nAnother approach is to train two different models, each optimizing one loss. So you\\nhave two models:\\nquality_model\\nMinimizes quality_loss and outputs the predicted quality of each post\\nengagement_model\\nMinimizes engagement_loss and outputs the predicted number of clicks of each\\npost\\n42 | Chapter 2: Introduction to Machine Learning Systems Design', '16 Anand Rajaraman, “More Data Usually Beats Better Algorithms, ” Datawocky, March 24, 2008,\\nhttps://oreil.ly/wNwhV.\\n17 Rich Sutton, “The Bitter Lesson, ” March 13, 2019, https://oreil.ly/RhOp9.\\n18 Tweet by Dr. Judea Pearl (@yudapearl), September 27, 2020, https://oreil.ly/wFbHb.\\nY ou can combine the models’ outputs and rank posts by their combined scores:\\nɑ quality_score + β engagement_score\\nNow you can tweak α and β without retraining your models!\\nIn general, when there are multiple objectives, it’s a good idea to decouple them\\nfirst because it makes model development and maintenance easier. First, it’s easier to\\ntweak your system without retraining models, as previously explained. Second, it’s\\neasier for maintenance since different objectives might need different maintenance\\nschedules. Spamming techniques evolve much faster than the way post quality is\\nperceived, so spam filtering systems need updates at a much higher frequency than\\nquality-ranking systems.\\nMind Versus Data\\nProgress in the last decade shows that the success of an ML system depends largely\\non the data it was trained on. Instead of focusing on improving ML algorithms, most\\ncompanies focus on managing and improving their data.16\\nDespite the success of models using massive amounts of data, many are skeptical of\\nthe emphasis on data as the way forward. In the last five years, at every academic\\nconference I attended, there were always some public debates on the power of mind\\nversus data. Mind might be disguised as inductive biases or intelligent architectural\\ndesigns. Data might be grouped together with computation since more data tends to\\nrequire more computation.\\nIn theory, you can both pursue architectural designs and leverage large data and\\ncomputation, but spending time on one often takes time away from another.17\\nIn the mind-over-data camp, there’s Dr. Judea Pearl, a Turing Award winner best\\nknown for his work on causal inference and Bayesian networks. The introduction\\nto his book The Book of Why  is entitled “Mind over Data, ” in which he emphasizes:\\n“Data is profoundly dumb. ” In one of his more controversial posts on Twitter in 2020,\\nhe expressed his strong opinion against ML approaches that rely heavily on data and\\nwarned that data-centric ML people might be out of a job in three to five years: “ML\\nwill not be the same in 3–5 years, and ML folks who continue to follow the current\\ndata-centric paradigm will find themselves outdated, if not jobless. Take note. ”18', 'warned that data-centric ML people might be out of a job in three to five years: “ML\\nwill not be the same in 3–5 years, and ML folks who continue to follow the current\\ndata-centric paradigm will find themselves outdated, if not jobless. Take note. ”18\\nMind Versus Data | 43', '19 “Deep Learning and Innate Priors” (Chris Manning versus Y ann LeCun debate), February 2, 2018, video,\\n1:02:55, https://oreil.ly/b3hb1.\\n20 Sutton, “The Bitter Lesson. ”\\n21 Alon Halevy, Peter Norvig, and Fernando Pereira, “The Unreasonable Effectiveness of Data, ” IEEE Computer\\nSociety, March/April 2009, https://oreil.ly/WkN6p.\\nThere’s also a milder opinion from Professor Christopher Manning, director of the\\nStanford Artificial Intelligence Laboratory, who argued that huge computation and\\na massive amount of data with a simple learning algorithm create incredibly bad\\nlearners. The structure allows us to design systems that can learn more from less\\ndata.19\\nMany people in ML today are in the data-over-mind camp. Professor Richard Sutton,\\na professor of computing science at the University of Alberta and a distinguished\\nresearch scientist at DeepMind, wrote a great blog post in which he claimed that\\nresearchers who chose to pursue intelligent designs over methods that leverage com‐\\nputation will eventually learn a bitter lesson: “The biggest lesson that can be read\\nfrom 70 years of AI research is that general methods that leverage computation are\\nultimately the most effective, and by a large margin.… Seeking an improvement that\\nmakes a difference in the shorter term, researchers seek to leverage their human\\nknowledge of the domain, but the only thing that matters in the long run is the\\nleveraging of computation. ”20\\nWhen asked how Google Search was doing so well, Peter Norvig, Google’s director\\nof search quality, emphasized the importance of having a large amount of data over\\nintelligent algorithms in their success: “We don’t have better algorithms. We just have\\nmore data. ”21\\nDr. Monica Rogati, former VP of data at Jawbone, argued that data lies at the\\nfoundation of data science, as shown in Figure 2-7. If you want to use data science, a\\ndiscipline of which ML is a part of, to improve your products or processes, you need\\nto start with building out your data, both in terms of quality and quantity. Without\\ndata, there’s no data science.\\nThe debate isn’t about whether finite data is necessary, but whether it’s sufficient. The\\nterm finite here is important, because if we had infinite data, it might be possible for\\nus to look up the answer. Having a lot of data is different from having infinite data.\\n44 | Chapter 2: Introduction to Machine Learning Systems Design', '22 Monica Rogati, “The AI Hierarchy of Needs, ” Hackernoon Newsletter, June 12, 2017, https://oreil.ly/3nxJ8.\\n23 Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson,\\n“One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling, ” arXiv, December\\n11, 2013, https://oreil.ly/1AdO6.\\nFigure 2-7. The data science hierarchy of needs. Source: Adapted from an image by\\nMonica Rogati22\\nRegardless of which camp will prove to be right eventually, no one can deny that\\ndata is essential, for now. Both the research and industry trends in the recent decades\\nshow the success of ML relies more and more on the quality and quantity of data.\\nModels are getting bigger and using more data. Back in 2013, people were getting\\nexcited when the One Billion Word Benchmark for Language Modeling was released,\\nwhich contains 0.8 billion tokens. 23 Six years later, OpenAI’s GPT-2 used a dataset of\\n10 billion tokens. And another year later, GPT-3 used 500 billion tokens. The growth\\nrate of the sizes of datasets is shown in Figure 2-8.\\nMind Versus Data | 45', 'Figure 2-8. The size of the datasets (log scale) used for language models over time\\nEven though much of the progress in deep learning in the last decade was fueled\\nby an increasingly large amount of data, more data doesn’t always lead to better\\nperformance for your model. More data at lower quality, such as data that is outdated\\nor data with incorrect labels, might even hurt your model’s performance.\\nSummary\\nI hope that this chapter has given you an introduction to ML systems design and the\\nconsiderations we need to take into account when designing an ML system.\\nEvery project must start with why this project needs to happen, and ML projects are\\nno exception. We started the chapter with an assumption that most businesses don’t\\ncare about ML metrics unless they can move business metrics. Therefore, if an ML\\nsystem is built for a business, it must be motivated by business objectives, which need\\nto be translated into ML objectives to guide the development of ML models.\\n46 | Chapter 2: Introduction to Machine Learning Systems Design', '24 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton, “ImageNet Classification with Deep Convolutional\\nNeural Networks, ” in Advances in Neural Information Processing Systems, vol. 25, ed. F . Pereira, C.J. Burges,\\nL. Bottou, and K.Q. Weinberger (Curran Associates, 2012), https://oreil.ly/MFYp9; Jacob Devlin, Ming-Wei\\nChang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding, ” arXiv, 2019, https://oreil.ly/TN8fN; “Better Language Models and Their Implica‐\\ntions, ” OpenAI blog, February 14, 2019, https://oreil.ly/SGV7g.\\nBefore building an ML system, we need to understand the requirements that the\\nsystem needs to meet to be considered a good system. The exact requirements vary\\nfrom use case to use case, and in this chapter, we focused on the four most general\\nrequirements: reliability, scalability, maintainability, and adaptability. Techniques to\\nsatisfy each of these requirements will be covered throughout the book.\\nBuilding an ML system isn’t a one-off task but an iterative process. In this chapter,\\nwe discussed the iterative process to develop an ML system that met those preceding\\nrequirements.\\nWe ended the chapter on a philosophical discussion of the role of data in ML systems.\\nThere are still many people who believe that having intelligent algorithms will eventu‐\\nally trump having a large amount of data. However, the success of systems including\\nAlexNet, BERT, and GPT showed that the progress of ML in the last decade relies on\\nhaving access to a large amount of data. 24 Regardless of whether data can overpower\\nintelligent design, no one can deny the importance of data in ML. A nontrivial part of\\nthis book will be devoted to shedding light on various data questions.\\nComplex ML systems are made up of simpler building blocks. Now that we’ve cov‐\\nered the high-level overview of an ML system in production, we’ll zoom in to its\\nbuilding blocks in the following chapters, starting with the fundamentals of data\\nengineering in the next chapter. If any of the challenges mentioned in this chapter\\nseem abstract to you, I hope that specific examples in the following chapters will\\nmake them more concrete.\\nSummary | 47', 'CHAPTER 3\\nData Engineering Fundamentals\\nThe rise of ML in recent years is tightly coupled with the rise of big data. Large\\ndata systems, even without ML, are complex. If you haven’t spent years and years\\nworking with them, it’s easy to get lost in acronyms. There are many challenges and\\npossible solutions that these systems generate. Industry standards, if there are any,\\nevolve quickly as new tools come out and the needs of the industry expand, creating a\\ndynamic and ever-changing environment. If you look into the data stack for different\\ntech companies, it might seem like each is doing its own thing.\\nIn this chapter, we’ll cover the basics of data engineering that will, hopefully, give you\\na steady piece of land to stand on as you explore the landscape for your own needs.\\nWe’ll start with different sources of data that you might work with in a typical ML\\nproject. We’ll continue to discuss the formats in which data can be stored. Storing\\ndata is only interesting if you intend on retrieving that data later. To retrieve stored\\ndata, it’s important to know not only how it’s formatted but also how it’s structured.\\nData models define how the data stored in a particular data format is structured.\\nIf data models describe the data in the real world, databases specify how the data\\nshould be stored on machines. We’ll continue to discuss data storage engines,\\nalso known as databases, for the two major types of processing: transactional and\\nanalytical.\\nWhen working with data in production, you usually work with data across multiple\\nprocesses and services. For example, you might have a feature engineering service\\nthat computes features from raw data, and a prediction service to generate predic‐\\ntions based on computed features. This means that you’ll have to pass computed fea‐\\ntures from the feature engineering service to the prediction service. In the following\\nsection of the chapter, we’ll discuss different modes of data passing across processes.\\n49', 'During the discussion of different modes of data passing, we’ll learn about two\\ndistinct types of data: historical data in data storage engines, and streaming data in\\nreal-time transports. These two different types of data require different processing\\nparadigms, which we’ll discuss in the section “Batch Processing Versus Stream Pro‐\\ncessing” on page 78.\\nKnowing how to collect, process, store, retrieve, and process an increasingly growing\\namount of data is essential to people who want to build ML systems in production.\\nIf you’re already familiar with data systems, you might want to move directly to\\nChapter 4 to learn more about how to sample and generate labels to create training\\ndata. If you want to learn more about data engineering from a systems perspective,\\nI recommend Martin Kleppmann’s excellent book Designing Data-Intensive Applica‐\\ntions (O’Reilly, 2017).\\nData Sources\\nAn ML system can work with data from many different sources. They have different\\ncharacteristics, can be used for different purposes, and require different processing\\nmethods. Understanding the sources your data comes from can help you use your\\ndata more efficiently. This section aims to give a quick overview of different data\\nsources to those unfamiliar with data in production. If you’ve already worked with\\nML in production for a while, feel free to skip this section.\\nOne source is user input data , data explicitly input by users. User input can be\\ntext, images, videos, uploaded files, etc. If it’s even remotely possible for users to\\ninput wrong data, they are going to do it. As a result, user input data can be easily\\nmalformatted. Text might be too long or too short. Where numerical values are\\nexpected, users might accidentally enter text. If you let users upload files, they might\\nupload files in the wrong formats. User input data requires more heavy-duty checking\\nand processing.\\nOn top of that, users also have little patience. In most cases, when we input data, we\\nexpect to get results back immediately. Therefore, user input data tends to require fast\\nprocessing.\\nAnother source is system-generated data . This is the data generated by different\\ncomponents of your systems, which include various types of logs and system outputs\\nsuch as model predictions.\\n50 | Chapter 3: Data Engineering Fundamentals', '1 “Interesting” in production usually means catastrophic, such as a crash or when your cloud bill hits an\\nastronomical amount.\\n2 As of November 2021, AWS S3 Standard, the storage option that allows you to access your data with the\\nlatency of milliseconds, costs about five times more per GB than S3 Glacier, the storage option that allows you\\nto retrieve your data with a latency from between 1 minute to 12 hours.\\n3 An ML engineer once mentioned to me that his team only used users’ historical product browsing and\\npurchases to make recommendations on what they might like to see next. I responded: “So you don’t use\\npersonal data at all?” He looked at me, confused. “If you meant demographic data like users’ age, location,\\nthen no, we don’t. But I’ d say that a person’s browsing and purchasing activities are extremely personal. ”\\nLogs can record the state and significant events of the system, such as memory usage,\\nnumber of instances, services called, packages used, etc. They can record the results\\nof different jobs, including large batch jobs for data processing and model training.\\nThese types of logs provide visibility into how the system is doing. The main purpose\\nof this visibility is for debugging and potentially improving the application. Most of\\nthe time, you don’t have to look at these types of logs, but they are essential when\\nsomething is on fire.\\nBecause logs are system generated, they are much less likely to be malformatted\\nthe way user input data is. Overall, logs don’t need to be processed as soon as they\\narrive, the way you would want to process user input data. For many use cases, it’s\\nacceptable to process logs periodically, such as hourly or even daily. However, you\\nmight still want to process your logs fast to be able to detect and be notified whenever\\nsomething interesting happens.1\\nBecause debugging ML systems is hard, it’s a common practice to log everything you\\ncan. This means that your volume of logs can grow very, very quickly. This leads to\\ntwo problems. The first is that it can be hard to know where to look because signals\\nare lost in the noise. There have been many services that process and analyze logs,\\nsuch as Logstash, Datadog, Logz.io, etc. Many of them use ML models to help you\\nprocess and make sense of your massive number of logs.\\nThe second problem is how to store a rapidly growing number of logs. Luckily, in\\nmost cases, you only have to store logs for as long as they are useful and can discard', 'process and make sense of your massive number of logs.\\nThe second problem is how to store a rapidly growing number of logs. Luckily, in\\nmost cases, you only have to store logs for as long as they are useful and can discard\\nthem when they are no longer relevant for you to debug your current system. If you\\ndon’t have to access your logs frequently, they can also be stored in low-access storage\\nthat costs much less than higher-frequency-access storage.2\\nThe system also generates data to record users’ behaviors, such as clicking, choosing a\\nsuggestion, scrolling, zooming, ignoring a pop-up, or spending an unusual amount of\\ntime on certain pages. Even though this is system-generated data, it’s still considered\\npart of user data and might be subject to privacy regulations.3\\nData Sources | 51', '4 John Koetsier, “ Apple Just Crippled IDFA, Sending an $80 Billion Industry Into Upheaval, ” Forbes, June 24,\\n2020, https://oreil.ly/rqPX9.\\n5 Patrick McGee and Yuan Y ang, “TikTok Wants to Keep Tracking iPhone Users with State-Backed Work‐\\naround, ” Ars Technica, March 16, 2021, https://oreil.ly/54pkg.\\nThere are also internal databases, generated by various services and enterprise appli‐\\ncations in a company. These databases manage their assets such as inventory, cus‐\\ntomer relationship, users, and more. This kind of data can be used by ML models\\ndirectly or by various components of an ML system. For example, when users enter\\na search query on Amazon, one or more ML models process that query to detect\\nits intention—if someone types in “frozen, ” are they looking for frozen foods or\\nDisney’s Frozen franchise?—then Amazon needs to check its internal databases for\\nthe availability of these products before ranking them and showing them to users.\\nThen there’s the wonderfully weird world of third-party data. First-party data is the\\ndata that your company already collects about your users or customers. Second-party\\ndata is the data collected by another company on their own customers that they\\nmake available to you, though you’ll probably have to pay for it. Third-party data\\ncompanies collect data on the public who aren’t their direct customers.\\nThe rise of the internet and smartphones has made it much easier for all types of\\ndata to be collected. It used to be especially easy with smartphones since each phone\\nused to have a unique advertiser ID—iPhones with Apple’s Identifier for Advertisers\\n(IDFA) and Android phones with their Android Advertising ID (AAID)—which\\nacted as a unique ID to aggregate all activities on a phone. Data from apps, websites,\\ncheck-in services, etc. are collected and (hopefully) anonymized to generate activity\\nhistory for each person.\\nData of all kinds can be bought, such as social media activities, purchase history, web\\nbrowsing habits, car rentals, and political leaning for different demographic groups\\ngetting as granular as men, age 25–34, working in tech, living in the Bay Area. From\\nthis data, you can infer information such as people who like brand A also like brand\\nB. This data can be especially helpful for systems such as recommender systems to\\ngenerate results relevant to users’ interests. Third-party data is usually sold after being\\ncleaned and processed by vendors.', 'this data, you can infer information such as people who like brand A also like brand\\nB. This data can be especially helpful for systems such as recommender systems to\\ngenerate results relevant to users’ interests. Third-party data is usually sold after being\\ncleaned and processed by vendors.\\nHowever, as users demand more data privacy, companies have been taking steps\\nto curb the usage of advertiser IDs. In early 2021, Apple made their IDFA opt-in.\\nThis change has reduced significantly the amount of third-party data available on\\niPhones, forcing many companies to focus more on first-party data. 4 To fight back\\nthis change, advertisers have been investing in workarounds. For example, the China\\nAdvertising Association, a state-supported trade association for China’s advertising\\nindustry, invested in a device fingerprinting system called CAID that allowed apps\\nlike TikTok and Tencent to keep tracking iPhone users.5\\n52 | Chapter 3: Data Engineering Fundamentals', '6 “ Access pattern” means the pattern in which a system or program reads or writes data.\\nData Formats\\nOnce you have data, you might want to store it (or “persist” it, in technical terms).\\nSince your data comes from multiple sources with different access patterns, 6 storing\\nyour data isn’t always straightforward and, for some cases, can be costly. It’s important\\nto think about how the data will be used in the future so that the format you use will\\nmake sense. Here are some of the questions you might want to consider:\\n• How do I store multimodal data, e.g., a sample that might contain both images•\\nand texts?\\n• Where do I store my data so that it’s cheap and still fast to access?•\\n• How do I store complex models so that they can be loaded and run correctly on•\\ndifferent hardware?\\nThe process of converting a data structure or object state into a format that can be\\nstored or transmitted and reconstructed later is data serialization. There are many,\\nmany data serialization formats. When considering a format to work with, you\\nmight want to consider different characteristics such as human readability, access\\npatterns, and whether it’s based on text or binary, which influences the size of its files.\\nTable 3-1 consists of just a few of the common formats that you might encounter in\\nyour work. For a more comprehensive list, check out the wonderful Wikipedia page\\n“Comparison of Data-Serialization Formats”.\\nTable 3-1. Common data formats and where they are used\\nFormat Binary/Text Human-readable Example use cases\\nJSON Text Yes Everywhere\\nCSV Text Yes Everywhere\\nParquet Binary No Hadoop, Amazon Redshift\\nAvro Binary primary No Hadoop\\nProtobuf Binary primary No Google, TensorFlow (TFRecord)\\nPickle Binary No Python, PyTorch serialization\\nWe’ll go over a few of these formats, starting with JSON. We’ll also go over the two\\nformats that are common and represent two distinct paradigms: CSV and Parquet.\\nData Formats | 53', 'JSON\\nJSON, JavaScript Object Notation, is everywhere. Even though it was derived from\\nJavaScript, it’s language-independent—most modern programming languages can\\ngenerate and parse JSON. It’s human-readable. Its key-value pair paradigm is simple\\nbut powerful, capable of handling data of different levels of structuredness. For\\nexample, your data can be stored in a structured format like the following:\\n{\\n  \"firstName\": \"Boatie\",\\n  \"lastName\": \"McBoatFace\",\\n  \"isVibing\": true,\\n  \"age\": 12,\\n  \"address\": {\\n    \"streetAddress\": \"12 Ocean Drive\",\\n    \"city\": \"Port Royal\",\\n    \"postalCode\": \"10021-3100\"\\n  }\\n}\\nThe same data can also be stored in an unstructured blob of text like the following:\\n{\\n  \"text\": \"Boatie McBoatFace, aged 12, is vibing, at 12 Ocean Drive, Port Royal, \\n           10021-3100\"\\n}\\nBecause JSON is ubiquitous, the pain it causes can also be felt everywhere. Once\\nyou’ve committed the data in your JSON files to a schema, it’s pretty painful to\\nretrospectively go back to change the schema. JSON files are text files, which means\\nthey take up a lot of space, as we’ll see in the section “Text Versus Binary Format” on\\npage 57.\\nRow-Major Versus Column-Major Format\\nThe two formats that are common and represent two distinct paradigms are CSV\\nand Parquet. CSV (comma-separated values) is row-major, which means consecutive\\nelements in a row are stored next to each other in memory. Parquet is column-major,\\nwhich means consecutive elements in a column are stored next to each other.\\nBecause modern computers process sequential data more efficiently than nonsequen‐\\ntial data, if a table is row-major, accessing its rows will be faster than accessing its\\ncolumns in expectation. This means that for row-major formats, accessing data by\\nrows is expected to be faster than accessing data by columns.\\n54 | Chapter 3: Data Engineering Fundamentals', 'Imagine we have a dataset of 1,000 examples, and each example has 10 features. If\\nwe consider each example as a row and each feature as a column, as is often the case\\nin ML, then the row-major formats like CSV are better for accessing examples, e.g.,\\naccessing all the examples collected today. Column-major formats like Parquet are\\nbetter for accessing features, e.g., accessing the timestamps of all your examples. See\\nFigure 3-1.\\nFigure 3-1. Row-major versus column-major formats\\nColumn-major formats allow flexible column-based reads, especially if your data is\\nlarge with thousands, if not millions, of features. Consider if you have data about\\nride-sharing transactions that has 1,000 features but you only want 4 features: time,\\nlocation, distance, price. With column-major formats, you can read the four columns\\ncorresponding to these four features directly. However, with row-major formats, if\\nyou don’t know the sizes of the rows, you will have to read in all columns then filter\\ndown to these four columns. Even if you know the sizes of the rows, it can still be\\nslow as you’ll have to jump around the memory, unable to take advantage of caching.\\nRow-major formats allow faster data writes. Consider the situation when you have to\\nkeep adding new individual examples to your data. For each individual example, it’ d\\nbe much faster to write it to a file where your data is already in a row-major format.\\nOverall, row-major formats are better when you have to do a lot of writes, whereas\\ncolumn-major ones are better when you have to do a lot of column-based reads.\\nData Formats | 55', '7 For more pandas quirks, check out my Just pandas Things GitHub repository.\\nNumPy Versus pandas\\nOne subtle point that a lot of people don’t pay attention to, which leads to misuses of\\npandas, is that this library is built around the columnar format.\\npandas is built around DataFrame, a concept inspired by R’s Data Frame, which is\\ncolumn-major. A DataFrame is a two-dimensional table with rows and columns.\\nIn NumPy, the major order can be specified. When an ndarray is created, it’s row-\\nmajor by default if you don’t specify the order. People coming to pandas from NumPy\\ntend to treat DataFrame the way they would ndarray, e.g., trying to access data by\\nrows, and find DataFrame slow.\\nIn the left panel of Figure 3-2, you can see that accessing a DataFrame by row is so\\nmuch slower than accessing the same DataFrame by column. If you convert this same\\nDataFrame to a NumPy ndarray, accessing a row becomes much faster, as you can see\\nin the right panel of the figure.7\\nFigure 3-2. (Left) Iterating a pandas DataFrame by column takes 0.07 seconds but\\niterating the same DataFrame by row takes 2.41 seconds. (Right) When you convert the\\nsame DataFrame into a NumPy ndarray, accessing its rows becomes much faster.\\n56 | Chapter 3: Data Engineering Fundamentals', '8 “ Announcing Amazon Redshift Data Lake Export: Share Data in Apache Parquet Format, ” Amazon AWS,\\nDecember 3, 2019, https://oreil.ly/ilDb6.\\nI use CSV as an example of the row-major format because it’s\\npopular and generally recognizable by everyone I’ve talked to in\\ntech. However, some of the early reviewers of this book pointed\\nout that they believe CSV to be a horrible data format. It serializes\\nnontext characters poorly. For example, when you write float values\\nto a CSV file, some precision might be lost—0.12345678901232323\\ncould be arbitrarily rounded up as “0.12345678901”—as com‐\\nplained about in a Stack Overflow thread  and Microsoft Com‐\\nmunity thread . People on Hacker News have passionately argued\\nagainst using CSV .\\nText Versus Binary Format\\nCSV and JSON are text files, whereas Parquet files are binary files. Text files are\\nfiles that are in plain text, which usually means they are human-readable. Binary\\nfiles are the catchall that refers to all nontext files. As the name suggests, binary files\\nare typically files that contain only 0s and 1s, and are meant to be read or used by\\nprograms that know how to interpret the raw bytes. A program has to know exactly\\nhow the data inside the binary file is laid out to make use of the file. If you open text\\nfiles in your text editor (e.g., VS Code, Notepad), you’ll be able to read the texts in\\nthem. If you open a binary file in your text editor, you’ll see blocks of numbers, likely\\nin hexadecimal values, for corresponding bytes of the file.\\nBinary files are more compact. Here’s a simple example to show how binary files\\ncan save space compared to text files. Consider that you want to store the number\\n1000000. If you store it in a text file, it’ll require 7 characters, and if each character is\\n1 byte, it’ll require 7 bytes. If you store it in a binary file as int32, it’ll take only 32 bits\\nor 4 bytes.\\nAs an illustration, I use interviews.csv, which is a CSV file (text format) of 17,654 rows\\nand 10 columns. When I converted it to a binary format (Parquet), the file size went\\nfrom 14 MB to 6 MB, as shown in Figure 3-3.\\nAWS recommends using the Parquet format because “the Parquet format is up to 2x\\nfaster to unload and consumes up to 6x less storage in Amazon S3, compared to text\\nformats. ”8\\nData Formats | 57', 'Figure 3-3. When stored in CSV format, my interview file is 14 MB. But when stored in\\nParquet, the same file is 6 MB.\\nData Models\\nData models describe how data is represented. Consider cars in the real world. In\\na database, a car can be described using its make, its model, its year, its color, and\\nits price. These attributes make up a data model for cars. Alternatively, you can also\\ndescribe a car using its owner, its license plate, and its history of registered addresses.\\nThis is another data model for cars.\\n58 | Chapter 3: Data Engineering Fundamentals', '9 Edgar F . Codd, “ A Relational Model of Data for Large Shared Data Banks, ” Communications of the ACM 13,\\nno. 6 (June 1970): 377–87.\\n10 For detail-oriented readers, not all tables are relations.\\nHow you choose to represent data not only affects the way your systems are built, but\\nalso the problems your systems can solve. For example, the way you represent cars in\\nthe first data model makes it easier for people looking to buy cars, whereas the second\\ndata model makes it easier for police officers to track down criminals.\\nIn this section, we’ll study two types of models that seem opposite to each other but\\nare actually converging: relational models and NoSQL models. We’ll go over examples\\nto show the types of problems each model is suited for.\\nRelational Model\\nRelational models are among the most persistent ideas in computer science. Invented\\nby Edgar F . Codd in 1970, 9 the relational model is still going strong today, even\\ngetting more popular. The idea is simple but powerful. In this model, data is organ‐\\nized into relations; each relation is a set of tuples. A table is an accepted visual\\nrepresentation of a relation, and each row of a table makes up a tuple, 10 as shown\\nin Figure 3-4. Relations are unordered. Y ou can shuffle the order of the rows or the\\norder of the columns in a relation and it’s still the same relation. Data following the\\nrelational model is usually stored in file formats like CSV or Parquet.\\nFigure 3-4. In a relation, the order of neither the rows nor the columns matters\\nIt’s often desirable for relations to be normalized. Data normalization can follow\\nnormal forms such as the first normal form (1NF), second normal form (2NF), etc.,\\nand readers interested can read more about it on Wikipedia. In this book, we’ll go\\nthrough an example to show how normalization works and how it can reduce data\\nredundancy and improve data integrity.\\nConsider the relation Book shown in Table 3-2. There are a lot of duplicates in this\\ndata. For example, rows 1 and 2 are nearly identical, except for format and price.\\nIf the publisher information changes—for example, its name changes from “Banana\\nData Models | 59', '11 Y ou can further normalize the Book relation, such as separating format into a separate relation.\\nPress” to “Pineapple Press”—or its country changes, we’ll have to update rows 1, 2,\\nand 4. If we separate publisher information into its own table, as shown in Tables\\n3-3 and 3-4, when a publisher’s information changes, we only have to update the\\nPublisher relation.11 This practice allows us to standardize spelling of the same value\\nacross different columns. It also makes it easier to make changes to these values,\\neither because these values change or when you want to translate them into different\\nlanguages.\\nTable 3-2. Initial Book relation\\nTitle Author Format Publisher Country Price\\nHarry Potter J.K. Rowling Paperback Banana Press UK $20\\nHarry Potter J.K. Rowling E-book Banana Press UK $10\\nSherlock Holmes Conan Doyle Paperback Guava Press US $30\\nThe Hobbit J.R.R. Tolkien Paperback Banana Press UK $30\\nSherlock Holmes Conan Doyle Paperback Guava Press US $15\\nTable 3-3. Updated Book relation\\nTitle Author Format Publisher ID Price\\nHarry Potter J.K. Rowling Paperback 1 $20\\nHarry Potter J.K. Rowling E-book 1 $10\\nSherlock Holmes Conan Doyle Paperback 2 $30\\nThe Hobbit J.R.R. Tolkien Paperback 1 $30\\nSherlock Holmes Conan Doyle Paperback 2 $15\\nTable 3-4. Publisher relation\\nPublisher ID Publisher Country\\n1 Banana Press UK\\n2 Guava Press US\\nOne major downside of normalization is that your data is now spread across multiple\\nrelations. Y ou can join the data from different relations back together, but joining can\\nbe expensive for large tables.\\nDatabases built around the relational data model are relational databases. Once\\nyou’ve put data in your databases, you’ll want a way to retrieve it. The language\\nthat you can use to specify the data that you want from a database is called a\\nquery language. The most popular query language for relational databases today is\\nSQL. Even though inspired by the relational model, the data model behind SQL has\\ndeviated from the original relational model . For example, SQL tables can contain\\n60 | Chapter 3: Data Engineering Fundamentals', '12 Greg Kemnitz, a coauthor of the original Postgres paper, shared on Quora that he once wrote a reporting SQL\\nquery that was 700 lines long and visited 27 different tables in lookups or joins. The query had about 1,000\\nlines of comments to help him remember what he was doing. It took him three days to compose, debug, and\\ntune.\\n13 Y annis E. Ioannidis, “Query Optimization, ” ACM Computing Surveys (CSUR) 28, no. 1 (1996): 121–23,\\nhttps://oreil.ly/omXMg\\n14 Ryan Marcus et al., “Neo: A Learned Query Optimizer, ” arXiv preprint arXiv:1904.03711 (2019),\\nhttps://oreil.ly/wHy6p.\\nrow duplicates, whereas true relations can’t contain duplicates. However, this subtle\\ndifference has been safely ignored by most people.\\nThe most important thing to note about SQL is that it’s a declarative language, as\\nopposed to Python, which is an imperative language. In the imperative paradigm, you\\nspecify the steps needed for an action and the computer executes these steps to return\\nthe outputs. In the declarative paradigm, you specify the outputs you want, and the\\ncomputer figures out the steps needed to get you the queried outputs.\\nWith an SQL database, you specify the pattern of data you want—the tables you want\\nthe data from, the conditions the results must meet, the basic data transformations\\nsuch as join, sort, group, aggregate, etc.—but not how to retrieve the data. It is up\\nto the database system to decide how to break the query into different parts, what\\nmethods to use to execute each part of the query, and the order in which different\\nparts of the query should be executed.\\nWith certain added features, SQL can be Turing-complete, which means that, in\\ntheory, SQL can be used to solve any computation problem (without making any\\nguarantee about the time or memory required). However, in practice, it’s not always\\neasy to write a query to solve a specific task, and it’s not always feasible or tractable to\\nexecute a query. Anyone working with SQL databases might have nightmarish mem‐\\nories of painfully long SQL queries that are impossible to understand and nobody\\ndares to touch for fear that things might break.12\\nFiguring out how to execute an arbitrary query is the hard part, which is the job\\nof query optimizers. A query optimizer examines all possible ways to execute a\\nquery and finds the fastest way to do so. 13 It’s possible to use ML to improve query\\noptimizers based on learning from incoming queries. 14 Query optimization is one', 'of query optimizers. A query optimizer examines all possible ways to execute a\\nquery and finds the fastest way to do so. 13 It’s possible to use ML to improve query\\noptimizers based on learning from incoming queries. 14 Query optimization is one\\nof the most challenging problems in database systems, and normalization means\\nthat data is spread out on multiple relations, which makes joining it together even\\nharder. Even though developing a query optimizer is hard, the good news is that you\\ngenerally only need one query optimizer and all your applications can leverage it.\\nData Models | 61', '15 Matthias Boehm, Alexandre V . Evfimievski, Niketan Pansare, and Berthold Reinwald, “Declarative Machine\\nLearning—A Classification of Basic Properties and Types, ” arXiv, May 19, 2016, https://oreil.ly/OvW07.\\nFrom Declarative Data Systems to Declarative ML Systems\\nPossibly inspired by the success of declarative data systems, many people have looked\\nforward to declarative ML. 15 With a declarative ML system, users only need to\\ndeclare the features’ schema and the task, and the system will figure out the best\\nmodel to perform that task with the given features. Users won’t have to write code\\nto construct, train, and tune models. Popular frameworks for declarative ML are\\nLudwig, developed at Uber, and H2O AutoML . In Ludwig, users can specify the\\nmodel structure—such as the number of fully connected layers and the number of\\nhidden units—on top of the features’ schema and output. In H2O AutoML, you don’t\\nneed to specify the model structure or hyperparameters. It experiments with multiple\\nmodel architectures and picks out the best model given the features and the task.\\nHere is an example to show how H2O AutoML works. Y ou give the system your data\\n(inputs and outputs) and specify the number of models you want to experiment. It’ll\\nexperiment with that number of models and show you the best-performing model:\\n# Identify predictors and response\\nx = train.columns\\ny = \"response\"\\nx.remove(y)\\n# For binary classification, response should be a factor\\ntrain[y] = train[y].asfactor()\\ntest[y] = test[y].asfactor()\\n# Run AutoML for 20 base models\\naml = H2OAutoML(max_models=20, seed=1)\\naml.train(x=x, y=y, training_frame=train)\\n# Show the best-performing models on the AutoML Leaderboard\\nlb = aml.leaderboard\\n# Get the best-performing model\\naml.leader\\nWhile declarative ML can be useful in many cases, it leaves unanswered the biggest\\nchallenges with ML in production. Declarative ML systems today abstract away the\\nmodel development part, and as we’ll cover in the next six chapters, with models\\nbeing increasingly commoditized, model development is often the easier part. The\\nhard part lies in feature engineering, data processing, model evaluation, data shift\\ndetection, continual learning, and so on.\\n62 | Chapter 3: Data Engineering Fundamentals', '16 James Phillips, “Surprises in Our NoSQL Adoption Survey, ” Couchbase, December 16, 2014,\\nhttps://oreil.ly/ueyEX.\\n17 Martin Kleppmann, Designing Data-Intensive Applications (Sebastopol, CA: O’Reilly, 2017).\\nNoSQL\\nThe relational data model has been able to generalize to a lot of use cases, from\\necommerce to finance to social networks. However, for certain use cases, this model\\ncan be restrictive. For example, it demands that your data follows a strict schema,\\nand schema management is painful. In a survey by Couchbase in 2014, frustration\\nwith schema management was the #1 reason for the adoption of their nonrelational\\ndatabase.16 It can also be difficult to write and execute SQL queries for specialized\\napplications.\\nThe latest movement against the relational data model is NoSQL. Originally started\\nas a hashtag for a meetup to discuss nonrelational databases, NoSQL has been retro‐\\nactively reinterpreted as Not Only SQL, 17 as many NoSQL data systems also support\\nrelational models. Two major types of nonrelational models are the document model\\nand the graph model. The document model targets use cases where data comes in\\nself-contained documents and relationships between one document and another are\\nrare. The graph model goes in the opposite direction, targeting use cases where\\nrelationships between data items are common and important. We’ll examine each of\\nthese two models, starting with the document model.\\nDocument model\\nThe document model is built around the concept of “document. ” A document is often\\na single continuous string, encoded as JSON, XML, or a binary format like BSON\\n(Binary JSON). All documents in a document database are assumed to be encoded\\nin the same format. Each document has a unique key that represents that document,\\nwhich can be used to retrieve it.\\nA collection of documents could be considered analogous to a table in a relational\\ndatabase, and a document analogous to a row. In fact, you can convert a relation into\\na collection of documents that way. For example, you can convert the book data in\\nTables 3-3 and 3-4 into three JSON documents as shown in Examples 3-1, 3-2, and\\n3-3. However, a collection of documents is much more flexible than a table. All rows\\nin a table must follow the same schema (e.g., have the same sequence of columns),\\nwhile documents in the same collection can have completely different schemas.\\nData Models | 63', 'Example 3-1. Document 1: harry_potter.json\\n{\\n  \"Title\": \"Harry Potter\",\\n  \"Author\": \"J .K. Rowling\",\\n  \"Publisher\": \"Banana Press\",\\n  \"Country\": \"UK\",\\n  \"Sold as\": [\\n    {\"Format\": \"Paperback\", \"Price\": \"$20\"},\\n    {\"Format\": \"E-book\", \"Price\": \"$10\"}\\n  ]\\n} \\nExample 3-2. Document 2: sherlock_holmes.json\\n{\\n  \"Title\": \"Sherlock Holmes\",\\n  \"Author\": \"Conan Doyle\",\\n  \"Publisher\": \"Guava Press\",\\n  \"Country\": \"US\",\\n  \"Sold as\": [\\n    {\"Format\": \"Paperback\", \"Price\": \"$30\"},\\n    {\"Format\": \"E-book\", \"Price\": \"$15\"}\\n  ]\\n} \\nExample 3-3. Document 3: the_hobbit.json\\n{\\n  \"Title\": \"The Hobbit\",\\n  \"Author\": \"J.R.R. Tolkien\",\\n  \"Publisher\": \"Banana Press\",\\n  \"Country\": \"UK\",\\n  \"Sold as\": [\\n    {\"Format\": \"Paperback\", \"Price\": \"$30\"},\\n  ]\\n} \\nBecause the document model doesn’t enforce a schema, it’s often referred to as sche‐\\nmaless. This is misleading because, as discussed previously, data stored in documents\\nwill be read later. The application that reads the documents usually assumes some\\nkind of structure of the documents. Document databases just shift the responsibility\\nof assuming structures from the application that writes the data to the application\\nthat reads the data.\\nThe document model has better locality than the relational model. Consider the book\\ndata example in Tables 3-3 and 3-4 where the information about a book is spread\\nacross both the Book table and the Publisher table (and potentially also the Format\\ntable). To retrieve information about a book, you’ll have to query multiple tables.\\n64 | Chapter 3: Data Engineering Fundamentals', 'In the document model, all information about a book can be stored in a document,\\nmaking it much easier to retrieve.\\nHowever, compared to the relational model, it’s harder and less efficient to execute\\njoins across documents compared to across tables. For example, if you want to find\\nall books whose prices are below $25, you’ll have to read all documents, extract the\\nprices, compare them to $25, and return all the documents containing the books with\\nprices below $25.\\nBecause of the different strengths of the document and relational data models, it’s\\ncommon to use both models for different tasks in the same database systems. More\\nand more database systems, such as PostgreSQL and MySQL, support them both.\\nGraph model\\nThe graph model is built around the concept of a “graph. ” A graph consists of nodes\\nand edges, where the edges represent the relationships between the nodes. A database\\nthat uses graph structures to store its data is called a graph database. If in document\\ndatabases, the content of each document is the priority, then in graph databases, the\\nrelationships between data items are the priority.\\nBecause the relationships are modeled explicitly in graph models, it’s faster to retrieve\\ndata based on relationships. Consider an example of a graph database in Figure 3-5.\\nThe data from this example could potentially come from a simple social network. In\\nthis graph, nodes can be of different data types: person, city, country, company, etc.\\nFigure 3-5. An example of a simple graph database\\nImagine you want to find everyone who was born in the USA. Given this graph, you\\ncan start from the node USA and traverse the graph following the edges “within”\\nand “born_in” to find all the nodes of the type “person. ” Now, imagine that instead\\nof using the graph model to represent this data, we use the relational model. There’ d\\nbe no easy way to write an SQL query to find everyone who was born in the USA,\\nData Models | 65', '18 In this specific example, replacing the null age values with –1 solved the problem.\\nespecially given that there are an unknown number of hops between country and\\nperson—there are three hops between Zhenzhong Xu and USA while there are only\\ntwo hops between Chloe He and USA. Similarly, there’ d be no easy way for this type\\nof query with a document database.\\nMany queries that are easy to do in one data model are harder to do in another data\\nmodel. Picking the right data model for your application can make your life so much\\neasier.\\nStructured Versus Unstructured Data\\nStructured data follows a predefined data model, also known as a data schema. For\\nexample, the data model might specify that each data item consists of two values: the\\nfirst value, “name, ” is a string of at most 50 characters, and the second value, “age, ” is\\nan 8-bit integer in the range between 0 and 200. The predefined structure makes your\\ndata easier to analyze. If you want to know the average age of people in the database,\\nall you have to do is to extract all the age values and average them out.\\nThe disadvantage of structured data is that you have to commit your data to a prede‐\\nfined schema. If your schema changes, you’ll have to retrospectively update all your\\ndata, often causing mysterious bugs in the process. For example, you’ve never kept\\nyour users’ email addresses before but now you do, so you have to retrospectively\\nupdate email information to all previous users. One of the strangest bugs one of my\\ncolleagues encountered was when they could no longer use users’ ages with their\\ntransactions, and their data schema replaced all the null ages with 0, and their ML\\nmodel thought the transactions were made by people 0 years old.18\\nBecause business requirements change over time, committing to a predefined data\\nschema can become too restricting. Or you might have data from multiple data\\nsources that are beyond your control, and it’s impossible to make them follow the\\nsame schema. This is where unstructured data becomes appealing. Unstructured data\\ndoesn’t adhere to a predefined data schema. It’s usually text but can also be numbers,\\ndates, images, audio, etc. For example, a text file of logs generated by your ML model\\nis unstructured data.\\nEven though unstructured data doesn’t adhere to a schema, it might still contain\\nintrinsic patterns that help you extract structures. For example, the following text\\nis unstructured, but you can notice the pattern that each line contains two values', 'is unstructured data.\\nEven though unstructured data doesn’t adhere to a schema, it might still contain\\nintrinsic patterns that help you extract structures. For example, the following text\\nis unstructured, but you can notice the pattern that each line contains two values\\nseparated by a comma, the first value is textual, and the second value is numerical.\\nHowever, there is no guarantee that all lines must follow this format. Y ou can add a\\nnew line to that text even if that line doesn’t follow this format.\\n66 | Chapter 3: Data Engineering Fundamentals', 'Lisa, 43\\nJack, 23\\nHuyen, 59\\nUnstructured data also allows for more flexible storage options. For example, if your\\nstorage follows a schema, you can only store data following that schema. But if your\\nstorage doesn’t follow a schema, you can store any type of data. Y ou can convert all\\nyour data, regardless of types and formats, into bytestrings and store them together.\\nA repository for storing structured data is called a data warehouse. A repository for\\nstoring unstructured data is called a data lake. Data lakes are usually used to store\\nraw data before processing. Data warehouses are used to store data that has been\\nprocessed into formats ready to be used. Table 3-5 shows a summary of the key\\ndifferences between structured and unstructured data.\\nTable 3-5. The key differences between structured and unstructured data\\nStructured data Unstructured data\\nSchema clearly defined Data doesn’t have to follow a schema\\nEasy to search and analyze Fast arrival\\nCan only handle data with a specific schema Can handle data from any source\\nSchema changes will cause a lot of troubles No need to worry about schema changes (yet), as the worry is shifted to the\\ndownstream applications that use this data\\nStored in data warehouses Stored in data lakes\\nData Storage Engines and Processing\\nData formats and data models specify the interface for how users can store and\\nretrieve data. Storage engines, also known as databases, are the implementation of\\nhow data is stored and retrieved on machines. It’s useful to understand different types\\nof databases as your team or your adjacent team might need to select a database\\nappropriate for your application.\\nTypically, there are two types of workloads that databases are optimized for, transac‐\\ntional processing and analytical processing, and there’s a big difference between them,\\nwhich we’ll cover in this section. We will then cover the basics of the ETL (extract,\\ntransform, load) process that you will inevitably encounter when building an ML\\nsystem in production.\\nTransactional and Analytical Processing\\nTraditionally, a transaction refers to the action of buying or selling something. In\\nthe digital world, a transaction refers to any kind of action: tweeting, ordering a\\nride through a ride-sharing service, uploading a new model, watching a Y ouTube\\nvideo, and so on. Even though these different transactions involve different types of\\nData Storage Engines and Processing | 67', '19 This paragraph, as well as many parts of this chapter, is inspired by Martin Kleppmann’s Designing Data-\\nIntensive Applications.\\n20 Kleppmann, Designing Data-Intensive Applications.\\ndata, the way they’re processed is similar across applications. The transactions are\\ninserted as they are generated, and occasionally updated when something changes, or\\ndeleted when they are no longer needed. 19 This type of processing is known as online\\ntransaction processing (OLTP).\\nBecause these transactions often involve users, they need to be processed fast (low\\nlatency) so that they don’t keep users waiting. The processing method needs to have\\nhigh availability—that is, the processing system needs to be available any time a\\nuser wants to make a transaction. If your system can’t process a transaction, that\\ntransaction won’t go through.\\nTransactional databases are designed to process online transactions and satisfy the\\nlow latency, high availability requirements. When people hear transactional data‐\\nbases, they usually think of ACID (atomicity, consistency, isolation, durability). Here\\nare their definitions for those needing a quick reminder:\\nAtomicity\\nTo guarantee that all the steps in a transaction are completed successfully as\\na group. If any step in the transaction fails, all other steps must fail also. For\\nexample, if a user’s payment fails, you don’t want to still assign a driver to that\\nuser.\\nConsistency\\nTo guarantee that all the transactions coming through must follow predefined\\nrules. For example, a transaction must be made by a valid user.\\nIsolation\\nTo guarantee that two transactions happen at the same time as if they were\\nisolated. Two users accessing the same data won’t change it at the same time. For\\nexample, you don’t want two users to book the same driver at the same time.\\nDurability\\nTo guarantee that once a transaction has been committed, it will remain commit‐\\nted even in the case of a system failure. For example, after you’ve ordered a ride\\nand your phone dies, you still want your ride to come.\\nHowever, transactional databases don’t necessarily need to be ACID, and some devel‐\\nopers find ACID to be too restrictive. According to Martin Kleppmann, “systems that\\ndo not meet the ACID criteria are sometimes called BASE, which stands for Basically\\nAvailable, Soft state, and Eventual consistency. This is even more vague than the\\ndefinition of ACID. ”20\\n68 | Chapter 3: Data Engineering Fundamentals', 'Because each transaction is often processed as a unit separately from other transac‐\\ntions, transactional databases are often row-major. This also means that transactional\\ndatabases might not be efficient for questions such as “What’s the average price for\\nall the rides in September in San Francisco?” This kind of analytical question requires\\naggregating data in columns across multiple rows of data. Analytical databases are\\ndesigned for this purpose. They are efficient with queries that allow you to look\\nat data from different viewpoints. We call this type of processing online analytical\\nprocessing (OLAP).\\nHowever, both the terms OLTP and OLAP have become outdated, as shown in\\nFigure 3-6, for three reasons. First, the separation of transactional and analytical data‐\\nbases was due to limitations of technology—it was hard to have databases that could\\nhandle both transactional and analytical queries efficiently. However, this separation\\nis being closed. Today, we have transactional databases that can handle analytical\\nqueries, such as CockroachDB. We also have analytical databases that can handle\\ntransactional queries, such as Apache Iceberg and DuckDB.\\nFigure 3-6. OLAP and OLTP are outdated terms, as of 2021, according to Google Trends\\nSecond, in the traditional OLTP or OLAP paradigms, storage and processing\\nare tightly coupled—how data is stored is also how data is processed. This may\\nresult in the same data being stored in multiple databases and using different pro‐\\ncessing engines to solve different types of queries. An interesting paradigm in the\\nlast decade has been to decouple storage from processing (also known as compute),\\nas adopted by many data vendors including Google’s BigQuery, Snowflake, IBM,\\nData Storage Engines and Processing | 69', '21 Tino Tereshko, “Separation of Storage and Compute in BigQuery, ” Google Cloud blog, November 29,\\n2017, https://oreil.ly/utf7z; Suresh H., “Snowflake Architecture and Key Concepts: A Comprehensive Guide, ”\\nHevo blog, January 18, 2019, https://oreil.ly/GyvKl; Preetam Kumar, “Cutting the Cord: Separating Data\\nfrom Compute in Y our Data Lake with Object Storage, ” IBM blog, September 21, 2017, https://oreil.ly/\\nNd3xD; “The Power of Separating Cloud Compute and Cloud Storage, ” Teradata, last accessed April 2022,\\nhttps://oreil.ly/f82gP.\\n22 Wikipedia, s.v. “Nearline storage, ” last accessed April 2022, https://oreil.ly/OCmiB.\\nand Teradata.21 In this paradigm, the data can be stored in the same place, with a\\nprocessing layer on top that can be optimized for different types of queries.\\nThird, “online” has become an overloaded term that can mean many different things.\\nOnline used to just mean “connected to the internet. ” Then, it grew to also mean\\n“in production”—we say a feature is online after that feature has been deployed in\\nproduction.\\nIn the data world today, online might refer to the speed at which your data is\\nprocessed and made available: online, nearline, or offline. According to Wikipedia,\\nonline processing means data is immediately available for input/output. Nearline,\\nwhich is short for near-online, means data is not immediately available but can be\\nmade online quickly without human intervention. Offline means data is not immedi‐\\nately available and requires some human intervention to become online.22\\nETL: Extract, Transform, and Load\\nIn the early days of the relational data model, data was mostly structured. When data\\nis extracted from different sources, it’s first transformed into the desired format before\\nbeing loaded into the target destination such as a database or a data warehouse. This\\nprocess is called ETL, which stands for extract, transform, and load.\\nEven before ML, ETL was all the rage in the data world, and it’s still relevant today\\nfor ML applications. ETL refers to the general purpose processing and aggregating of\\ndata into the shape and the format that you want.\\nExtract is extracting the data you want from all your data sources. Some of them will\\nbe corrupted or malformatted. In the extracting phase, you need to validate your data\\nand reject the data that doesn’t meet your requirements. For rejected data, you might\\nhave to notify the sources. Since this is the first step of the process, doing it correctly', 'be corrupted or malformatted. In the extracting phase, you need to validate your data\\nand reject the data that doesn’t meet your requirements. For rejected data, you might\\nhave to notify the sources. Since this is the first step of the process, doing it correctly\\ncan save you a lot of time downstream.\\nTransform is the meaty part of the process, where most of the data processing is done.\\nY ou might want to join data from multiple sources and clean it. Y ou might want to\\nstandardize the value ranges (e.g., one data source might use “Male” and “Female” for\\ngenders, but another uses “M” and “F” or “1” and “2”). Y ou can apply operations such\\nas transposing, deduplicating, sorting, aggregating, deriving new features, more data\\nvalidating, etc.\\n70 | Chapter 3: Data Engineering Fundamentals', '23 In the first draft of this book, I had cost as a reason why you shouldn’t store everything. However, as of today,\\nstorage has become so cheap that the storage cost is rarely a problem.\\nLoad is deciding how and how often to load your transformed data into the target\\ndestination, which can be a file, a database, or a data warehouse.\\nThe idea of ETL sounds simple but powerful, and it’s the underlying structure of\\nthe data layer at many organizations. An overview of the ETL process is shown in\\nFigure 3-7.\\nFigure 3-7. An overview of the ETL process\\nWhen the internet first became ubiquitous and hardware had just become so much\\nmore powerful, collecting data suddenly became so much easier. The amount of data\\ngrew rapidly. Not only that, but the nature of data also changed. The number of data\\nsources expanded, and data schemas evolved.\\nFinding it difficult to keep data structured, some companies had this idea: “Why\\nnot just store all data in a data lake so we don’t have to deal with schema changes?\\nWhichever application needs data can just pull out raw data from there and process\\nit. ” This process of loading data into storage first then processing it later is sometimes\\ncalled ELT (extract, load, transform). This paradigm allows for the fast arrival of data\\nsince there’s little processing needed before data is stored.\\nHowever, as data keeps on growing, this idea becomes less attractive. It’s inefficient\\nto search through a massive amount of raw data for the data that you want. 23\\nAt the same time, as companies switch to running applications on the cloud\\nData Storage Engines and Processing | 71', 'and infrastructures become standardized, data structures also become standardized.\\nCommitting data to a predefined schema becomes more feasible.\\nAs companies weigh the pros and cons of storing structured data versus storing\\nunstructured data, vendors evolve to offer hybrid solutions that combine the flexibil‐\\nity of data lakes and the data management aspect of data warehouses. For example,\\nDatabricks and Snowflake both provide data lakehouse solutions.\\nModes of Dataflow\\nIn this chapter, we’ve been discussing data formats, data models, data storage, and\\nprocessing for data used within the context of a single process. Most of the time, in\\nproduction, you don’t have a single process but multiple. A question arises: how do\\nwe pass data between different processes that don’t share memory?\\nWhen data is passed from one process to another, we say that the data flows from\\none process to another, which gives us a dataflow. There are three main modes of\\ndataflow:\\n• Data passing through databases•\\n• Data passing through services using requests such as the requests provided by•\\nREST and RPC APIs (e.g., POST/GET requests)\\n• Data passing through a real-time transport like Apache Kafka and Amazon•\\nKinesis\\nWe’ll go over each of them in this section.\\nData Passing Through Databases\\nThe easiest way to pass data between two processes is through databases, which we’ve\\ndiscussed in the section “Data Storage Engines and Processing”  on page 67. For\\nexample, to pass data from process A to process B, process A can write that data into\\na database, and process B simply reads from that database.\\nThis mode, however, doesn’t always work because of two reasons. First, it requires\\nthat both processes must be able to access the same database. This might be infeasible,\\nespecially if the two processes are run by two different companies.\\nSecond, it requires both processes to access data from databases, and read/write\\nfrom databases can be slow, making it unsuitable for applications with strict latency\\nrequirements—e.g., almost all consumer-facing applications.\\n72 | Chapter 3: Data Engineering Fundamentals', 'Data Passing Through Services\\nOne way to pass data between two processes is to send data directly through a\\nnetwork that connects these two processes. To pass data from process B to process\\nA, process A first sends a request to process B that specifies the data A needs, and\\nB returns the requested data through the same network. Because processes communi‐\\ncate through requests, we say that this is request-driven.\\nThis mode of data passing is tightly coupled with the service-oriented architecture.\\nA service is a process that can be accessed remotely, e.g., through a network. In this\\nexample, B is exposed to A as a service that A can send requests to. For B to be able to\\nrequest data from A, A will also need to be exposed to B as a service.\\nTwo services in communication with each other can be run by different companies\\nin different applications. For example, a service might be run by a stock exchange\\nthat keeps track of the current stock prices. Another service might be run by an\\ninvestment firm that requests the current stock prices and uses them to predict future\\nstock prices.\\nTwo services in communication with each other can also be parts of the same\\napplication. Structuring different components of your application as separate services\\nallows each component to be developed, tested, and maintained independently of\\none another. Structuring an application as separate services gives you a microservice\\narchitecture.\\nTo put the microservice architecture in the context of ML systems, imagine you’re\\nan ML engineer working on the price optimization problem for a company that\\nowns a ride-sharing application like Lyft. In reality, Lyft has hundreds of services  in\\nits microservice architecture, but for the sake of simplicity, let’s consider only three\\nservices:\\nDriver management service\\nPredicts how many drivers will be available in the next minute in a given area.\\nRide management service\\nPredicts how many rides will be requested in the next minute in a given area.\\nPrice optimization service\\nPredicts the optimal price for each ride. The price for a ride should be low\\nenough for riders to be willing to pay, yet high enough for drivers to be willing to\\ndrive and for the company to make a profit.\\nModes of Dataflow | 73', '24 In practice, the price optimization might not have to request the predicted number of rides/drivers every time\\nit has to make a price prediction. It’s a common practice to use the cached predicted number of rides/drivers\\nand request new predictions every minute or so.\\n25 Kleppmann, Designing Data-Intensive Applications.\\n26 Tyson Trautmann, “Debunking the Myths of RPC and REST, ” Ethereal Bits, December 4, 2012 (accessed via\\nthe Internet Archive), https://oreil.ly/4sUrL.\\nBecause the price depends on supply (the available drivers) and demand (the reques‐\\nted rides), the price optimization service needs data from both the driver manage‐\\nment and ride management services. Each time a user requests a ride, the price\\noptimization service requests the predicted number of rides and predicted number of\\ndrivers to predict the optimal price for this ride.24\\nThe most popular styles of requests used for passing data through networks are REST\\n(representational state transfer) and RPC (remote procedure call). Their detailed\\nanalysis is beyond the scope of this book, but one major difference is that REST\\nwas designed for requests over networks, whereas RPC “tries to make a request to\\na remote network service look the same as calling a function or method in your\\nprogramming language.” Because of this, “REST seems to be the predominant style\\nfor public APIs. The main focus of RPC frameworks is on requests between services\\nowned by the same organization, typically within the same data center. ”25\\nImplementations of a REST architecture are said to be RESTful. Even though many\\npeople think of REST as HTTP , REST doesn’t exactly mean HTTP because HTTP is\\njust an implementation of REST.26\\nData Passing Through Real-Time Transport\\nTo understand the motivation for real-time transports, let’s go back to the preceding\\nexample of the ride-sharing app with three simple services: driver management, ride\\nmanagement, and price optimization. In the last section, we discussed how the price\\noptimization service needs data from the ride and driver management services to\\npredict the optimal price for each ride.\\nNow, imagine that the driver management service also needs to know the number of\\nrides from the ride management service to know how many drivers to mobilize. It\\nalso wants to know the predicted prices from the price optimization service to use\\nthem as incentives for potential drivers (e.g., if you get on the road now you can', 'rides from the ride management service to know how many drivers to mobilize. It\\nalso wants to know the predicted prices from the price optimization service to use\\nthem as incentives for potential drivers (e.g., if you get on the road now you can\\nget a 2x surge charge). Similarly, the ride management service might also want data\\nfrom the driver management and price optimization services. If we pass data through\\nservices as discussed in the previous section, each of these services needs to send\\nrequests to the other two services, as shown in Figure 3-8.\\n74 | Chapter 3: Data Engineering Fundamentals', 'Figure 3-8. In the request-driven architecture, each service needs to send requests to two\\nother services\\nWith only three services, data passing is already getting complicated. Imagine having\\nhundreds, if not thousands of services like what major internet companies have.\\nInterservice data passing can blow up and become a bottleneck, slowing down the\\nentire system.\\nRequest-driven data passing is synchronous: the target service has to listen to the\\nrequest for the request to go through. If the price optimization service requests data\\nfrom the driver management service and the driver management service is down, the\\nprice optimization service will keep resending the request until it times out. And if\\nthe price optimization service is down before it receives a response, the response will\\nbe lost. A service that is down can cause all services that require data from it to be\\ndown.\\nWhat if there’s a broker that coordinates data passing among services? Instead of\\nhaving services request data directly from each other and creating a web of complex\\ninterservice data passing, each service only has to communicate with the broker, as\\nshown in Figure 3-9. For example, instead of having other services request the driver\\nmanagement services for the predicted number of drivers for the next minute, what\\nif whenever the driver management service makes a prediction, this prediction is\\nbroadcast to a broker? Whichever service wants data from the driver management\\nservice can check that broker for the most recent predicted number of drivers.\\nSimilarly, whenever the price optimization service makes a prediction about the surge\\ncharge for the next minute, this prediction is broadcast to the broker.\\nModes of Dataflow | 75', 'Figure 3-9. With a broker, a service only has to communicate with the broker instead of\\nwith other services\\nTechnically, a database can be a broker—each service can write data to a database\\nand other services that need the data can read from that database. However, as\\nmentioned in the section “Data Passing Through Databases” on page 72, reading and\\nwriting from databases are too slow for applications with strict latency requirements.\\nInstead of using databases to broker data, we use in-memory storage to broker data.\\nReal-time transports can be thought of as in-memory storage for data passing among\\nservices.\\nA piece of data broadcast to a real-time transport is called an event. This architecture\\nis, therefore, also called event-driven. A real-time transport is sometimes called an\\nevent bus.\\nRequest-driven architecture works well for systems that rely more on logic than on\\ndata. Event-driven architecture works better for systems that are data-heavy.\\nThe two most common types of real-time transports are pubsub, which is short for\\npublish-subscribe, and message queue. In the pubsub model, any service can publish\\nto different topics in a real-time transport, and any service that subscribes to a topic\\ncan read all the events in that topic. The services that produce data don’t care about\\nwhat services consume their data. Pubsub solutions often have a retention policy—\\ndata will be retained in the real-time transport for a certain period of time (e.g., seven\\ndays) before being deleted or moved to a permanent storage (like Amazon S3). See\\nFigure 3-10.\\n76 | Chapter 3: Data Engineering Fundamentals', '27 If you want to learn more about how Apache Kafka works, Mitch Seymour has a great animation to explain it\\nusing otters!\\nFigure 3-10. Incoming events are stored in in-memory storage before being discarded or\\nmoved to more permanent storage\\nIn a message queue model, an event often has intended consumers (an event with\\nintended consumers is called a message), and the message queue is responsible for\\ngetting the message to the right consumers.\\nExamples of pubsub solutions are Apache Kafka and Amazon Kinesis. 27 Examples of\\nmessage queues are Apache RocketMQ and RabbitMQ. Both paradigms have gained\\na lot of traction in the last few years. Figure 3-11 shows some of the companies that\\nuse Apache Kafka and RabbitMQ.\\nFigure 3-11. Companies that use Apache Kafka and RabbitMQ. Source: Screenshot\\nfrom Stackshare\\nModes of Dataflow | 77', 'Batch Processing Versus Stream Processing\\nOnce your data arrives in data storage engines like databases, data lakes, or data\\nwarehouses, it becomes historical data. This is opposed to streaming data (data that\\nis still streaming in). Historical data is often processed in batch jobs—jobs that are\\nkicked off periodically. For example, once a day, you might want to kick off a batch\\njob to compute the average surge charge for all the rides in the last day.\\nWhen data is processed in batch jobs, we refer to it as batch processing. Batch process‐\\ning has been a research subject for many decades, and companies have come up with\\ndistributed systems like MapReduce and Spark to process batch data efficiently.\\nWhen you have data in real-time transports like Apache Kafka and Amazon Kinesis,\\nwe say that you have streaming data. Stream processing refers to doing computation\\non streaming data. Computation on streaming data can also be kicked off periodi‐\\ncally, but the periods are usually much shorter than the periods for batch jobs (e.g.,\\nevery five minutes instead of every day). Computation on streaming data can also be\\nkicked off whenever the need arises. For example, whenever a user requests a ride,\\nyou process your data stream to see what drivers are currently available.\\nStream processing, when done right, can give low latency because you can process\\ndata as soon as data is generated, without having to first write it into databases. Many\\npeople believe that stream processing is less efficient than batch processing because\\nyou can’t leverage tools like MapReduce or Spark. This is not always the case, for\\ntwo reasons. First, streaming technologies like Apache Flink are proven to be highly\\nscalable and fully distributed, which means they can do computation in parallel.\\nSecond, the strength of stream processing is in stateful computation. Consider the\\ncase where you want to process user engagement during a 30-day trial. If you kick\\noff this batch job every day, you’ll have to do computation over the last 30 days every\\nday. With stream processing, it’s possible to continue computing only the new data\\neach day and joining the new data computation with the older data computation,\\npreventing redundancy.\\nBecause batch processing happens much less frequently than stream processing, in\\nML, batch processing is usually used to compute features that change less often, such\\nas drivers’ ratings (if a driver has had hundreds of rides, their rating is less likely', 'preventing redundancy.\\nBecause batch processing happens much less frequently than stream processing, in\\nML, batch processing is usually used to compute features that change less often, such\\nas drivers’ ratings (if a driver has had hundreds of rides, their rating is less likely\\nto change significantly from one day to the next). Batch features—features extracted\\nthrough batch processing—are also known as static features.\\nStream processing is used to compute features that change quickly, such as how\\nmany drivers are available right now, how many rides have been requested in the last\\nminute, how many rides will be finished in the next two minutes, the median price\\nof the last 10 rides in this area, etc. Features about the current state of the system\\nlike these are important to make the optimal price predictions. Streaming features—\\nfeatures extracted through stream processing—are also known as dynamic features.\\n78 | Chapter 3: Data Engineering Fundamentals', '28 Kostas Tzoumas, “Batch Is a Special Case of Streaming, ” Ververica, September 15, 2015, https://oreil.ly/IcIl2.\\nFor many problems, you need not only batch features or streaming features, but both.\\nY ou need infrastructure that allows you to process streaming data as well as batch\\ndata and join them together to feed into your ML models. We’ll discuss more on how\\nbatch features and streaming features can be used together to generate predictions in\\nChapter 7.\\nTo do computation on data streams, you need a stream computation engine (the\\nway Spark and MapReduce are batch computation engines). For simple streaming\\ncomputation, you might be able to get away with the built-in stream computation\\ncapacity of real-time transports like Apache Kafka, but Kafka stream processing is\\nlimited in its ability to deal with various data sources.\\nFor ML systems that leverage streaming features, the streaming computation is\\nrarely simple. The number of stream features used in an application such as fraud\\ndetection and credit scoring can be in the hundreds, if not thousands. The stream\\nfeature extraction logic can require complex queries with join and aggregation along\\ndifferent dimensions. To extract these features requires efficient stream processing\\nengines. For this purpose, you might want to look into tools like Apache Flink, KSQL,\\nand Spark Streaming. Of these three engines, Apache Flink and KSQL are more\\nrecognized in the industry and provide a nice SQL abstraction for data scientists.\\nStream processing is more difficult because the data amount is unbounded and the\\ndata comes in at variable rates and speeds. It’s easier to make a stream processor\\ndo batch processing than to make a batch processor do stream processing. Apache\\nFlink’s core maintainers have been arguing for years that batch processing is a special\\ncase of stream processing.28\\nSummary\\nThis chapter is built on the foundations established in Chapter 2 around the impor‐\\ntance of data in developing ML systems. In this chapter, we learned it’s important\\nto choose the right format to store our data to make it easier to use the data in\\nthe future. We discussed different data formats and the pros and cons of row-major\\nversus column-major formats as well as text versus binary formats.\\nWe continued to cover three major data models: relational, document, and graph.\\nEven though the relational model is the most well known given the popularity of', 'versus column-major formats as well as text versus binary formats.\\nWe continued to cover three major data models: relational, document, and graph.\\nEven though the relational model is the most well known given the popularity of\\nSQL, all three models are widely used today, and each is good for a certain set of\\ntasks.\\nWhen talking about the relational model compared to the document model, many\\npeople think of the former as structured and the latter as unstructured. The division\\nSummary | 79', 'between structured and unstructured data is quite fluid—the main question is who\\nhas to shoulder the responsibility of assuming the structure of data. Structured data\\nmeans that the code that writes the data has to assume the structure. Unstructured\\ndata means that the code that reads the data has to assume the structure.\\nWe continued the chapter with data storage engines and processing. We studied data‐\\nbases optimized for two distinct types of data processing: transactional processing\\nand analytical processing. We studied data storage engines and processing together\\nbecause traditionally storage is coupled with processing: transactional databases for\\ntransactional processing and analytical databases for analytical processing. However,\\nin recent years, many vendors have worked on decoupling storage and processing.\\nToday, we have transactional databases that can handle analytical queries and analyti‐\\ncal databases that can handle transactional queries.\\nWhen discussing data formats, data models, data storage engines, and processing,\\ndata is assumed to be within a process. However, while working in production, you’ll\\nlikely work with multiple processes, and you’ll likely need to transfer data between\\nthem. We discussed three modes of data passing. The simplest mode is passing\\nthrough databases. The most popular mode of data passing for processes is data\\npassing through services. In this mode, a process is exposed as a service that another\\nprocess can send requests for data. This mode of data passing is tightly coupled with\\nmicroservice architectures, where each component of an application is set up as a\\nservice.\\nA mode of data passing that has become increasingly popular over the last decade is\\ndata passing through a real-time transport like Apache Kafka and RabbitMQ. This\\nmode of data passing is somewhere between passing through databases and passing\\nthrough services: it allows for asynchronous data passing with reasonably low latency.\\nAs data in real-time transports have different properties from data in databases, they\\nrequire different processing techniques, as discussed in the section “Batch Process‐\\ning Versus Stream Processing” on page 78. Data in databases is often processed in\\nbatch jobs and produces static features, whereas data in real-time transports is often\\nprocessed using stream computation engines and produces dynamic features. Some\\npeople argue that batch processing is a special case of stream processing, and stream', 'batch jobs and produces static features, whereas data in real-time transports is often\\nprocessed using stream computation engines and produces dynamic features. Some\\npeople argue that batch processing is a special case of stream processing, and stream\\ncomputation engines can be used to unify both processing pipelines.\\nOnce we have our data systems figured out, we can collect data and create training\\ndata, which will be the focus of the next chapter.\\n80 | Chapter 3: Data Engineering Fundamentals', 'CHAPTER 4\\nTraining Data\\nIn Chapter 3, we covered how to handle data from the systems perspective. In this\\nchapter, we’ll go over how to handle data from the data science perspective. Despite\\nthe importance of training data in developing and improving ML models, ML curric‐\\nula are heavily skewed toward modeling, which is considered by many practitioners\\nthe “fun” part of the process. Building a state-of-the-art model is interesting. Spend‐\\ning days wrangling with a massive amount of malformatted data that doesn’t even fit\\ninto your machine’s memory is frustrating.\\nData is messy, complex, unpredictable, and potentially treacherous. If not handled\\nproperly, it can easily sink your entire ML operation. But this is precisely the reason\\nwhy data scientists and ML engineers should learn how to handle data well, saving us\\ntime and headache down the road.\\nIn this chapter, we will go over techniques to obtain or create good training data.\\nTraining data, in this chapter, encompasses all the data used in the developing phase\\nof ML models, including the different splits used for training, validation, and testing\\n(the train, validation, test splits). This chapter starts with different sampling techni‐\\nques to select data for training. We’ll then address common challenges in creating\\ntraining data, including the label multiplicity problem, the lack of labels problem, the\\nclass imbalance problem, and techniques in data augmentation to address the lack of\\ndata problem.\\nWe use the term “training data” instead of “training dataset” because “dataset” denotes\\na set that is finite and stationary. Data in production is neither finite nor stationary, a\\nphenomenon that we will cover in the section “Data Distribution Shifts” on page 237.\\nLike other steps in building ML systems, creating training data is an iterative process.\\nAs your model evolves through a project lifecycle, your training data will likely also\\nevolve.\\n81', '1 Some readers might argue that this approach might not work with large models, as certain large models don’t\\nwork for small datasets but work well with a lot more data. In this case, it’s still important to experiment with\\ndatasets of different sizes to figure out the effect of the dataset size on your model.\\nBefore we move forward, I just want to echo a word of caution that has been said\\nmany times yet is still not enough. Data is full of potential biases. These biases\\nhave many possible causes. There are biases caused during collecting, sampling, or\\nlabeling. Historical data might be embedded with human biases, and ML models,\\ntrained on this data, can perpetuate them. Use data but don’t trust it too much!\\nSampling\\nSampling is an integral part of the ML workflow that is, unfortunately, often over‐\\nlooked in typical ML coursework. Sampling happens in many steps of an ML project\\nlifecycle, such as sampling from all possible real-world data to create training data;\\nsampling from a given dataset to create splits for training, validation, and testing; or\\nsampling from all possible events that happen within your ML system for monitoring\\npurposes. In this section, we’ll focus on sampling methods for creating training data,\\nbut these sampling methods can also be used for other steps in an ML project\\nlifecycle.\\nIn many cases, sampling is necessary. One case is when you don’t have access to all\\npossible data in the real world, the data that you use to train your model is a subset\\nof real-world data, created by one sampling method or another. Another case is when\\nit’s infeasible to process all the data that you have access to—because it requires too\\nmuch time or resources—so you have to sample that data to create a subset that\\nis feasible to process. In many other cases, sampling is helpful as it allows you to\\naccomplish a task faster and cheaper. For example, when considering a new model,\\nyou might want to do a quick experiment with a small subset of your data to see if the\\nnew model is promising first before training this new model on all your data.1\\nUnderstanding different sampling methods and how they are being used in our\\nworkflow can, first, help us avoid potential sampling biases, and second, help us\\nchoose the methods that improve the efficiency of the data we sample.\\nThere are two families of sampling: nonprobability sampling and random sampling.\\nWe’ll start with nonprobability sampling methods, followed by several common ran‐\\ndom sampling methods.', 'choose the methods that improve the efficiency of the data we sample.\\nThere are two families of sampling: nonprobability sampling and random sampling.\\nWe’ll start with nonprobability sampling methods, followed by several common ran‐\\ndom sampling methods.\\n82 | Chapter 4: Training Data', '2 James J. Heckman, “Sample Selection Bias as a Specification Error, ” Econometrica 47, no. 1 (January 1979):\\n153–61, https://oreil.ly/I5AhM.\\nNonprobability Sampling\\nNonprobability sampling is when the selection of data isn’t based on any probability\\ncriteria. Here are some of the criteria for nonprobability sampling:\\nConvenience sampling\\nSamples of data are selected based on their availability. This sampling method is\\npopular because, well, it’s convenient.\\nSnowball sampling\\nFuture samples are selected based on existing samples. For example, to scrape\\nlegitimate Twitter accounts without having access to Twitter databases, you start\\nwith a small number of accounts, then you scrape all the accounts they follow,\\nand so on.\\nJudgment sampling\\nExperts decide what samples to include.\\nQuota sampling\\nY ou select samples based on quotas for certain slices of data without any random‐\\nization. For example, when doing a survey, you might want 100 responses from\\neach of the age groups: under 30 years old, between 30 and 60 years old, and\\nabove 60 years old, regardless of the actual age distribution.\\nThe samples selected by nonprobability criteria are not representative of the real-\\nworld data and therefore are riddled with selection biases. 2 Because of these biases,\\nyou might think that it’s a bad idea to select data to train ML models using this family\\nof sampling methods. Y ou’re right. Unfortunately, in many cases, the selection of data\\nfor ML models is still driven by convenience.\\nOne example of these cases is language modeling. Language models are often trained\\nnot with data that is representative of all possible texts but with data that can be easily\\ncollected—Wikipedia, Common Crawl, Reddit.\\nAnother example is data for sentiment analysis of general text. Much of this data\\nis collected from sources with natural labels (ratings) such as IMDB reviews and\\nAmazon reviews. These datasets are then used for other sentiment analysis tasks.\\nIMDB reviews and Amazon reviews are biased toward users who are willing to leave\\nreviews online, and not necessarily representative of people who don’t have access to\\nthe internet or people who aren’t willing to put reviews online.\\nSampling | 83', '3 Rachel Lerman, “Google Is Testing Its Self-Driving Car in Kirkland, ” Seattle Times, February 3, 2016,\\nhttps://oreil.ly/3IA1V.\\n4 Population here refers to a “statistical population”, a (potentially infinite) set of all possible samples that can be\\nsampled.\\nA third example is data for training self-driving cars. Initially, data collected for\\nself-driving cars came largely from two areas: Phoenix, Arizona (because of its lax\\nregulations), and the Bay Area in California (because many companies that build\\nself-driving cars are located here). Both areas have generally sunny weather. In 2016,\\nWaymo expanded its operations to Kirkland, Washington, specially for Kirkland’s\\nrainy weather,3 but there’s still a lot more self-driving car data for sunny weather than\\nfor rainy or snowy weather.\\nNonprobability sampling can be a quick and easy way to gather your initial data to\\nget your project off the ground. However, for reliable models, you might want to use\\nprobability-based sampling, which we will cover next.\\nSimple Random Sampling\\nIn the simplest form of random sampling, you give all samples in the population\\nequal probabilities of being selected. 4 For example, you randomly select 10% of the\\npopulation, giving all members of this population an equal 10% chance of being\\nselected.\\nThe advantage of this method is that it’s easy to implement. The drawback is that\\nrare categories of data might not appear in your selection. Consider the case where\\na class appears only in 0.01% of your data population. If you randomly select 1% of\\nyour data, samples of this rare class will unlikely be selected. Models trained on this\\nselection might think that this rare class doesn’t exist.\\nStratified Sampling\\nTo avoid the drawback of simple random sampling, you can first divide your popula‐\\ntion into the groups that you care about and sample from each group separately. For\\nexample, to sample 1% of data that has two classes, A and B, you can sample 1% of\\nclass A and 1% of class B. This way, no matter how rare class A or B is, you’ll ensure\\nthat samples from it will be included in the selection. Each group is called a stratum,\\nand this method is called stratified sampling.\\n84 | Chapter 4: Training Data', '5 Multilabel tasks are tasks where one example can have multiple labels.\\nOne drawback of this sampling method is that it isn’t always possible, such as when\\nit’s impossible to divide all samples into groups. This is especially challenging when\\none sample might belong to multiple groups, as in the case of multilabel tasks. 5 For\\ninstance, a sample can be both class A and class B.\\nWeighted Sampling\\nIn weighted sampling, each sample is given a weight, which determines the probabil‐\\nity of it being selected. For example, if you have three samples, A, B, and C, and want\\nthem to be selected with the probabilities of 50%, 30%, and 20% respectively, you can\\ngive them the weights 0.5, 0.3, and 0.2.\\nThis method allows you to leverage domain expertise. For example, if you know that\\na certain subpopulation of data, such as more recent data, is more valuable to your\\nmodel and want it to have a higher chance of being selected, you can give it a higher\\nweight.\\nThis also helps with the case when the data you have comes from a different distribu‐\\ntion compared to the true data. For example, if in your data, red samples account for\\n25% and blue samples account for 75%, but you know that in the real world, red and\\nblue have equal probability to happen, you can give red samples weights three times\\nhigher than blue samples.\\nIn Python, you can do weighted sampling with random.choices as follows:\\n# Choose two items from the list such that 1, 2, 3, 4 each has\\n# 20% chance of being selected, while 100 and 1000 each have only 10% chance.\\nimport random\\nrandom.choices(population=[1, 2, 3, 4, 100, 1000],\\n               weights=[0.2, 0.2, 0.2, 0.2, 0.1, 0.1],\\n               k=2)\\n# This is equivalent to the following\\nrandom.choices(population=[1, 1, 2, 2, 3, 3, 4, 4, 100, 1000],\\n               k=2)\\nA common concept in ML that is closely related to weighted sampling is sample\\nweights. Weighted sampling is used to select samples to train your model with,\\nwhereas sample weights are used to assign “weights” or “importance” to training\\nsamples. Samples with higher weights affect the loss function more. Changing sample\\nweights can change your model’s decision boundaries significantly, as shown in\\nFigure 4-1.\\nSampling | 85', '6 “SVM: Weighted Samples, ” scikit-learn, https://oreil.ly/BDqbk.\\nFigure 4-1. Sample weights can affect the decision boundary. On the left is when all\\nsamples are given equal weights. On the right is when samples are given different\\nweights. Source: scikit-learn6\\nReservoir Sampling\\nReservoir sampling is a fascinating algorithm that is especially useful when you have\\nto deal with streaming data, which is usually what you have in production.\\nImagine you have an incoming stream of tweets and you want to sample a certain\\nnumber, k, of tweets to do analysis or train a model on. Y ou don’t know how many\\ntweets there are, but you know you can’t fit them all in memory, which means you\\ndon’t know in advance the probability at which a tweet should be selected. Y ou want\\nto ensure that:\\n• Every tweet has an equal probability of being selected.•\\n• Y ou can stop the algorithm at any time and the tweets are sampled with the•\\ncorrect probability.\\nOne solution for this problem is reservoir sampling. The algorithm involves a reser‐\\nvoir, which can be an array, and consists of three steps:\\n1. Put the first k elements into the reservoir.1.\\n2. For each incoming nth element, generate a random number i such that 1 ≤ i ≤ n.2.\\n3. If 1 ≤ i ≤ k: replace the ith element in the reservoir with the nth element. Else, do3.\\nnothing.\\n86 | Chapter 4: Training Data', 'This means that each incoming nth element has kn probability of being in the reser‐\\nvoir. Y ou can also prove that each element in the reservoir has kn probability of\\nbeing there. This means that all samples have an equal chance of being selected. If\\nwe stop the algorithm at any time, all samples in the reservoir have been sampled\\nwith the correct probability. Figure 4-2 shows an illustrative example of how reservoir\\nsampling works.\\nFigure 4-2. A visualization of how reservoir sampling works\\nImportance Sampling\\nImportance sampling is one of the most important sampling methods, not just in\\nML. It allows us to sample from a distribution when we only have access to another\\ndistribution.\\nImagine you have to sample x from a distribution P(x), but P(x) is really expensive,\\nslow, or infeasible to sample from. However, you have a distribution Q(x) that is a lot\\neasier to sample from. So you sample x from Q(x) instead and weigh this sample by\\nPxQx. Q(x) is called the proposal distribution or the importance distribution. Q(x) can\\nbe any distribution as long as Q(x) > 0 whenever P(x) ≠ 0. The following equation\\nshows that in expectation, x sampled from P(x) is equal to x sampled from Q(x)\\nweighted by PxQx:\\nEP x x = ∑\\nx\\nP x x = ∑\\nx\\nQ x xP x\\nQ x = EQ x xP x\\nQ x\\nOne example where importance sampling is used in ML is policy-based reinforce‐\\nment learning. Consider the case when you want to update your policy. Y ou want to\\nestimate the value functions of the new policy, but calculating the total rewards of\\ntaking an action can be costly because it requires considering all possible outcomes\\nuntil the end of the time horizon after that action. However, if the new policy is\\nrelatively close to the old policy, you can calculate the total rewards based on the old\\npolicy instead and reweight them according to the new policy. The rewards from the\\nold policy make up the proposal distribution.\\nSampling | 87', '7 Xiaojin Zhu, “Semi-Supervised Learning with Graphs” (doctoral diss., Carnegie Mellon University, 2005),\\nhttps://oreil.ly/VYy4C.\\nLabeling\\nDespite the promise of unsupervised ML, most ML models in production today are\\nsupervised, which means that they need labeled data to learn from. The performance\\nof an ML model still depends heavily on the quality and quantity of the labeled data\\nit’s trained on.\\nIn a talk to my students, Andrej Karpathy, director of AI at Tesla, shared an anecdote\\nabout how when he decided to have an in-house labeling team, his recruiter asked\\nhow long he’ d need this team for. He responded: “How long do we need an engineer‐\\ning team for?” Data labeling has gone from being an auxiliary task to being a core\\nfunction of many ML teams in production.\\nIn this section, we will discuss the challenge of obtaining labels for your data. We’ll\\nfirst discuss the labeling method that usually comes first in data scientists’ mind\\nwhen talking about labeling: hand-labeling. We will then discuss tasks with natural\\nlabels, which are tasks where labels can be inferred from the system without requiring\\nhuman annotations, followed by what to do when natural and hand labels are lacking.\\nHand Labels\\nAnyone who has ever had to work with data in production has probably felt this at a\\nvisceral level: acquiring hand labels for your data is difficult for many, many reasons.\\nFirst, hand-labeling data can be expensive, especially if subject matter expertise is\\nrequired. To classify whether a comment is spam, you might be able to find 20\\nannotators on a crowdsourcing platform and train them in 15 minutes to label your\\ndata. However, if you want to label chest X-rays, you’ d need to find board-certified\\nradiologists, whose time is limited and expensive.\\nSecond, hand labeling poses a threat to data privacy. Hand labeling means that some‐\\none has to look at your data, which isn’t always possible if your data has strict privacy\\nrequirements. For example, you can’t just ship your patients’ medical records or your\\ncompany’s confidential financial information to a third-party service for labeling. In\\nmany cases, your data might not even be allowed to leave your organization, and you\\nmight have to hire or contract annotators to label your data on premises.\\nThird, hand labeling is slow. For example, accurate transcription of speech utterance\\nat the phonetic level can take 400 times longer than the utterance duration. 7 So if', 'might have to hire or contract annotators to label your data on premises.\\nThird, hand labeling is slow. For example, accurate transcription of speech utterance\\nat the phonetic level can take 400 times longer than the utterance duration. 7 So if\\nyou want to annotate 1 hour of speech, it’ll take 400 hours or almost 3 months for a\\nperson to do so. In a study to use ML to help classify lung cancers from X-rays, my\\ncolleagues had to wait almost a year to obtain sufficient labels.\\n88 | Chapter 4: Training Data', 'Slow labeling leads to slow iteration speed and makes your model less adaptive to\\nchanging environments and requirements. If the task changes or data changes, you’ll\\nhave to wait for your data to be relabeled before updating your model. Imagine the\\nscenario when you have a sentiment analysis model to analyze the sentiment of every\\ntweet that mentions your brand. It has only two classes: NEGATIVE and POSITIVE.\\nHowever, after deployment, your PR team realizes that the most damage comes from\\nangry tweets and they want to attend to angry messages faster. So you have to update\\nyour sentiment analysis model to have three classes: NEGATIVE, POSITIVE, and\\nANGRY . To do so, you will need to look at your data again to see which existing\\ntraining examples should be relabeled ANGRY . If you don’t have enough ANGRY\\nexamples, you will have to collect more data. The longer the process takes, the more\\nyour existing model performance will degrade.\\nLabel multiplicity\\nOften, to obtain enough labeled data, companies have to use data from multiple\\nsources and rely on multiple annotators who have different levels of expertise. These\\ndifferent data sources and annotators also have different levels of accuracy. This leads\\nto the problem of label ambiguity or label multiplicity: what to do when there are\\nmultiple conflicting labels for a data instance.\\nConsider this simple task of entity recognition. Y ou give three annotators the follow‐\\ning sample and ask them to annotate all entities they can find:\\nDarth Sidious, known simply as the Emperor, was a Dark Lord of the Sith who reigned\\nover the galaxy as Galactic Emperor of the First Galactic Empire.\\nY ou receive back three different solutions, as shown in Table 4-1. Three annotators\\nhave identified different entities. Which one should your model train on? A model\\ntrained on data labeled by annotator 1 will perform very differently from a model\\ntrained on data labeled by annotator 2.\\nTable 4-1. Entities identified by different annotators might be very different\\nAnnotator # entities Annotation\\n1 3 [Darth Sidious], known simply as the Emperor, was a [ Dark Lord of the Sith ] who reigned over the\\ngalaxy as [Galactic Emperor of the First Galactic Empire ].\\n2 6 [Darth Sidious], known simply as the [ Emperor], was a [ Dark Lord] of the [ Sith] who reigned over the\\ngalaxy as [Galactic Emperor] of the [ First Galactic Empire].\\n3 4 [Darth Sidious], known simply as the [ Emperor], was a [ Dark Lord of the Sith ] who reigned over the', '2 6 [Darth Sidious], known simply as the [ Emperor], was a [ Dark Lord] of the [ Sith] who reigned over the\\ngalaxy as [Galactic Emperor] of the [ First Galactic Empire].\\n3 4 [Darth Sidious], known simply as the [ Emperor], was a [ Dark Lord of the Sith ] who reigned over the\\ngalaxy as [Galactic Emperor of the First Galactic Empire ].\\nLabeling | 89', '8 If something is so obvious to label, you wouldn’t need domain expertise.\\nDisagreements among annotators are extremely common. The higher the level of\\ndomain expertise required, the higher the potential for annotating disagreement. 8 If\\none human expert thinks the label should be A while another believes it should be B,\\nhow do we resolve this conflict to obtain one single ground truth? If human experts\\ncan’t agree on a label, what does human-level performance even mean?\\nTo minimize the disagreement among annotators, it’s important to first have a clear\\nproblem definition. For example, in the preceding entity recognition task, some disa‐\\ngreements could have been eliminated if we clarify that in case of multiple possible\\nentities, pick the entity that comprises the longest substring. This means Galactic\\nEmperor of the First Galactic Empire  instead of Galactic Emperor and First Galactic\\nEmpire. Second, you need to incorporate that definition into the annotators’ training\\nto make sure that all annotators understand the rules.\\nData lineage\\nIndiscriminately using data from multiple sources, generated with different annota‐\\ntors, without examining their quality can cause your model to fail mysteriously.\\nConsider a case when you’ve trained a moderately good model with 100K data\\nsamples. Y our ML engineers are confident that more data will improve the model\\nperformance, so you spend a lot of money to hire annotators to label another million\\ndata samples.\\nHowever, the model performance actually decreases after being trained on the new\\ndata. The reason is that the new million samples were crowdsourced to annotators\\nwho labeled data with much less accuracy than the original data. It can be especially\\ndifficult to remedy this if you’ve already mixed your data and can’t differentiate new\\ndata from old data.\\nIt’s good practice to keep track of the origin of each of your data samples as well as its\\nlabels, a technique known as data lineage. Data lineage helps you both flag potential\\nbiases in your data and debug your models. For example, if your model fails mostly\\non the recently acquired data samples, you might want to look into how the new data\\nwas acquired. On more than one occasion, we’ve discovered that the problem wasn’t\\nwith our model, but because of the unusually high number of wrong labels in the data\\nthat we’ d acquired recently.\\n90 | Chapter 4: Training Data', 'Natural Labels\\nHand-labeling isn’t the only source for labels. Y ou might be lucky enough to work\\non tasks with natural ground truth labels. Tasks with natural labels are tasks where\\nthe model’s predictions can be automatically evaluated or partially evaluated by the\\nsystem. An example is the model that estimates time of arrival for a certain route on\\nGoogle Maps. If you take that route, by the end of your trip, Google Maps knows how\\nlong the trip actually took, and thus can evaluate the accuracy of the predicted time\\nof arrival. Another example is stock price prediction. If your model predicts a stock’s\\nprice in the next two minutes, then after two minutes, you can compare the predicted\\nprice with the actual price.\\nThe canonical example of tasks with natural labels is recommender systems. The goal\\nof a recommender system is to recommend to users items relevant to them. Whether\\na user clicks on the recommended item or not can be seen as the feedback for that\\nrecommendation. A recommendation that gets clicked on can be presumed to be\\ngood (i.e., the label is POSITIVE) and a recommendation that doesn’t get clicked on\\nafter a period of time, say 10 minutes, can be presumed to be bad (i.e., the label is\\nNEGATIVE).\\nMany tasks can be framed as recommendation tasks. For example, you can frame the\\ntask of predicting ads’ click-through rates as recommending the most relevant ads\\nto users based on their activity histories and profiles. Natural labels that are inferred\\nfrom user behaviors like clicks and ratings are also known as behavioral labels.\\nEven if your task doesn’t inherently have natural labels, it might be possible to set\\nup your system in a way that allows you to collect some feedback on your model.\\nFor example, if you’re building a machine translation system like Google Translate,\\nyou can have the option for the community to submit alternative translations for bad\\ntranslations—these alternative translations can be used to train the next iteration of\\nyour models (though you might want to review these suggested translations first).\\nNewsfeed ranking is not a task with inherent labels, but by adding the Like button\\nand other reactions to each newsfeed item, Facebook is able to collect feedback on\\ntheir ranking algorithm.\\nTasks with natural labels are fairly common in the industry. In a survey of 86\\ncompanies in my network, I found that 63% of them work with tasks with natural', 'and other reactions to each newsfeed item, Facebook is able to collect feedback on\\ntheir ranking algorithm.\\nTasks with natural labels are fairly common in the industry. In a survey of 86\\ncompanies in my network, I found that 63% of them work with tasks with natural\\nlabels, as shown in Figure 4-3. This doesn’t mean that 63% of tasks that can benefit\\nfrom ML solutions have natural labels. What is more likely is that companies find it\\neasier and cheaper to first start on tasks that have natural labels.\\nLabeling | 91', '9 We’ll cover programmatic labels in the section “Weak supervision” on page 95.\\nFigure 4-3. Sixty-three percent of companies in my network work on tasks with natural\\nlabels. The percentages don’t sum to 1 because a company can work with tasks with\\ndifferent label sources.9\\nIn the previous example, a recommendation that doesn’t get clicked on after a period\\nof time can be presumed to be bad. This is called an implicit label, as this negative\\nlabel is presumed from the lack of a positive label. It’s different from explicit labels\\nwhere users explicitly demonstrate their feedback on a recommendation by giving it a\\nlow rating or downvoting it.\\nFeedback loop length\\nFor tasks with natural ground truth labels, the time it takes from when a predic‐\\ntion is served until when the feedback on it is provided is the feedback loop\\nlength. Tasks with short feedback loops are tasks where labels are generally available\\nwithin minutes. Many recommender systems have short feedback loops. If the rec‐\\nommended items are related products on Amazon or people to follow on Twitter, the\\ntime between when the item is recommended until it’s clicked on, if it’s clicked on at\\nall, is short.\\n92 | Chapter 4: Training Data', 'However, not all recommender systems have minute-long feedback loops. If you\\nwork with longer content types like blog posts or articles or Y ouTube videos, the\\nfeedback loop can be hours. If you build a system to recommend clothes for users like\\nthe one Stitch Fix has, you wouldn’t get feedback until users have received the items\\nand tried them on, which could be weeks later.\\nDifferent Types of User Feedback\\nIf you want to extract labels from user feedback, it’s important to note that there\\nare different types of user feedback. They can occur at different stages during a user\\njourney on your app and differ by volume, strength of signal, and feedback loop\\nlength.\\nFor example, consider an ecommerce application similar to what Amazon has. Types\\nof feedback a user on this application can provide might include clicking on a product\\nrecommendation, adding a product to cart, buying a product, rating, leaving a review,\\nand returning a previously bought product.\\nClicking on a product happens much faster and more frequently (and therefore\\nincurs a higher volume) than purchasing a product. However, buying a product is a\\nmuch stronger signal on whether a user likes that product compared to just clicking\\non it.\\nWhen building a product recommender system, many companies focus on optimiz‐\\ning for clicks, which give them a higher volume of feedback to evaluate their models.\\nHowever, some companies focus on purchases, which gives them a stronger signal\\nthat is also more correlated to their business metrics (e.g., revenue from product\\nsales). Both approaches are valid. There’s no definite answer to what type of feedback\\nyou should optimize for your use case, and it merits serious discussions between all\\nstakeholders involved.\\nChoosing the right window length requires thorough consideration, as it involves the\\nspeed and accuracy trade-off. A short window length means that you can capture\\nlabels faster, which allows you to use these labels to detect issues with your model and\\naddress those issues as soon as possible. However, a short window length also means\\nthat you might prematurely label a recommendation as bad before it’s clicked on.\\nNo matter how long you set your window length to be, there might still be premature\\nnegative labels. In early 2021, a study by the Ads team at Twitter found that even\\nthough the majority of clicks on ads happen within the first five minutes, some clicks\\nLabeling | 93', '10 Sofia Ira Ktena, Alykhan Tejani, Lucas Theis, Pranay Kumar Myana, Deepak Dilipkumar, Ferenc Huszar,\\nSteven Y oo, and Wenzhe Shi, “ Addressing Delayed Feedback for Continuous Training with Neural Networks\\nin CTR Prediction, ” arXiv, July 15, 2019, https://oreil.ly/5y2WA.\\nhappen hours after when the ad is shown. 10 This means that this type of label tends\\nto give an underestimate of the actual click-through rate. If you only record 1,000\\nPOSITIVE labels, the actual number of clicks might be a bit over 1,000.\\nFor tasks with long feedback loops, natural labels might not arrive for weeks or even\\nmonths. Fraud detection is an example of a task with long feedback loops. For a\\ncertain period of time after a transaction, users can dispute whether that transaction\\nis fraudulent or not. For example, when a customer read their credit card statement\\nand saw a transaction they didn’t recognize, they might dispute it with their bank,\\ngiving the bank the feedback to label that transaction as fraudulent. A typical dispute\\nwindow is one to three months. After the dispute window has passed, if there’s no\\ndispute from the user, you might presume the transaction to be legitimate.\\nLabels with long feedback loops are helpful for reporting a model’s performance on\\nquarterly or yearly business reports. However, they are not very helpful if you want\\nto detect issues with your models as soon as possible. If there’s a problem with your\\nfraud detection model and it takes you months to catch, by the time the problem is\\nfixed, all the fraudulent transactions your faulty model let through might have caused\\na small business to go bankrupt.\\nHandling the Lack of Labels\\nBecause of the challenges in acquiring sufficient high-quality labels, many techniques\\nhave been developed to address the problems that result. In this section, we will\\ncover four of them: weak supervision, semi-supervision, transfer learning, and active\\nlearning. A summary of these methods is shown in Table 4-2.\\nTable 4-2. Summaries of four techniques for handling the lack of hand-labeled data\\nMethod How Ground truths required?\\nWeak\\nsupervision\\nLeverages (often noisy) heuristics to\\ngenerate labels\\nNo, but a small number of labels are recommended to guide the\\ndevelopment of heuristics\\nSemi-\\nsupervision\\nLeverages structural assumptions to\\ngenerate labels\\nYes, a small number of initial labels as seeds to generate more\\nlabels\\nTransfer\\nlearning\\nLeverages models pretrained on\\nanother task for your new task', 'development of heuristics\\nSemi-\\nsupervision\\nLeverages structural assumptions to\\ngenerate labels\\nYes, a small number of initial labels as seeds to generate more\\nlabels\\nTransfer\\nlearning\\nLeverages models pretrained on\\nanother task for your new task\\nNo for zero-shot learning\\nYes for fine-tuning, though the number of ground truths\\nrequired is often much smaller than what would be needed\\nif you train the model from scratch\\nActive learning Labels data samples that are most\\nuseful to your model\\nYes\\n94 | Chapter 4: Training Data', '11 Alexander Ratner, Stephen H. Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher Ré, “Snorkel:\\nRapid Training Data Creation with Weak Supervision, ” Proceedings of the VLDB Endowment 11, no. 3 (2017):\\n269–82, https://oreil.ly/vFPjk.\\nWeak supervision\\nIf hand labeling is so problematic, what if we don’t use hand labels altogether? One\\napproach that has gained popularity is weak supervision. One of the most popular\\nopen source tools for weak supervision is Snorkel, developed at the Stanford AI Lab.11\\nThe insight behind weak supervision is that people rely on heuristics, which can be\\ndeveloped with subject matter expertise, to label data. For example, a doctor might\\nuse the following heuristics to decide whether a patient’s case should be prioritized as\\nemergent:\\nIf the nurse’s note mentions a serious condition like pneumonia, the patient’s case\\nshould be given priority consideration.\\nLibraries like Snorkel are built around the concept of a labeling function (LF): a\\nfunction that encodes heuristics. The preceding heuristics can be expressed by the\\nfollowing function:\\ndef labeling_function(note):\\n   if \"pneumonia\" in note:\\n     return \"EMERGENT\"\\nLFs can encode many different types of heuristics. Here are some of them:\\nKeyword heuristic\\nSuch as the preceding example\\nRegular expressions\\nSuch as if the note matches or fails to match a certain regular expression\\nDatabase lookup\\nSuch as if the note contains the disease listed in the dangerous disease list\\nThe outputs of other models\\nSuch as if an existing system classifies this as EMERGENT\\nAfter you’ve written LFs, you can apply them to the samples you want to label.\\nBecause LFs encode heuristics, and heuristics are noisy, labels produced by LFs are\\nnoisy. Multiple LFs might apply to the same data examples, and they might give\\nconflicting labels. One function might think a nurse’s note is EMERGENT but another\\nfunction might think it’s not. One heuristic might be much more accurate than\\nanother heuristic, which you might not know because you don’t have ground truth\\nlabels to compare them to. It’s important to combine, denoise, and reweight all LFs to\\nLabeling | 95', '12 Ratner et al., “Snorkel: Rapid Training Data Creation with Weak Supervision. ”\\nget a set of most likely to be correct labels. Figure 4-4 shows at a high level how LFs\\nwork.\\nFigure 4-4. A high-level overview of how labeling functions are combined. Source:\\nAdapted from an image by Ratner et al.12\\nIn theory, you don’t need any hand labels for weak supervision. However, to get a\\nsense of how accurate your LFs are, a small number of hand labels is recommended.\\nThese hand labels can help you discover patterns in your data to write better LFs.\\nWeak supervision can be especially useful when your data has strict privacy require‐\\nments. Y ou only need to see a small, cleared subset of data to write LFs, which can be\\napplied to the rest of your data without anyone looking at it.\\nWith LFs, subject matter expertise can be versioned, reused, and shared. Expertise\\nowned by one team can be encoded and used by another team. If your data changes\\nor your requirements change, you can just reapply LFs to your data samples. The\\napproach of using LFs to generate labels for your data is also known as programmatic\\nlabeling. Table 4-3 shows some of the advantages of programmatic labeling over hand\\nlabeling.\\nTable 4-3. The advantages of programmatic labeling over hand labeling\\nHand labeling Programmatic labeling\\nExpensive: Especially when subject matter expertise\\nrequired\\nCost saving: Expertise can be versioned, shared, and reused across an\\norganization\\nLack of privacy: Need to ship data to human\\nannotators\\nPrivacy: Create LFs using a cleared data subsample and then apply\\nLFs to other data without looking at individual samples\\nSlow: Time required scales linearly with number of\\nlabels needed\\nFast: Easily scale from 1K to 1M samples\\nNonadaptive: Every change requires relabeling the\\ndata\\nAdaptive: When changes happen, just reapply LFs!\\n96 | Chapter 4: Training Data', '13 Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Matthew P . Lungren, Daniel L. Rubin, and Christopher\\nRé, “Cross-Modal Data Programming Enables Rapid Medical Machine Learning, ” Patterns 1, no. 2 (2020):\\n100019, https://oreil.ly/nKt8E.\\n14 The two tasks in this study use only 18 and 20 LFs respectively. In practice, I’ve seen teams using hundreds of\\nLFs for each task.\\n15 Dummon et al., “Cross-Modal Data Programming. ”\\nHere is a case study to show how well weak supervision works in practice. In a study\\nwith Stanford Medicine,13 models trained with weakly supervised labels obtained by a\\nsingle radiologist after eight hours of writing LFs had comparable performance with\\nmodels trained on data obtained through almost a year of hand labeling, as shown in\\nFigure 4-5. There are two interesting facts about the results of the experiment. First,\\nthe models continued improving with more unlabeled data even without more LFs.\\nSecond, LFs were being reused across tasks. The researchers were able to reuse six\\nLFs between the CXR (chest X-rays) task and EXR (extremity X-rays) task.14\\nFigure 4-5. Comparison of the performance of a model trained on fully supervised labels\\n(FS) and a model trained with programmatic labels (DP) on CXR and EXR tasks.\\nSource: Dunnmon et al.15\\nMy students often ask that if heuristics work so well to label data, why do we need\\nML models? One reason is that LFs might not cover all data samples, so we can train\\nML models on data programmatically labeled with LFs and use this trained model to\\ngenerate predictions for samples that aren’t covered by any LF .\\nWeak supervision is a simple but powerful paradigm. However, it’s not perfect. In\\nsome cases, the labels obtained by weak supervision might be too noisy to be useful.\\nBut even in these cases, weak supervision can be a good way to get you started when\\nLabeling | 97', '16 Avrim Blum and Tom Mitchell, “Combining Labeled and Unlabeled Data with Co-Training, ” in Proceedings of\\nthe Eleventh Annual Conference on Computational Learning Theory (July 1998): 92–100, https://oreil.ly/T79AE.\\nyou want to explore the effectiveness of ML without wanting to invest too much in\\nhand labeling up front.\\nSemi-supervision\\nIf weak supervision leverages heuristics to obtain noisy labels, semi-supervision\\nleverages structural assumptions to generate new labels based on a small set of initial\\nlabels. Unlike weak supervision, semi-supervision requires an initial set of labels.\\nSemi-supervised learning is a technique that was used back in the 90s, 16 and since\\nthen many semi-supervision methods have been developed. A comprehensive review\\nof semi-supervised learning is out of the scope of this book. We’ll go over a small\\nsubset of these methods to give readers a sense of how they are used. For a compre‐\\nhensive review, I recommend “Semi-Supervised Learning Literature Survey”  (Xiaojin\\nZhu, 2008) and “ A Survey on Semi-Supervised Learning” (Engelen and Hoos, 2018).\\nA classic semi-supervision method is self-training. Y ou start by training a model on\\nyour existing set of labeled data and use this model to make predictions for unlabeled\\nsamples. Assuming that predictions with high raw probability scores are correct, you\\nadd the labels predicted with high probability to your training set and train a new\\nmodel on this expanded training set. This goes on until you’re happy with your model\\nperformance.\\nAnother semi-supervision method assumes that data samples that share similar char‐\\nacteristics share the same labels. The similarity might be obvious, such as in the task\\nof classifying the topic of Twitter hashtags. Y ou can start by labeling the hashtag\\n“#AI” as Computer Science. Assuming that hashtags that appear in the same tweet or\\nprofile are likely about the same topic, given the profile of MIT CSAIL in Figure 4-6,\\nyou can also label the hashtags “#ML ” and “#BigData” as Computer Science.\\nFigure 4-6. Because #ML and #BigData appear in the same Twitter profile as #AI, we\\ncan assume that they belong to the same topic\\n98 | Chapter 4: Training Data', '17 Avital Oliver, Augustus Odena, Colin Raffel, Ekin D. Cubuk, and Ian J. Goodfellow, “Realistic Evaluation of\\nDeep Semi-Supervised Learning Algorithms, ” NeurIPS 2018 Proceedings, https://oreil.ly/dRmPV.\\n18 A token can be a word, a character, or part of a word.\\nIn most cases, the similarity can only be discovered by more complex methods.\\nFor example, you might need to use a clustering method or a k-nearest neighbors\\nalgorithm to discover samples that belong to the same cluster.\\nA semi-supervision method that has gained popularity in recent years is the\\nperturbation-based method. It’s based on the assumption that small perturbations\\nto a sample shouldn’t change its label. So you apply small perturbations to your train‐\\ning instances to obtain new training instances. The perturbations might be applied\\ndirectly to the samples (e.g., adding white noise to images) or to their representations\\n(e.g., adding small random values to embeddings of words). The perturbed samples\\nhave the same labels as the unperturbed samples. We’ll discuss more about this in the\\nsection “Perturbation” on page 114.\\nIn some cases, semi-supervision approaches have reached the performance of purely\\nsupervised learning, even when a substantial portion of the labels in a given dataset\\nhas been discarded.17\\nSemi-supervision is the most useful when the number of training labels is limited.\\nOne thing to consider when doing semi-supervision with limited data is how much\\nof this limited data should be used to evaluate multiple candidate models and select\\nthe best one. If you use a small amount, the best performing model on this small\\nevaluation set might be the one that overfits the most to this set. On the other hand,\\nif you use a large amount of data for evaluation, the performance boost gained by\\nselecting the best model based on this evaluation set might be less than the boost\\ngained by adding the evaluation set to the limited training set. Many companies\\novercome this trade-off by using a reasonably large evaluation set to select the best\\nmodel, then continuing training the champion model on the evaluation set.\\nTransfer learning\\nTransfer learning refers to the family of methods where a model developed for a task\\nis reused as the starting point for a model on a second task. First, the base model is\\ntrained for a base task. The base task is usually a task that has cheap and abundant\\ntraining data. Language modeling is a great candidate because it doesn’t require', 'is reused as the starting point for a model on a second task. First, the base model is\\ntrained for a base task. The base task is usually a task that has cheap and abundant\\ntraining data. Language modeling is a great candidate because it doesn’t require\\nlabeled data. Language models can be trained on any body of text—books, Wikipedia\\narticles, chat histories—and the task is: given a sequence of tokens, 18 predict the next\\ntoken. When given the sequence “I bought NVIDIA shares because I believe in the\\nimportance of, ” a language model might output “hardware” or “GPU” as the next\\ntoken.\\nLabeling | 99', '19 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-tuning for Text Classification, ” arXiv,\\nJanuary 18, 2018, https://oreil.ly/DBEbw.\\n20 Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig, “Pre-train,\\nPrompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing, ” arXiv, July\\n28, 2021, https://oreil.ly/0lBgn.\\n21 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, “BERT: Pre-training of Deep Bidirec‐\\ntional Transformers for Language Understanding, ” arXiv, October 11, 2018, https://oreil.ly/RdIGU; Tom B.\\nBrown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\\net al., “Language Models Are Few-Shot Learners, ” OpenAI, 2020, https://oreil.ly/YVmrr.\\nThe trained model can then be used for the task that you’re interested in—a down‐\\nstream task—such as sentiment analysis, intent detection, or question answering. In\\nsome cases, such as in zero-shot learning scenarios, you might be able to use the base\\nmodel on a downstream task directly. In many cases, you might need to fine-tune\\nthe base model. Fine-tuning means making small changes to the base model, such as\\ncontinuing to train the base model or a part of the base model on data from a given\\ndownstream task.19\\nSometimes, you might need to modify the inputs using a template to prompt the base\\nmodel to generate the outputs you want. 20 For example, to use a language model as\\nthe base model for a question answering task, you might want to use this prompt:\\nQ: When was the United States founded?\\nA: July 4, 1776.\\nQ: Who wrote the Declaration of Independence?\\nA: Thomas Jefferson.\\nQ: What year was Alexander Hamilton born?\\nA:\\nWhen you input this prompt into a language model such as GPT-3, it might output\\nthe year Alexander Hamilton was born.\\nTransfer learning is especially appealing for tasks that don’t have a lot of labeled\\ndata. Even for tasks that have a lot of labeled data, using a pretrained model as the\\nstarting point can often boost the performance significantly compared to training\\nfrom scratch.\\nTransfer learning has gained a lot of interest in recent years for the right reasons.\\nIt has enabled many applications that were previously impossible due to the lack\\nof training samples. A nontrivial portion of ML models in production today are\\nthe results of transfer learning, including object detection models that leverage mod‐', 'It has enabled many applications that were previously impossible due to the lack\\nof training samples. A nontrivial portion of ML models in production today are\\nthe results of transfer learning, including object detection models that leverage mod‐\\nels pretrained on ImageNet and text classification models that leverage pretrained\\nlanguage models such as BERT or GPT-3. 21 Transfer learning also lowers the entry\\n100 | Chapter 4: Training Data', '22 Burr Settles, Active Learning (Williston, VT: Morgan & Claypool, 2012).\\nbarriers into ML, as it helps reduce the up-front cost needed for labeling data to build\\nML applications.\\nA trend that has emerged in the last five years is that (usually) the larger the pre‐\\ntrained base model, the better its performance on downstream tasks. Large models\\nare expensive to train. Based on the configuration of GPT-3, it’s estimated that the\\ncost of training this model is in the tens of millions USD. Many have hypothesized\\nthat in the future only a handful of companies will be able to afford to train large\\npretrained models. The rest of the industry will use these pretrained models directly\\nor fine-tune them for their specific needs.\\nActive learning\\nActive learning is a method for improving the efficiency of data labels. The hope\\nhere is that ML models can achieve greater accuracy with fewer training labels if\\nthey can choose which data samples to learn from. Active learning is sometimes\\ncalled query learning—though this term is getting increasingly unpopular—because\\na model (active learner) sends back queries in the form of unlabeled samples to be\\nlabeled by annotators (usually humans).\\nInstead of randomly labeling data samples, you label the samples that are most help‐\\nful to your models according to some metrics or heuristics. The most straightforward\\nmetric is uncertainty measurement—label the examples that your model is the least\\ncertain about, hoping that they will help your model learn the decision boundary bet‐\\nter. For example, in the case of classification problems where your model outputs raw\\nprobabilities for different classes, it might choose the data samples with the lowest\\nprobabilities for the predicted class. Figure 4-7 illustrates how well this method works\\non a toy example.\\nFigure 4-7. How uncertainty-based active learning works. (a) A toy dataset of 400\\ninstances, evenly sampled from two class Gaussians. (b) A model trained on 30 samples\\nrandomly labeled gives an accuracy of 70%. (c) A model trained on 30 samples chosen\\nby active learning gives an accuracy of 90%. Source: Burr Settles22\\nLabeling | 101', '23 We’ll cover ensembles in Chapter 6.\\n24 Dana Angluin, “Queries and Concept Learning, ” Machine Learning 2 (1988): 319–42, https://oreil.ly/0uKs4.\\n25 Thanks to Eugene Y an for this wonderful example!\\nAnother common heuristic is based on disagreement among multiple candidate\\nmodels. This method is called query-by-committee, an example of an ensemble\\nmethod.23 Y ou need a committee of several candidate models, which are usually the\\nsame model trained with different sets of hyperparameters or the same model trained\\non different slices of data. Each model can make one vote for which samples to label\\nnext, and it might vote based on how uncertain it is about the prediction. Y ou then\\nlabel the samples that the committee disagrees on the most.\\nThere are other heuristics such as choosing samples that, if trained on them, will give\\nthe highest gradient updates or will reduce the loss the most. For a comprehensive\\nreview of active learning methods, check out “ Active Learning Literature Survey”\\n(Settles 2010).\\nThe samples to be labeled can come from different data regimes. They can be synthe‐\\nsized where your model generates samples in the region of the input space that it’s\\nmost uncertain about. 24 They can come from a stationary distribution where you’ve\\nalready collected a lot of unlabeled data and your model chooses samples from this\\npool to label. They can come from the real-world distribution where you have a\\nstream of data coming in, as in production, and your model chooses samples from\\nthis stream of data to label.\\nI’m most excited about active learning when a system works with real-time data. Data\\nchanges all the time, a phenomenon we briefly touched on in Chapter 1  and will\\nfurther detail in Chapter 8. Active learning in this data regime will allow your model\\nto learn more effectively in real time and adapt faster to changing environments.\\nClass Imbalance\\nClass imbalance typically refers to a problem in classification tasks where there is a\\nsubstantial difference in the number of samples in each class of the training data. For\\nexample, in a training dataset for the task of detecting lung cancer from X-ray images,\\n99.99% of the X-rays might be of normal lungs, and only 0.01% might contain\\ncancerous cells.\\nClass imbalance can also happen with regression tasks where the labels are continu‐\\nous. Consider the task of estimating health-care bills. 25 Health-care bills are highly\\nskewed—the median bill is low, but the 95th percentile bill is astronomical. When', 'cancerous cells.\\nClass imbalance can also happen with regression tasks where the labels are continu‐\\nous. Consider the task of estimating health-care bills. 25 Health-care bills are highly\\nskewed—the median bill is low, but the 95th percentile bill is astronomical. When\\npredicting hospital bills, it might be more important to predict accurately the bills\\nat the 95th percentile than the median bills. A 100% difference in a $250 bill is\\nacceptable (actual $500, predicted $250), but a 100% difference on a $10k bill is not\\n102 | Chapter 4: Training Data', '26 Andrew Ng, “Bridging AI’s Proof-of-Concept to Production Gap” (HAI Seminar, September 22, 2020), video,\\n1:02:07, https://oreil.ly/FSFWS.\\n27 And this is why accuracy is a bad metric for tasks with class imbalance, as we’ll explore more in the section\\n“Handling Class Imbalance” on page 105.\\n(actual $20k, predicted $10k). Therefore, we might have to train the model to be\\nbetter at predicting 95th percentile bills, even if it reduces the overall metrics.\\nChallenges of Class Imbalance\\nML, especially deep learning, works well in situations when the data distribution is\\nmore balanced, and usually not so well when the classes are heavily imbalanced, as\\nillustrated in Figure 4-8. Class imbalance can make learning difficult for the following\\nthree reasons.\\nFigure 4-8. ML works well in situations where the classes are balanced. Source: Adapted\\nfrom an image by Andrew Ng26\\nThe first reason is that class imbalance often means there’s insufficient signal for\\nyour model to learn to detect the minority classes. In the case where there is a small\\nnumber of instances in the minority class, the problem becomes a few-shot learning\\nproblem where your model only gets to see the minority class a few times before\\nhaving to make a decision on it. In the case where there is no instance of the rare\\nclasses in your training set, your model might assume these rare classes don’t exist.\\nThe second reason is that class imbalance makes it easier for your model to get\\nstuck in a nonoptimal solution by exploiting a simple heuristic instead of learning\\nanything useful about the underlying pattern of the data. Consider the preceding lung\\ncancer detection example. If your model learns to always output the majority class,\\nits accuracy is already 99.99%. 27 This heuristic can be very hard for gradient descent\\nClass Imbalance | 103', '28 I imagined that it’ d be easier to learn ML theory if I didn’t have to figure out how to deal with class imbalance.\\n29 The Nilson Report, “Payment Card Fraud Losses Reach $27.85 Billion, ” PR Newswire, November 21, 2019,\\nhttps://oreil.ly/NM5zo.\\n30 “Job Market Expert Explains Why Only 2% of Job Seekers Get Interviewed, ” WebWire, January 7, 2014,\\nhttps://oreil.ly/UpL8S.\\n31 “Email and Spam Data, ” Talos Intelligence, last accessed May 2021, https://oreil.ly/lI5Jr.\\nalgorithms to beat because a small amount of randomness added to this heuristic\\nmight lead to worse accuracy.\\nThe third reason is that class imbalance leads to asymmetric costs of error—the cost\\nof a wrong prediction on a sample of the rare class might be much higher than a\\nwrong prediction on a sample of the majority class.\\nFor example, misclassification on an X-ray with cancerous cells is much more dan‐\\ngerous than misclassification on an X-ray of a normal lung. If your loss function isn’t\\nconfigured to address this asymmetry, your model will treat all samples the same way.\\nAs a result, you might obtain a model that performs equally well on both majority\\nand minority classes, while you much prefer a model that performs less well on the\\nmajority class but much better on the minority one.\\nWhen I was in school, most datasets I was given had more or less balanced classes. 28\\nIt was a shock for me to start working and realize that class imbalance is the norm. In\\nreal-world settings, rare events are often more interesting (or more dangerous) than\\nregular events, and many tasks focus on detecting those rare events.\\nThe classical example of tasks with class imbalance is fraud detection. Most credit\\ncard transactions are not fraudulent. As of 2018, 6.8¢ for every $100 in cardholder\\nspending is fraudulent.29 Another is churn prediction. The majority of your custom‐\\ners are probably not planning on canceling their subscription. If they are, your\\nbusiness has more to worry about than churn prediction algorithms. Other examples\\ninclude disease screening (most people, fortunately, don’t have terminal illness) and\\nresume screening (98% of job seekers are eliminated at the initial resume screening30).\\nA less obvious example of a task with class imbalance is object detection . Object\\ndetection algorithms currently work by generating a large number of bounding boxes\\nover an image then predicting which boxes are most likely to have objects in them.\\nMost bounding boxes do not contain a relevant object.', 'detection algorithms currently work by generating a large number of bounding boxes\\nover an image then predicting which boxes are most likely to have objects in them.\\nMost bounding boxes do not contain a relevant object.\\nOutside the cases where class imbalance is inherent in the problem, class imbalance\\ncan also be caused by biases during the sampling process. Consider the case when you\\nwant to create training data to detect whether an email is spam or not. Y ou decide\\nto use all the anonymized emails from your company’s email database. According to\\nTalos Intelligence, as of May 2021, nearly 85% of all emails are spam.31 But most spam\\n104 | Chapter 4: Training Data', '32 Nathalie Japkowciz and Shaju Stephen, “The Class Imbalance Problem: A Systematic Study, ” 2002,\\nhttps://oreil.ly/d7lVu.\\n33 Nathalie Japkowicz, “The Class Imbalance Problem: Significance and Strategies, ” 2000, https://oreil.ly/Ma50Z.\\n34 Wan Ding, Dong-Y an Huang, Zhuo Chen, Xinguo Yu, and Weisi Lin, “Facial Action Recognition Using\\nVery Deep Networks for Highly Imbalanced Class Distribution, ” 2017 Asia-Pacific Signal and Information\\nProcessing Association Annual Summit and Conference (APSIPA ASC), 2017, https://oreil.ly/WeW6J.\\nemails were filtered out before they reached your company’s database, so in your\\ndataset, only a small percentage is spam.\\nAnother cause for class imbalance, though less common, is due to labeling errors.\\nAnnotators might have read the instructions wrong or followed the wrong instruc‐\\ntions (thinking there are only two classes, POSITIVE and NEGATIVE, while there\\nare actually three), or simply made errors. Whenever faced with the problem of class\\nimbalance, it’s important to examine your data to understand the causes of it.\\nHandling Class Imbalance\\nBecause of its prevalence in real-world applications, class imbalance has been thor‐\\noughly studied over the last two decades. 32 Class imbalance affects tasks differently\\nbased on the level of imbalance. Some tasks are more sensitive to class imbalance\\nthan others. Japkowicz showed that sensitivity to imbalance increases with the\\ncomplexity of the problem, and that noncomplex, linearly separable problems are\\nunaffected by all levels of class imbalance. 33 Class imbalance in binary classification\\nproblems is a much easier problem than class imbalance in multiclass classification\\nproblems. Ding et al. showed that very deep neural networks—with “very deep”\\nmeaning over 10 layers back in 2017—performed much better on imbalanced data\\nthan shallower neural networks.34\\nThere have been many techniques suggested to mitigate the effect of class imbalance.\\nHowever, as neural networks have grown to be much larger and much deeper, with\\nmore learning capacity, some might argue that you shouldn’t try to “fix” class imbal‐\\nance if that’s how the data looks in the real world. A good model should learn to\\nmodel that imbalance. However, developing a model good enough for that can be\\nchallenging, so we still have to rely on special training techniques.\\nIn this section, we will cover three approaches to handling class imbalance: choosing', 'model that imbalance. However, developing a model good enough for that can be\\nchallenging, so we still have to rely on special training techniques.\\nIn this section, we will cover three approaches to handling class imbalance: choosing\\nthe right metrics for your problem; data-level methods, which means changing the\\ndata distribution to make it less imbalanced; and algorithm-level methods, which\\nmeans changing your learning method to make it more robust to class imbalance.\\nThese techniques might be necessary but not sufficient. For a comprehensive survey,\\nI recommend “Survey on Deep Learning with Class Imbalance” (Johnson and Khosh‐\\ngoftaar 2019).\\nClass Imbalance | 105', 'Using the right evaluation metrics\\nThe most important thing to do when facing a task with class imbalance is to choose\\nthe appropriate evaluation metrics. Wrong metrics will give you the wrong ideas of\\nhow your models are doing and, subsequently, won’t be able to help you develop or\\nchoose models good enough for your task.\\nThe overall accuracy and error rate are the most frequently used metrics to report\\nthe performance of ML models. However, these are insufficient metrics for tasks with\\nclass imbalance because they treat all classes equally, which means the performance of\\nyour model on the majority class will dominate these metrics. This is especially bad\\nwhen the majority class isn’t what you care about.\\nConsider a task with two labels: CANCER (the positive class) and NORMAL (the\\nnegative class), where 90% of the labeled data is NORMAL. Consider two models, A\\nand B, with the confusion matrices shown in Tables 4-4 and 4-5.\\nTable 4-4. Model A’s confusion matrix; model A can detect 10 out of 100 CANCER cases\\nModel A Actual CANCER Actual NORMAL\\nPredicted CANCER 10 10\\nPredicted NORMAL 90 890\\nTable 4-5. Model B’s confusion matrix; model B can detect 90 out of 100 CANCER cases\\nModel B Actual CANCER Actual NORMAL\\nPredicted CANCER 90 90\\nPredicted NORMAL 10 810\\nIf you’re like most people, you’ d probably prefer model B to make predictions for you\\nsince it has a better chance of telling you if you actually have cancer. However, they\\nboth have the same accuracy of 0.9.\\nMetrics that help you understand your model’s performance with respect to specific\\nclasses would be better choices. Accuracy can still be a good metric if you use it for\\neach class individually. The accuracy of model A on the CANCER class is 10% and\\nthe accuracy of model B on the CANCER class is 90%.\\n106 | Chapter 4: Training Data', '35 As of July 2021, when you use scikit-learn.metrics.f1_score, pos_label is set to 1 by default, but you can\\nchange it to 0 if you want 0 to be your positive label.\\nF1, precision, and recall are metrics that measure your model’s performance with\\nrespect to the positive class in binary classification problems, as they rely on true\\npositive—an outcome where the model correctly predicts the positive class.35\\nPrecision, Recall, and F1\\nFor readers needing a refresh, precision, recall, and F1 scores, for binary tasks, are\\ncalculated using the count of true positives, true negatives, false positives, and false\\nnegatives. Definitions for these terms are shown in Table 4-6.\\nTable 4-6. Definitions of True Positive, False Positive, False Negative,\\nand True Negative in a binary classification task\\nPredicted Positive Predicted Negative\\nPositive label True Positive (hit) False Negative (type II error, miss)\\nNegative label False Positive (type I error, false alarm) True Negative (correct rejection)\\nPrecision = True Positive / (True Positive + False Positive)\\nRecall = True Positive / (True Positive + False Negative)\\nF1 = 2 × Precision × Recall / (Precision + Recall)\\nF1, precision, and recall are asymmetric metrics, which means that their values\\nchange depending on which class is considered the positive class. In our case, if we\\nconsider CANCER the positive class, model A ’s F1 is 0.17. However, if we consider\\nNORMAL the positive class, model A ’s F1 is 0.95. Accuracy, precision, recall, and F1\\nscores of model A and model B when CANCER is the positive class are shown in\\nTable 4-7.\\nTable 4-7. Both models have the same accuracy even though one model is clearly superior\\nCANCER (1) NORMAL (0) Accuracy Precision Recall F1\\nModel A 10/100 890/900 0.9 0.5 0.1 0.17\\nModel B 90/100 810/900 0.9 0.5 0.9 0.64\\nMany classification problems can be modeled as regression problems. Y our model\\ncan output a probability, and based on that probability, you classify the sample. For\\nexample, if the value is greater than 0.5, it’s a positive label, and if it’s less than or\\nClass Imbalance | 107', '36 Jesse Davis and Mark Goadrich, “The Relationship Between Precision-Recall and ROC Curves, ” Proceedings of\\nthe 23rd International Conference on Machine Learning, 2006, https://oreil.ly/s40F3.\\nequal to 0.5, it’s a negative label. This means that you can tune the threshold to\\nincrease the true positive rate (also known as recall) while decreasing the false positive\\nrate (also known as the probability of false alarm), and vice versa. We can plot the true\\npositive rate against the false positive rate for different thresholds. This plot is known\\nas the ROC curve  (receiver operating characteristics). When your model is perfect,\\nthe recall is 1.0, and the curve is just a line at the top. This curve shows you how\\nyour model’s performance changes depending on the threshold, and helps you choose\\nthe threshold that works best for you. The closer to the perfect line, the better your\\nmodel’s performance.\\nThe area under the curve (AUC) measures the area under the ROC curve. Since\\nthe closer to the perfect line the better, the larger this area the better, as shown in\\nFigure 4-9.\\nFigure 4-9. ROC curve\\nLike F1 and recall, the ROC curve focuses only on the positive class and doesn’t show\\nhow well your model does on the negative class. Davis and Goadrich suggested that\\nwe should plot precision against recall instead, in what they termed the Precision-\\nRecall Curve. They argued that this curve gives a more informative picture of an\\nalgorithm’s performance on tasks with heavy class imbalance.36\\n108 | Chapter 4: Training Data', '37 Rafael Alencar, “Resampling Strategies for Imbalanced Datasets, ” Kaggle, https://oreil.ly/p8Whs.\\n38 Ivan Tomek, “ An Experiment with the Edited Nearest-Neighbor Rule, ” IEEE Transactions on Systems, Man,\\nand Cybernetics (June 1976): 448–52, https://oreil.ly/JCxHZ.\\n39 N.V . Chawla, K.W . Bowyer, L.O. Hall, and W .P . Kegelmeyer, “SMOTE: Synthetic Minority Over-sampling\\nTechnique, Journal of Artificial Intelligence Research 16 (2002): 341–78, https://oreil.ly/f6y46.\\nData-level methods: Resampling\\nData-level methods modify the distribution of the training data to reduce the level of\\nimbalance to make it easier for the model to learn. A common family of techniques\\nis resampling. Resampling includes oversampling, adding more instances from the\\nminority classes, and undersampling, removing instances of the majority classes. The\\nsimplest way to undersample is to randomly remove instances from the majority\\nclass, whereas the simplest way to oversample is to randomly make copies of the\\nminority class until you have a ratio that you’re happy with. Figure 4-10  shows a\\nvisualization of oversampling and undersampling.\\nFigure 4-10. Illustrations of how undersampling and oversampling work. Source:\\nAdapted from an image by Rafael Alencar37\\nA popular method of undersampling low-dimensional data that was developed back\\nin 1976 is Tomek links.38 With this technique, you find pairs of samples from opposite\\nclasses that are close in proximity and remove the sample of the majority class in each\\npair.\\nWhile this makes the decision boundary more clear and arguably helps models learn\\nthe boundary better, it may make the model less robust because the model doesn’t get\\nto learn from the subtleties of the true decision boundary.\\nA popular method of oversampling low-dimensional data is SMOTE (synthetic\\nminority oversampling technique).39 It synthesizes novel samples of the minority class\\nClass Imbalance | 109', '40 “Convex” here approximately means “linear. ”\\n41 Jianping Zhang and Inderjeet Mani, “kNN Approach to Unbalanced Data Distributions: A Case Study\\ninvolving Information Extraction” (Workshop on Learning from Imbalanced Datasets II, ICML, Washington,\\nDC, 2003), https://oreil.ly/qnpra; Miroslav Kubat and Stan Matwin, “ Addressing the Curse of Imbalanced\\nTraining Sets: One-Sided Selection, ” 2000, https://oreil.ly/8pheJ.\\n42 Hansang Lee, Minseok Park, and Junmo Kim, “Plankton Classification on Imbalanced Large Scale Database\\nvia Convolutional Neural Networks with Transfer Learning, ” 2016 IEEE International Conference on Image\\nProcessing (ICIP), 2016, https://oreil.ly/YiA8p.\\n43 Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent Gauen, Ryan Dailey, et\\nal., “Dynamic Sampling in Convolutional Neural Networks for Imbalanced Data Classification, ” 2018 IEEE\\nConference on Multimedia Information Processing and Retrieval (MIPR), 2018, https://oreil.ly/D3Ak5.\\nthrough sampling convex combinations of existing data points within the minority\\nclass.40\\nBoth SMOTE and Tomek links have only been proven effective in low-dimensional\\ndata. Many of the sophisticated resampling techniques, such as Near-Miss and\\none-sided selection,41 require calculating the distance between instances or between\\ninstances and the decision boundaries, which can be expensive or infeasible for\\nhigh-dimensional data or in high-dimensional feature space, such as the case with\\nlarge neural networks.\\nWhen you resample your training data, never evaluate your model on resampled\\ndata, since it will cause your model to overfit to that resampled distribution.\\nUndersampling runs the risk of losing important data from removing data. Oversam‐\\npling runs the risk of overfitting on training data, especially if the added copies of the\\nminority class are replicas of existing data. Many sophisticated sampling techniques\\nhave been developed to mitigate these risks.\\nOne such technique is two-phase learning. 42 Y ou first train your model on the resam‐\\npled data. This resampled data can be achieved by randomly undersampling large\\nclasses until each class has only N instances. Y ou then fine-tune your model on the\\noriginal data.\\nAnother technique is dynamic sampling: oversample the low-performing classes and\\nundersample the high-performing classes during the training process. Introduced by\\nPouyanfar et al., 43 the method aims to show the model less of what it has already', 'original data.\\nAnother technique is dynamic sampling: oversample the low-performing classes and\\nundersample the high-performing classes during the training process. Introduced by\\nPouyanfar et al., 43 the method aims to show the model less of what it has already\\nlearned and more of what it has not.\\nAlgorithm-level methods\\nIf data-level methods mitigate the challenge of class imbalance by altering the distri‐\\nbution of your training data, algorithm-level methods keep the training data distribu‐\\ntion intact but alter the algorithm to make it more robust to class imbalance.\\n110 | Chapter 4: Training Data', '44 Charles Elkan, “The Foundations of Cost-Sensitive Learning, ” Proceedings of the Seventeenth International\\nJoint Conference on Artificial Intelligence (IJCAI’01), 2001, https://oreil.ly/WGq5M.\\nBecause the loss function (or the cost function) guides the learning process, many\\nalgorithm-level methods involve adjustment to the loss function. The key idea is\\nthat if there are two instances, x1 and x2, and the loss resulting from making the\\nwrong prediction on x1 is higher than x2, the model will prioritize making the correct\\nprediction on x1 over making the correct prediction on x2. By giving the training\\ninstances we care about higher weight, we can make the model focus more on\\nlearning these instances.\\nLet Lx;θ be the loss caused by the instance x for the model with the parameter set θ.\\nThe model’s loss is often defined as the average loss caused by all instances. N denotes\\nthe total number of training samples.\\nL X; θ = ∑x\\n1\\nN L x; θ\\nThis loss function values the loss caused by all instances equally, even though wrong\\npredictions on some instances might be much costlier than wrong predictions on\\nother instances. There are many ways to modify this cost function. In this section, we\\nwill focus on three of them, starting with cost-sensitive learning.\\nCost-sensitive learning.    Back in 2001, based on the insight that misclassification of\\ndifferent classes incurs different costs, Elkan proposed cost-sensitive learning in\\nwhich the individual loss function is modified to take into account this varying cost. 44\\nThe method started by using a cost matrix to specify Cij: the cost if class i is classified\\nas class j. If i = j, it’s a correct classification, and the cost is usually 0. If not, it’s a\\nmisclassification. If classifying POSITIVE examples as NEGATIVE is twice as costly\\nas the other way around, you can make C10 twice as high as C01.\\nFor example, if you have two classes, POSITIVE and NEGATIVE, the cost matrix can\\nlook like that in Table 4-8.\\nTable 4-8. Example of a cost matrix\\nActual NEGATIVE Actual POSITIVE\\nPredicted NEGATIVE C(0, 0) = C00 C(1, 0) = C10\\nPredicted POSITIVE C(0, 1) = C01 C(1, 1) = C11\\nClass Imbalance | 111', '45 Yin Cui, Menglin Jia, Tsung-Yi Lin, Y ang Song, and Serge Belongie, “Class-Balanced Loss Based on\\nEffective Number of Samples, ” Proceedings of the Conference on Computer Vision and Pattern, 2019,\\nhttps://oreil.ly/jCzGH.\\n46 Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár, “Focal Loss for Dense Object\\nDetection, ” arXiv, August 7, 2017, https://oreil.ly/Km2dF.\\nThe loss caused by instance x of class i will become the weighted average of all\\npossible classifications of instance x.\\nL x; θ = ∑ jCijP j x; θ\\nThe problem with this loss function is that you have to manually define the cost\\nmatrix, which is different for different tasks at different scales.\\nClass-balanced loss.    What might happen with a model trained on an imbalanced data‐\\nset is that it’ll bias toward majority classes and make wrong predictions on minority\\nclasses. What if we punish the model for making wrong predictions on minority\\nclasses to correct this bias?\\nIn its vanilla form, we can make the weight of each class inversely proportional to the\\nnumber of samples in that class, so that the rarer classes have higher weights. In the\\nfollowing equation, N denotes the total number of training samples:\\nWi = N\\nnumber of samples of class i\\nThe loss caused by instance x of class i will become as follows, with Loss( x, j) being\\nthe loss when x is classified as class j. It can be cross entropy or any other loss\\nfunction.\\nL x; θ = Wi∑jP j x; θ Loss x, j\\nA more sophisticated version of this loss can take into account the overlap among\\nexisting samples, such as class-balanced loss based on effective number of samples.45\\nFocal loss.    In our data, some examples are easier to classify than others, and our\\nmodel might learn to classify them quickly. We want to incentivize our model to\\nfocus on learning the samples it still has difficulty classifying. What if we adjust the\\nloss so that if a sample has a lower probability of being right, it’ll have a higher\\nweight? This is exactly what focal loss does. 46 The equation for focal loss and its\\nperformance compared to cross entropy loss is shown in Figure 4-11.\\n112 | Chapter 4: Training Data', '47 Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera, “ A Review\\non Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches, ” IEEE\\nTransactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, no. 4 (July 2012):\\n463–84, https://oreil.ly/1ND4g.\\nIn practice, ensembles have shown to help with the class imbalance problem. 47 How‐\\never, we don’t include ensembling in this section because class imbalance isn’t usually\\nwhy ensembles are used. Ensemble techniques will be covered in Chapter 6.\\nFigure 4-11. The model trained with focal loss (FL) shows reduced loss values compared\\nto the model trained with cross entropy loss (CE). Source: Adapted from an image by Lin\\net al.\\nData Augmentation\\nData augmentation is a family of techniques that are used to increase the amount\\nof training data. Traditionally, these techniques are used for tasks that have limited\\ntraining data, such as in medical imaging. However, in the last few years, they have\\nshown to be useful even when we have a lot of data—augmented data can make our\\nmodels more robust to noise and even adversarial attacks.\\nData augmentation has become a standard step in many computer vision tasks and\\nis finding its way into natural language processing (NLP) tasks. The techniques\\ndepend heavily on the data format, as image manipulation is different from text\\nmanipulation. In this section, we will cover three main types of data augmentation:\\nsimple label-preserving transformations; perturbation, which is a term for “adding\\nData Augmentation | 113', '48 Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton, “ImageNet Classification with Deep Convolutional\\nNeural Networks, 2012, https://oreil.ly/aphzA.\\n49 Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi, “One Pixel Attack for Fooling Deep Neural\\nNetworks, ” IEEE Transactions on Evolutionary Computation 23, no. 5 (2019): 828–41, https://oreil.ly/LzN9D.\\nnoises”; and data synthesis. In each type, we’ll go over examples for both computer\\nvision and NLP .\\nSimple Label-Preserving Transformations\\nIn computer vision, the simplest data augmentation technique is to randomly modify\\nan image while preserving its label. Y ou can modify the image by cropping, flipping,\\nrotating, inverting (horizontally or vertically), erasing part of the image, and more.\\nThis makes sense because a rotated image of a dog is still a dog. Common ML frame‐\\nworks like PyTorch, TensorFlow, and Keras all have support for image augmentation.\\nAccording to Krizhevsky et al. in their legendary AlexNet paper, “The transformed\\nimages are generated in Python code on the CPU while the GPU is training on\\nthe previous batch of images. So these data augmentation schemes are, in effect,\\ncomputationally free. ”48\\nIn NLP , you can randomly replace a word with a similar word, assuming that this\\nreplacement wouldn’t change the meaning or the sentiment of the sentence, as shown\\nin Table 4-9. Similar words can be found either with a dictionary of synonymous\\nwords or by finding words whose embeddings are close to each other in a word\\nembedding space.\\nTable 4-9. Three sentences generated from an original sentence\\nOriginal sentence I’m so happy to see you.\\nGenerated sentences I’m so glad to see you.\\nI’m so happy to see y’all.\\nI’m very happy to see you.\\nThis type of data augmentation is a quick way to double or triple your training data.\\nPerturbation\\nPerturbation is also a label-preserving operation, but because sometimes it’s used to\\ntrick models into making wrong predictions, I thought it deserves its own section.\\nNeural networks, in general, are sensitive to noise. In the case of computer vision, this\\nmeans that adding a small amount of noise to an image can cause a neural network\\nto misclassify it. Su et al. showed that 67.97% of the natural images in the Kaggle\\nCIFAR-10 test dataset and 16.04% of the ImageNet test images can be misclassified by\\nchanging just one pixel (see Figure 4-12).49\\n114 | Chapter 4: Training Data', '50 Su et al., “One Pixel Attack. ”\\nFigure 4-12. Changing one pixel can cause a neural network to make wrong predictions.\\nThe three models used are AllConv, NiN, and VGG. The original labels made by those\\nmodels are above the labels made after one pixel was changed. Source: Su et al.50\\nData Augmentation | 115', '51 Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy, “Explaining and Harnessing Adversarial Exam‐\\nples, ” arXiv, March 20, 2015, https://oreil.ly/9v2No; Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza,\\nAaron Courville, and Y oshua Bengio, “Maxout Networks, arXiv, February 18, 2013, https://oreil.ly/L8mch.\\n52 Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard, “DeepFool: A Simple and Accurate\\nMethod to Fool Deep Neural Networks, ” in Proceedings of IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016, https://oreil.ly/dYVL8.\\n53 Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii, “Virtual Adversarial Training: A Regula‐\\nrization Method for Supervised and Semi-Supervised Learning, ” IEEE Transactions on Pattern Analysis and\\nMachine Intelligence, 2017, https://oreil.ly/MBQeu.\\n54 Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ”\\nUsing deceptive data to trick a neural network into making wrong predictions is\\ncalled adversarial attacks. Adding noise to samples is a common technique to create\\nadversarial samples. The success of adversarial attacks is especially exaggerated as the\\nresolution of images increases.\\nAdding noisy samples to training data can help models recognize the weak spots in\\ntheir learned decision boundary and improve their performance. 51 Noisy samples can\\nbe created by either adding random noise or by a search strategy. Moosavi-Dezfooli\\net al. proposed an algorithm, called DeepFool, that finds the minimum possible noise\\ninjection needed to cause a misclassification with high confidence. 52 This type of\\naugmentation is called adversarial augmentation.53\\nAdversarial augmentation is less common in NLP (an image of a bear with randomly\\nadded pixels still looks like a bear, but adding random characters to a random\\nsentence will likely render it gibberish), but perturbation has been used to make\\nmodels more robust. One of the most notable examples is BERT, where the model\\nchooses 15% of all tokens in each sequence at random and chooses to replace 10% of\\nthe chosen tokens with random words. For example, given the sentence “My dog is\\nhairy, ” and the model randomly replacing “hairy” with “apple, ” the sentence becomes\\n“My dog is apple. ” So 1.5% of all tokens might result in nonsensical meaning. Their\\nablation studies show that a small fraction of random replacement gives their model a\\nsmall performance boost.54', 'hairy, ” and the model randomly replacing “hairy” with “apple, ” the sentence becomes\\n“My dog is apple. ” So 1.5% of all tokens might result in nonsensical meaning. Their\\nablation studies show that a small fraction of random replacement gives their model a\\nsmall performance boost.54\\nIn Chapter 6, we’ll go over how to use perturbation not just as a way to improve your\\nmodel’s performance, but also as a way to evaluate its performance.\\nData Synthesis\\nSince collecting data is expensive and slow, with many potential privacy concerns, it’ d\\nbe a dream if we could sidestep it altogether and train our models with synthesized\\ndata. Even though we’re still far from being able to synthesize all training data, it’s\\npossible to synthesize some training data to boost a model’s performance.\\n116 | Chapter 4: Training Data', \"55 Hongyi Zhang, Moustapha Cisse, Y ann N. Dauphin, and David Lopez-Paz, “mixup: Beyond Empirical Risk\\nMinimization, ” ICLR 2018, https://oreil.ly/lIM5E.\\n56 Veit Sandfort, Ke Y an, Perry J. Pickhardt, and Ronald M. Summers, “Data Augmentation Using Generative\\nAdversarial Networks (CycleGAN) to Improve Generalizability in CT Segmentation Tasks, ” Scientific Reports\\n9, no. 1 (2019): 16884, https://oreil.ly/TDUwm.\\nIn NLP , templates can be a cheap way to bootstrap your model. One team I worked\\nwith used templates to bootstrap training data for their conversational AI (chatbot).\\nA template might look like: “Find me a [CUISINE] restaurant within [NUMBER]\\nmiles of [LOCATION]” (see Table 4-10). With lists of all possible cuisines, reasonable\\nnumbers (you would probably never want to search for restaurants beyond 1,000\\nmiles), and locations (home, office, landmarks, exact addresses) for each city, you can\\ngenerate thousands of training queries from a template.\\nTable 4-10. Three sentences generated from a template\\nTemplate Find me a [CUISINE] restaurant within [NUMBER] miles of [LOCATION].\\nGenerated queries Find me a Vietnamese restaurant within 2 miles of my office.\\nFind me a Thai restaurant within 5 miles of my home.\\nFind me a Mexican restaurant within 3 miles of Google headquarters.\\nIn computer vision, a straightforward way to synthesize new data is to combine exist‐\\ning examples with discrete labels to generate continuous labels. Consider a task of\\nclassifying images with two possible labels: DOG (encoded as 0) and CAT (encoded\\nas 1). From example x1 of label DOG and example x2 of label CAT, you can generate x'\\nsuch as:\\nx′ = γx1 + 1 − γ x2\\nThe label of x' is a combination of the labels of x1 and x2: γ× 0 + 1 − γ × 1. This\\nmethod is called mixup. The authors showed that mixup improves models’ generali‐\\nzation, reduces their memorization of corrupt labels, increases their robustness to\\nadversarial examples, and stabilizes the training of generative adversarial networks.55\\nUsing neural networks to synthesize training data is an exciting approach that is\\nactively being researched but not yet popular in production. Sandfort et al. showed\\nthat by adding images generated using CycleGAN to their original training data,\\nthey were able to improve their model’s performance significantly on computed\\ntomography (CT) segmentation tasks.56\\nIf you’re interested in learning more about data augmentation for computer vision, “ A\", 'that by adding images generated using CycleGAN to their original training data,\\nthey were able to improve their model’s performance significantly on computed\\ntomography (CT) segmentation tasks.56\\nIf you’re interested in learning more about data augmentation for computer vision, “ A\\nSurvey on Image Data Augmentation for Deep Learning”  (Shorten and Khoshgoftaar\\n2019) is a comprehensive review.\\nData Augmentation | 117', 'Summary\\nTraining data still forms the foundation of modern ML algorithms. No matter how\\nclever your algorithms might be, if your training data is bad, your algorithms won’t\\nbe able to perform well. It’s worth it to invest time and effort to curate and create\\ntraining data that will enable your algorithms to learn something meaningful.\\nIn this chapter, we’ve discussed the multiple steps to create training data. We first\\ncovered different sampling methods, both nonprobability sampling and random sam‐\\npling, that can help us sample the right data for our problem.\\nMost ML algorithms in use today are supervised ML algorithms, so obtaining labels is\\nan integral part of creating training data. Many tasks, such as delivery time estimation\\nor recommender systems, have natural labels. Natural labels are usually delayed, and\\nthe time it takes from when a prediction is served until when the feedback on it is\\nprovided is the feedback loop length. Tasks with natural labels are fairly common\\nin the industry, which might mean that companies prefer to start on tasks that have\\nnatural labels over tasks without natural labels.\\nFor tasks that don’t have natural labels, companies tend to rely on human annotators\\nto annotate their data. However, hand labeling comes with many drawbacks. For\\nexample, hand labels can be expensive and slow. To combat the lack of hand labels,\\nwe discussed alternatives including weak supervision, semi-supervision, transfer\\nlearning, and active learning.\\nML algorithms work well in situations when the data distribution is more balanced,\\nand not so well when the classes are heavily imbalanced. Unfortunately, problems\\nwith class imbalance are the norm in the real world. In the following section, we\\ndiscussed why class imbalance made it hard for ML algorithms to learn. We also\\ndiscussed different techniques to handle class imbalance, from choosing the right\\nmetrics to resampling data to modifying the loss function to encourage the model to\\npay attention to certain samples.\\nWe ended the chapter with a discussion on data augmentation techniques that can be\\nused to improve a model’s performance and generalization for both computer vision\\nand NLP tasks.\\nOnce you have your training data, you will want to extract features from it to train\\nyour ML models, which we will cover in the next chapter.\\n118 | Chapter 4: Training Data', 'CHAPTER 5\\nFeature Engineering\\nIn 2014, the paper “Practical Lessons from Predicting Clicks on Ads at Facebook”\\nclaimed that having the right features is the most important thing in developing their\\nML models. Since then, many of the companies that I’ve worked with have discovered\\ntime and time again that once they have a workable model, having the right features\\ntends to give them the biggest performance boost compared to clever algorithmic\\ntechniques such as hyperparameter tuning. State-of-the-art model architectures can\\nstill perform poorly if they don’t use a good set of features.\\nDue to its importance, a large part of many ML engineering and data science jobs\\nis to come up with new useful features. In this chapter, we will go over common\\ntechniques and important considerations with respect to feature engineering. We\\nwill dedicate a section to go into detail about a subtle yet disastrous problem that\\nhas derailed many ML systems in production: data leakage and how to detect and\\navoid it.\\nWe will end the chapter discussing how to engineer good features, taking into\\naccount both the feature importance and feature generalization. Talking about feature\\nengineering, some people might think of feature stores. Since feature stores are closer\\nto infrastructure to support multiple ML applications, we’ll cover feature stores in\\nChapter 10.\\n119', '1 Loris Nanni, Stefano Ghidoni, and Sheryl Brahnam, “Handcrafted vs. Non-handcrafted Features for Com‐\\nputer Vision Classification, ” Pattern Recognition 71 (November 2017): 158–72, https://oreil.ly/CGfYQ; Wikipe‐\\ndia, s.v. “Feature learning, ” https://oreil.ly/fJmwN.\\nLearned Features Versus Engineered Features\\nWhen I cover this topic in class, my students frequently ask: “Why do we have to\\nworry about feature engineering? Doesn’t deep learning promise us that we no longer\\nhave to engineer features?”\\nThey are right. The promise of deep learning is that we won’t have to handcraft\\nfeatures. For this reason, deep learning is sometimes called feature learning. 1 Many\\nfeatures can be automatically learned and extracted by algorithms. However, we’re\\nstill far from the point where all features can be automated. This is not to mention\\nthat, as of this writing, the majority of ML applications in production aren’t deep\\nlearning. Let’s go over an example to understand what features can be automatically\\nextracted and what features still need to be handcrafted.\\nImagine that you want to build a sentiment analysis classifier to classify whether a\\ncomment is spam or not. Before deep learning, when given a piece of text, you would\\nhave to manually apply classical text processing techniques such as lemmatization,\\nexpanding contractions, removing punctuation, and lowercasing everything. After\\nthat, you might want to split your text into n-grams with n values of your choice.\\nFor those unfamiliar, an n-gram is a contiguous sequence of n items from a given\\nsample of text. The items can be phonemes, syllables, letters, or words. For example,\\ngiven the post “I like food, ” its word-level 1-grams are [“I” , “like” , “food”] and its\\nword-level 2-grams are [“I like” , “like food”]. This sentence’s set of n-gram features, if\\nwe want n to be 1 and 2, is: [“I” , “like” , “food” , “I like” , “like food”].\\nFigure 5-1 shows an example of classical text processing techniques you can use to\\nhandcraft n-gram features for your text.\\n120 | Chapter 5: Feature Engineering', 'Figure 5-1. An example of techniques that you can use to handcraft n-gram features for\\nyour text\\nOnce you’ve generated n-grams for your training data, you can create a vocabulary\\nthat maps each n-gram to an index. Then you can convert each post into a vector\\nbased on its n-grams’ indices. For example, if we have a vocabulary of seven n-grams\\nas shown in Table 5-1, each post can be a vector of seven elements. Each element\\ncorresponds to the number of times the n-gram at that index appears in the post. “I\\nlike food” will be encoded as the vector [1, 1, 0, 1, 1, 0, 1]. This vector can then be\\nused as an input into an ML model.\\nTable 5-1. Example of a 1-gram and 2-gram vocabulary\\nI like good food I like good food like food\\n0 1 2 3 4 5 6\\nLearned Features Versus Engineered Features | 121', 'Feature engineering requires knowledge of domain-specific techniques—in this case,\\nthe domain is natural language processing (NLP) and the native language of the text.\\nIt tends to be an iterative process, which can be brittle. When I followed this method\\nfor one of my early NLP projects, I kept having to restart my process either because I\\nhad forgotten to apply one technique or because one technique I used turned out to\\nbe working poorly and I had to undo it.\\nHowever, much of this pain has been alleviated since the rise of deep learning. Instead\\nof having to worry about lemmatization, punctuation, or stopword removal, you can\\njust split your raw text into words (i.e., tokenization), create a vocabulary out of those\\nwords, and convert each of your words into one-shot vectors using this vocabulary.\\nY our model will hopefully learn to extract useful features from this. In this new\\nmethod, much of feature engineering for text has been automated. Similar progress\\nhas been made for images too. Instead of having to manually extract features from\\nraw images and input those features into your ML models, you can just input raw\\nimages directly into your deep learning models.\\nHowever, an ML system will likely need data beyond just text and images. For\\nexample, when detecting whether a comment is spam or not, on top of the text in the\\ncomment itself, you might want to use other information about:\\nThe comment\\nHow many upvotes/downvotes does it have?\\nThe user who posted this comment\\nWhen was this account created, how often do they post, and how many upvotes/\\ndownvotes do they have?\\nThe thread in which the comment was posted\\nHow many views does it have? Popular threads tend to attract more spam.\\nThere are many possible features to use in your model. Some of them are shown\\nin Figure 5-2. The process of choosing what information to use and how to extract\\nthis information into a format usable by your ML models is feature engineering.\\nFor complex tasks such as recommending videos for users to watch next on TikTok,\\nthe number of features used can go up to millions. For domain-specific tasks such\\nas predicting whether a transaction is fraudulent, you might need subject matter\\nexpertise with banking and frauds to be able to come up with useful features.\\n122 | Chapter 5: Feature Engineering', '2 In my experience, how well a person handles missing values for a given dataset during interviews strongly\\ncorrelates with how well they will do in their day-to-day jobs.\\nFigure 5-2. Some of the possible features about a comment, a thread, or a user to be\\nincluded in your model\\nCommon Feature Engineering Operations\\nBecause of the importance and the ubiquity of feature engineering in ML projects,\\nthere have been many techniques developed to streamline the process. In this section,\\nwe will discuss several of the most important operations that you might want to\\nconsider while engineering features from your data. They include handling missing\\nvalues, scaling, discretization, encoding categorical features, and generating the old-\\nschool but still very effective cross features as well as the newer and exciting posi‐\\ntional features. This list is nowhere near being comprehensive, but it does comprise\\nsome of the most common and useful operations to give you a good starting point.\\nLet’s dive in!\\nHandling Missing Values\\nOne of the first things you might notice when dealing with data in production\\nis that some values are missing. However, one thing that many ML engineers I’ve\\ninterviewed don’t know is that not all types of missing values are equal. 2 To illustrate\\nthis point, consider the task of predicting whether someone is going to buy a house in\\nthe next 12 months. A portion of the data we have is in Table 5-2.\\nCommon Feature Engineering Operations | 123', 'Table 5-2. Example data for predicting house buying in the next 12 months\\nID Age Gender Annual income Marital status Number of children Job Buy?\\n1 A 150,000 1 Engineer No\\n2 27 B 50,000 Teacher No\\n3 A 100,000 Married 2 Yes\\n4 40 B 2 Engineer Yes\\n5 35 B Single 0 Doctor Yes\\n6 A 50,000 0 Teacher No\\n7 33 B 60,000 Single Teacher No\\n8 20 B 10,000 Student No\\nThere are three types of missing values. The official names for these types are a little\\nbit confusing, so we’ll go into detailed examples to mitigate the confusion.\\nMissing not at random (MNAR)\\nThis is when the reason a value is missing is because of the true value itself.\\nIn this example, we might notice that some respondents didn’t disclose their\\nincome. Upon investigation it may turn out that the income of respondents who\\nfailed to report tends to be higher than that of those who did disclose. The income\\nvalues are missing for reasons related to the values themselves.\\nMissing at random (MAR)\\nThis is when the reason a value is missing is not due to the value itself, but due\\nto another observed variable . In this example, we might notice that age values\\nare often missing for respondents of the gender “ A, ” which might be because the\\npeople of gender A in this survey don’t like disclosing their age.\\nMissing completely at random (MCAR)\\nThis is when there’s no pattern in when the value is missing . In this example, we\\nmight think that the missing values for the column “Job” might be completely\\nrandom, not because of the job itself and not because of any other variable. Peo‐\\nple just forget to fill in that value sometimes for no particular reason. However,\\nthis type of missing is very rare. There are usually reasons why certain values are\\nmissing, and you should investigate.\\nWhen encountering missing values, you can either fill in the missing values with\\ncertain values (imputation) or remove the missing values (deletion). We’ll go over\\nboth.\\n124 | Chapter 5: Feature Engineering', '3 Rachel Bogardus Drew, “3 Facts About Marriage and Homeownership, ” Joint Center for Housing Studies of\\nHarvard University, December 17, 2014, https://oreil.ly/MWxFp.\\nDeletion\\nWhen I ask candidates about how to handle missing values during interviews, many\\ntend to prefer deletion, not because it’s a better method, but because it’s easier to do.\\nOne way to delete is column deletion: if a variable has too many missing values, just\\nremove that variable. For example, in the example above, over 50% of the values for\\nthe variable “Marital status” are missing, so you might be tempted to remove this\\nvariable from your model. The drawback of this approach is that you might remove\\nimportant information and reduce the accuracy of your model. Marital status might\\nbe highly correlated to buying houses, as married couples are much more likely to be\\nhomeowners than single people.3\\nAnother way to delete is row deletion: if a sample has missing value(s), just remove\\nthat sample. This method can work when the missing values are completely at\\nrandom (MCAR) and the number of examples with missing values is small, such as\\nless than 0.1%. Y ou don’t want to do row deletion if that means 10% of your data\\nsamples are removed.\\nHowever, removing rows of data can also remove important information that your\\nmodel needs to make predictions, especially if the missing values are not at random\\n(MNAR). For example, you don’t want to remove samples of gender B respondents\\nwith missing income because the fact that income is missing is information itself\\n(missing income might mean higher income, and thus, more correlated to buying a\\nhouse) and can be used to make predictions.\\nOn top of that, removing rows of data can create biases in your model, especially if\\nthe missing values are at random (MAR). For example, if you remove all examples\\nmissing age values in the data in Table 5-2, you will remove all respondents with\\ngender A from your data, and your model won’t be able to make good predictions for\\nrespondents with gender A.\\nImputation\\nEven though deletion is tempting because it’s easy to do, deleting data can lead to\\nlosing important information and introduce biases into your model. If you don’t want\\nto delete missing values, you will have to impute them, which means “fill them with\\ncertain values. ” Deciding which “certain values” to use is the hard part.\\nCommon Feature Engineering Operations | 125', '4 Feature scaling once boosted my model’s performance by almost 10%.\\nOne common practice is to fill in missing values with their defaults. For example, if\\nthe job is missing, you might fill it with an empty string “” . Another common practice\\nis to fill in missing values with the mean, median, or mode (the most common value).\\nFor example, if the temperature value is missing for a data sample whose month value\\nis July, it’s not a bad idea to fill it with the median temperature of July.\\nBoth practices work well in many cases, but sometimes they can cause hair-pulling\\nbugs. One time, in one of the projects I was helping with, we discovered that the\\nmodel was spitting out garbage because the app’s frontend no longer asked users to\\nenter their age, so age values were missing, and the model filled them with 0. But the\\nmodel never saw the age value of 0 during training, so it couldn’t make reasonable\\npredictions.\\nIn general, you want to avoid filling missing values with possible values, such as\\nfilling the missing number of children with 0—0 is a possible value for the number of\\nchildren. It makes it hard to distinguish between people whose information is missing\\nand people who don’t have children.\\nMultiple techniques might be used at the same time or in sequence to handle missing\\nvalues for a particular set of data. Regardless of what techniques you use, one thing\\nis certain: there is no perfect way to handle missing values. With deletion, you\\nrisk losing important information or accentuating biases. With imputation, you risk\\ninjecting your own bias into and adding noise to your data, or worse, data leakage. If\\nyou don’t know what data leakage is, don’t panic, we’ll cover it in the section “Data\\nLeakage” on page 135.\\nScaling\\nConsider the task of predicting whether someone will buy a house in the next 12\\nmonths, and the data shown in Table 5-2. The values of the variable Age in our data\\nrange from 20 to 40, whereas the values of the variable Annual Income range from\\n10,000 to 150,000. When we input these two variables into an ML model, it won’t\\nunderstand that 150,000 and 40 represent different things. It will just see them both\\nas numbers, and because the number 150,000 is much bigger than 40, it might give it\\nmore importance, regardless of which variable is actually more useful for generating\\npredictions.\\nBefore inputting features into models, it’s important to scale them to be similar', 'as numbers, and because the number 150,000 is much bigger than 40, it might give it\\nmore importance, regardless of which variable is actually more useful for generating\\npredictions.\\nBefore inputting features into models, it’s important to scale them to be similar\\nranges. This process is called feature scaling. This is one of the simplest things you can\\ndo that often results in a performance boost for your model. Neglecting to do so can\\ncause your model to make gibberish predictions, especially with classical algorithms\\nlike gradient-boosted trees and logistic regression.4\\n126 | Chapter 5: Feature Engineering', '5 Changyong Feng, Hongyue Wang, Naiji Lu, Tian Chen, Hua He, Ying Lu, and Xin M. Tu, “Log-\\nTransformation and Its Implications for Data Analysis, ” Shanghai Archives of Psychiatry 26, no. 2 (April\\n2014): 105–9, https://oreil.ly/hHJjt.\\nAn intuitive way to scale your features is to get them to be in the range [0, 1]. Given a\\nvariable x, its values can be rescaled to be in this range using the following formula:\\nx′ = x− min x\\nmax x − min x\\nY ou can validate that if x is the maximum value, the scaled value x′ will be 1. If x is the\\nminimum value, the scaled value x′ will be 0.\\nIf you want your feature to be in an arbitrary range [ a, b]—empirically, I find the\\nrange [–1, 1] to work better than the range [0, 1]—you can use the following formula:\\nx′ = a+ x− min x b− a\\nmax x − min x\\nScaling to an arbitrary range works well when you don’t want to make any assump‐\\ntions about your variables. If you think that your variables might follow a normal\\ndistribution, it might be helpful to normalize them so that they have zero mean and\\nunit variance. This process is called standardization:\\nx′ = x− x\\nσ ,\\nwith x being the mean of variable x, and σ being its standard deviation.\\nIn practice, ML models tend to struggle with features that follow a skewed distri‐\\nbution. To help mitigate the skewness, a technique commonly used is log trans‐\\nformation: apply the log function to your feature. An example of how the log\\ntransformation can make your data less skewed is shown in Figure 5-3 . While this\\ntechnique can yield performance gain in many cases, it doesn’t work for all cases, and\\nyou should be wary of the analysis performed on log-transformed data instead of the\\noriginal data.5\\nCommon Feature Engineering Operations | 127', 'Figure 5-3. In many cases, the log transformation can help reduce the skewness of\\nyour data\\nThere are two important things to note about scaling. One is that it’s a common\\nsource of data leakage (this will be covered in greater detail in the section “Data\\nLeakage” on page 135). Another is that it often requires global statistics—you have\\nto look at the entire or a subset of training data to calculate its min, max, or mean.\\nDuring inference, you reuse the statistics you had obtained during training to scale\\nnew data. If the new data has changed significantly compared to the training, these\\nstatistics won’t be very useful. Therefore, it’s important to retrain your model often to\\naccount for these changes.\\nDiscretization\\nThis technique is included in this book for completeness, though in practice, I’ve\\nrarely found discretization to help. Imagine that we’ve built a model with the data\\nin Table 5-2 . During training, our model has seen the annual income values of\\n“150,000, ” “50,000, ” “100,000, ” and so on. During inference, our model encounters an\\nexample with an annual income of “9,000.50. ”\\nIntuitively, we know that $9,000.50 a year isn’t much different from $10,000/year, and\\nwe want our model to treat both of them the same way. But the model doesn’t know\\nthat. Our model only knows that 9,000.50 is different from 10,000, and it will treat\\nthem differently.\\nDiscretization is the process of turning a continuous feature into a discrete feature.\\nThis process is also known as quantization or binning. This is done by creating\\nbuckets for the given values. For annual income, you might want to group them into\\nthree buckets as follows:\\n128 | Chapter 5: Feature Engineering', '• Lower income: less than $35,000/year•\\n• Middle income: between $35,000 and $100,000/year•\\n• Upper income: more than $100,000/year•\\nInstead of having to learn an infinite number of possible incomes, our model can\\nfocus on learning only three categories, which is a much easier task to learn. This\\ntechnique is supposed to be more helpful with limited training data.\\nEven though, by definition, discretization is meant for continuous features, it can be\\nused for discrete features too. The age variable is discrete, but it might still be useful\\nto group the values into buckets such as follows:\\n• Less than 18•\\n• Between 18 and 22•\\n• Between 22 and 30•\\n• Between 30 and 40•\\n• Between 40 and 65•\\n• Over 65•\\nThe downside is that this categorization introduces discontinuities at the category\\nboundaries—$34,999 is now treated as completely different from $35,000, which is\\ntreated the same as $100,000. Choosing the boundaries of categories might not be all\\nthat easy. Y ou can try to plot the histograms of the values and choose the boundaries\\nthat make sense. In general, common sense, basic quantiles, and sometimes subject\\nmatter expertise can help.\\nEncoding Categorical Features\\nWe’ve talked about how to turn continuous features into categorical features. In this\\nsection, we’ll discuss how to best handle categorical features.\\nPeople who haven’t worked with data in production tend to assume that categories\\nare static, which means the categories don’t change over time. This is true for many\\ncategories. For example, age brackets and income brackets are unlikely to change,\\nand you know exactly how many categories there are in advance. Handling these\\ncategories is straightforward. Y ou can just give each category a number and you’re\\ndone.\\nHowever, in production, categories change. Imagine you’re building a recommender\\nsystem to predict what products users might want to buy from Amazon. One of the\\nfeatures you want to use is the product brand. When looking at Amazon’s historical\\nCommon Feature Engineering Operations | 129', '6 “Two Million Brands on Amazon, ” Marketplace Pulse, June 11, 2019, https://oreil.ly/zrqtd.\\n7 Wikipedia, s.v. “Feature hashing, ” https://oreil.ly/tINTc.\\ndata, you realize that there are a lot of brands. Even back in 2019, there were already\\nover two million brands on Amazon!6\\nThe number of brands is overwhelming, but you think: “I can still handle this. ” Y ou\\nencode each brand as a number, so now you have two million numbers, from 0 to\\n1,999,999, corresponding to two million brands. Y our model does spectacularly on\\nthe historical test set, and you get approval to test it on 1% of today’s traffic.\\nIn production, your model crashes because it encounters a brand it hasn’t seen before\\nand therefore can’t encode. New brands join Amazon all the time. To address this,\\nyou create a category UNKNOWN with the value of 2,000,000 to catch all the brands\\nyour model hasn’t seen during training.\\nY our model doesn’t crash anymore, but your sellers complain that their new\\nbrands are not getting any traffic. It’s because your model didn’t see the category\\nUNKNOWN in the train set, so it just doesn’t recommend any product of the\\nUNKNOWN brand. Y ou fix this by encoding only the top 99% most popular brands\\nand encode the bottom 1% brand as UNKNOWN. This way, at least your model\\nknows how to deal with UNKNOWN brands.\\nY our model seems to work fine for about one hour, then the click-through rate on\\nproduct recommendations plummets. Over the last hour, 20 new brands joined your\\nsite; some of them are new luxury brands, some of them are sketchy knockoff brands,\\nsome of them are established brands. However, your model treats them all the same\\nway it treats unpopular brands in the training data.\\nThis isn’t an extreme example that only happens if you work at Amazon. This\\nproblem happens quite a lot. For example, if you want to predict whether a comment\\nis spam, you might want to use the account that posted this comment as a feature,\\nand new accounts are being created all the time. The same goes for new product\\ntypes, new website domains, new restaurants, new companies, new IP addresses, and\\nso on. If you work with any of them, you’ll have to deal with this problem.\\nFinding a way to solve this problem turns out to be surprisingly difficult. Y ou don’t\\nwant to put them into a set of buckets because it can be really hard—how would you\\neven go about putting new user accounts into different groups?\\nOne solution to this problem is the hashing trick, popularized by the package Vowpal', 'want to put them into a set of buckets because it can be really hard—how would you\\neven go about putting new user accounts into different groups?\\nOne solution to this problem is the hashing trick, popularized by the package Vowpal\\nWabbit developed at Microsoft.7 The gist of this trick is that you use a hash function\\nto generate a hashed value of each category. The hashed value will become the index\\nof that category. Because you can specify the hash space, you can fix the number\\nof encoded values for a feature in advance, without having to know how many\\n130 | Chapter 5: Feature Engineering', '8 Lucas Bernardi, “Don’t Be Tricked by the Hashing Trick, ” Booking.com, January 10, 2018,\\nhttps://oreil.ly/VZmaY.\\ncategories there will be. For example, if you choose a hash space of 18 bits, which\\ncorresponds to 218 = 262,144 possible hashed values, all the categories, even the ones\\nthat your model has never seen before, will be encoded by an index between 0 and\\n262,143.\\nOne problem with hashed functions is collision: two categories being assigned the\\nsame index. However, with many hash functions, the collisions are random; new\\nbrands can share an index with any of the existing brands instead of always sharing\\nan index with unpopular brands, which is what happens when we use the preceding\\nUNKNOWN category. The impact of colliding hashed features is, fortunately, not\\nthat bad. In research done by Booking.com, even for 50% colliding features, the\\nperformance loss is less than 0.5%, as shown in Figure 5-4.8\\nFigure 5-4. A 50% collision rate only causes the log loss to increase less than 0.5%.\\nSource: Lucas Bernardi\\nY ou can choose a hash space large enough to reduce the collision. Y ou can also\\nchoose a hash function with properties that you want, such as a locality-sensitive\\nhashing function where similar categories (such as websites with similar names) are\\nhashed into values close to each other.\\nCommon Feature Engineering Operations | 131', '9 Huifeng Guo, Ruiming Tang, Yunming Y e, Zhenguo Li, and Xiuqiang He, “DeepFM: A Factorization-\\nMachine Based Neural Network for CTR Prediction, ” Proceedings of the Twenty-Sixth International Joint\\nConference on Artificial Intelligence (IJCAI, 2017), https://oreil.ly/1Vs3v; Jianxun Lian, Xiaohuan Zhou, Fuz‐\\nheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun, “xDeepFM: Combining Explicit and Implicit\\nFeature Interactions for Recommender Systems, ” arXiv, 2018, https://oreil.ly/WFmFt.\\nBecause it’s a trick, it’s often considered hacky by academics and excluded from ML\\ncurricula. But its wide adoption in the industry is a testimonial to how effective the\\ntrick is. It’s essential to Vowpal Wabbit and it’s part of the frameworks of scikit-learn,\\nTensorFlow, and gensim. It can be especially useful in continual learning settings\\nwhere your model learns from incoming examples in production. We’ll cover contin‐\\nual learning in Chapter 9.\\nFeature Crossing\\nFeature crossing is the technique to combine two or more features to generate\\nnew features. This technique is useful to model the nonlinear relationships between\\nfeatures. For example, for the task of predicting whether someone will want to buy a\\nhouse in the next 12 months, you suspect that there might be a nonlinear relationship\\nbetween marital status and number of children, so you combine them to create a new\\nfeature “marriage and children” as in Table 5-3.\\nTable 5-3. Example of how two features can be combined to create a new feature\\nMarriage Single Married Single Single Married\\nChildren 0 2 1 0 1\\nMarriage and children Single, 0 Married, 2 Single, 1 Single, 0 Married, 1\\nBecause feature crossing helps model nonlinear relationships between variables, it’s\\nessential for models that can’t learn or are bad at learning nonlinear relationships,\\nsuch as linear regression, logistic regression, and tree-based models. It’s less impor‐\\ntant in neural networks, but it can still be useful because explicit feature crossing\\noccasionally helps neural networks learn nonlinear relationships faster. DeepFM and\\nxDeepFM are the family of models that have successfully leveraged explicit feature\\ninteractions for recommender systems and click-through-rate prediction.9\\nA caveat of feature crossing is that it can make your feature space blow up. Imagine\\nfeature A has 100 possible values and feature B has 100 possible features; crossing\\nthese two features will result in a feature with 100 × 100 = 10,000 possible values. Y ou', 'A caveat of feature crossing is that it can make your feature space blow up. Imagine\\nfeature A has 100 possible values and feature B has 100 possible features; crossing\\nthese two features will result in a feature with 100 × 100 = 10,000 possible values. Y ou\\nwill need a lot more data for models to learn all these possible values. Another caveat\\nis that because feature crossing increases the number of features models use, it can\\nmake models overfit to the training data.\\n132 | Chapter 5: Feature Engineering', '10 Flavian Vasile, Elena Smirnova, and Alexis Conneau, “Meta-Prod2Vec—Product Embeddings Using Side-\\nInformation for Recommendation, ” arXiv, July 25, 2016, https://oreil.ly/KDaEd; “Product Embeddings and\\nVectors, ” Coveo, https://oreil.ly/ShaSY.\\n11 Andrew Zhai, “Representation Learning for Recommender Systems, ” August 15, 2021, https://oreil.ly/OchiL.\\nDiscrete and Continuous Positional Embeddings\\nFirst introduced to the deep learning community in the paper “ Attention Is All\\nY ou Need” (Vaswani et al. 2017), positional embedding has become a standard data\\nengineering technique for many applications in both computer vision and NLP . We’ll\\nwalk through an example to show why positional embedding is necessary and how to\\ndo it.\\nConsider the task of language modeling where you want to predict the next token\\n(e.g., a word, character, or subword) based on the previous sequence of tokens. In\\npractice, a sequence length can be up to 512, if not larger. However, for simplicity,\\nlet’s use words as our tokens and use the sequence length of 8. Given an arbitrary\\nsequence of 8 words, such as “Sometimes all I really want to do is, ” we want to predict\\nthe next word.\\nEmbeddings\\nAn embedding is a vector that represents a piece of data. We call the set of all possible\\nembeddings generated by the same algorithm for a type of data “an embedding space. ”\\nAll embedding vectors in the same space are of the same size.\\nOne of the most common uses of embeddings is word embeddings, where you can\\nrepresent each word with a vector. However, embeddings for other types of data are\\nincreasingly popular. For example, ecommerce solutions like Criteo and Coveo have\\nembeddings for products.10 Pinterest has embeddings for images, graphs, queries, and\\neven users.11 Given that there are so many types of data with embeddings, there has\\nbeen a lot of interest in creating universal embeddings for multimodal data.\\nIf we use a recurrent neural network, it will process words in sequential order, which\\nmeans the order of words is implicitly inputted. However, if we use a model like a\\ntransformer, words are processed in parallel, so words’ positions need to be explicitly\\ninputted so that our model knows the order of these words (“a dog bites a child”\\nis very different from “a child bites a dog”). We don’t want to input the absolute\\npositions, 0, 1, 2, …, 7, into our model because empirically, neural networks don’t\\nwork well with inputs that aren’t unit-variance (that’s why we scale our features, as', 'is very different from “a child bites a dog”). We don’t want to input the absolute\\npositions, 0, 1, 2, …, 7, into our model because empirically, neural networks don’t\\nwork well with inputs that aren’t unit-variance (that’s why we scale our features, as\\ndiscussed previously in the section “Scaling” on page 126).\\nCommon Feature Engineering Operations | 133', 'If we rescale the positions to between 0 and 1, so 0, 1, 2, …, 7 become 0, 0.143, 0.286,\\n…, 1, the differences between the two positions will be too small for neural networks\\nto learn to differentiate.\\nA way to handle position embeddings is to treat it the way we’ d treat word embed‐\\nding. With word embedding, we use an embedding matrix with the vocabulary size as\\nits number of columns, and each column is the embedding for the word at the index\\nof that column. With position embedding, the number of columns is the number of\\npositions. In our case, since we only work with the previous sequence size of 8, the\\npositions go from 0 to 7 (see Figure 5-5).\\nThe embedding size for positions is usually the same as the embedding size for words\\nso that they can be summed. For example, the embedding for the word “food” at\\nposition 0 is the sum of the embedding vector for the word “food” and the embed‐\\nding vector for position 0. This is the way position embeddings are implemented\\nin Hugging Face’s BERT as of August 2021. Because the embeddings change as the\\nmodel weights get updated, we say that the position embeddings are learned.\\nFigure 5-5. One way to embed positions is to treat them the way you’ d treat word\\nembeddings\\nPosition embeddings can also be fixed. The embedding for each position is still\\na vector with S elements ( S is the position embedding size), but each element is\\npredefined using a function, usually sine and cosine. In the original Transformer\\npaper, if the element is at an even index, use sine. Else, use cosine. See Figure 5-6.\\n134 | Chapter 5: Feature Engineering', 'Figure 5-6. Example of fixed position embedding. H is the dimension of the outputs\\nproduced by the model.\\nFixed positional embedding is a special case of what is known as Fourier features.\\nIf positions in positional embeddings are discrete, Fourier features can also be contin‐\\nuous. Consider the task involving representations of 3D objects, such as a teapot.\\nEach position on the surface of the teapot is represented by a three-dimensional\\ncoordinate, which is continuous. When positions are continuous, it’ d be very hard\\nto build an embedding matrix with continuous column indices, but fixed position\\nembeddings using sine and cosine functions still work.\\nThe following is the generalized format for the embedding vector at coordinate v,\\nalso called the Fourier features of coordinate v. Fourier features have been shown\\nto improve models’ performance for tasks that take in coordinates (or positions) as\\ninputs. If interested, you might want to read more about it in “Fourier Features Let\\nNetworks Learn High Frequency Functions in Low Dimensional Domains” (Tancik et\\nal. 2020).\\nγ v = a1 cos 2πb1\\nTv , a1 sin 2πb1\\nTv , ...,am cos 2πbm\\nTv , am sin 2πbm\\nTv\\nT\\nData Leakage\\nIn July 2021, MIT Technology Review ran a provocative article titled “Hundreds of AI\\nTools Have Been Built to Catch Covid. None of Them Helped. ” These models were\\ntrained to predict COVID-19 risks from medical scans. The article listed multiple\\nexamples where ML models that performed well during evaluation failed to be usable\\nin actual production settings.\\nIn one example, researchers trained their model on a mix of scans taken when\\npatients were lying down and standing up. “Because patients scanned while lying\\ndown were more likely to be seriously ill, the model learned to predict serious covid\\nrisk from a person’s position. ”\\nData Leakage | 135', '12 Will Douglas Heaven, “Hundreds of AI Tools Have Been Built to Catch Covid. None of Them Helped, ” MIT\\nTechnology Review, July 30, 2021, https://oreil.ly/Ig1b1.\\n13 Zidmie, “The leak explained!” Kaggle, https://oreil.ly/1JgLj.\\n14 Addison Howard, “Competition Recap—Congratulations to our Winners!” Kaggle, https://oreil.ly/wVUU4.\\nIn some other cases, models were “found to be picking up on the text font that certain\\nhospitals used to label the scans. As a result, fonts from hospitals with more serious\\ncaseloads became predictors of covid risk. ”12\\nBoth of these are examples of data leakage. Data leakage refers to the phenomenon\\nwhen a form of the label “leaks” into the set of features used for making predictions,\\nand this same information is not available during inference.\\nData leakage is challenging because often the leakage is nonobvious. It’s dangerous\\nbecause it can cause your models to fail in an unexpected and spectacular way, even\\nafter extensive evaluation and testing. Let’s go over another example to demonstrate\\nwhat data leakage is.\\nSuppose you want to build an ML model to predict whether a CT scan of a lung\\nshows signs of cancer. Y ou obtained the data from hospital A, removed the doctors’\\ndiagnosis from the data, and trained your model. It did really well on the test data\\nfrom hospital A, but poorly on the data from hospital B.\\nAfter extensive investigation, you learned that at hospital A, when doctors think that\\na patient has lung cancer, they send that patient to a more advanced scan machine,\\nwhich outputs slightly different CT scan images. Y our model learned to rely on\\nthe information on the scan machine used to make predictions on whether a scan\\nimage shows signs of lung cancer. Hospital B sends the patients to different CT scan\\nmachines at random, so your model has no information to rely on. We say that labels\\nare leaked into the features during training.\\nData leakage can happen not only with newcomers to the field, but has also happened\\nto several experienced researchers whose work I admire, and in one of my own\\nprojects. Despite its prevalence, data leakage is rarely covered in ML curricula.\\nCautionary Tale: Data Leakage with Kaggle Competition\\nIn 2020, the University of Liverpool launched an Ion Switching competition on\\nKaggle. The task was to identify the number of ion channels open at each time\\npoint. They synthesized test data from training data, and some people were able to', 'Cautionary Tale: Data Leakage with Kaggle Competition\\nIn 2020, the University of Liverpool launched an Ion Switching competition on\\nKaggle. The task was to identify the number of ion channels open at each time\\npoint. They synthesized test data from training data, and some people were able to\\nreverse engineer and obtain test labels from the leak. 13 The two winning teams in this\\ncompetition are the two teams that were able to exploit the leak, though they might\\nhave still been able to win without exploiting the leak.14\\n136 | Chapter 5: Feature Engineering', 'Common Causes for Data Leakage\\nIn this section, we’ll go over some common causes for data leakage and how to avoid\\nthem.\\nSplitting time-correlated data randomly instead of by time\\nWhen I learned ML in college, I was taught to randomly split my data into train,\\nvalidation, and test splits. This is also how data is often reportedly split in ML\\nresearch papers. However, this is also one common cause for data leakage.\\nIn many cases, data is time-correlated, which means that the time the data is gener‐\\nated affects its label distribution. Sometimes, the correlation is obvious, as in the case\\nof stock prices. To oversimplify it, the prices of similar stocks tend to move together.\\nIf 90% of the tech stocks go down today, it’s very likely the other 10% of the tech\\nstocks go down too. When building models to predict the future stock prices, you\\nwant to split your training data by time, such as training your model on data from\\nthe first six days and evaluating it on data from the seventh day. If you randomly split\\nyour data, prices from the seventh day will be included in your train split and leak\\ninto your model the condition of the market on that day. We say that the information\\nfrom the future is leaked into the training process.\\nHowever, in many cases, the correlation is nonobvious. Consider the task of predict‐\\ning whether someone will click on a song recommendation. Whether someone will\\nlisten to a song depends not only on their music taste but also on the general music\\ntrend that day. If an artist passes away one day, people will be much more likely\\nto listen to that artist. By including samples from a certain day in the train split,\\ninformation about the music trend that day will be passed into your model, making it\\neasier for it to make predictions on other samples on that same day.\\nTo prevent future information from leaking into the training process and allowing\\nmodels to cheat during evaluation, split your data by time, instead of splitting ran‐\\ndomly, whenever possible. For example, if you have data from five weeks, use the first\\nfour weeks for the train split, then randomly split week 5 into validation and test\\nsplits as shown in Figure 5-7.\\nData Leakage | 137', 'Figure 5-7. Split data by time to prevent future information from leaking into the\\ntraining process\\nScaling before splitting\\nAs discussed in the section “Scaling” on page 126, it’s important to scale your features.\\nScaling requires global statistics—e.g., mean, variance—of your data. One common\\nmistake is to use the entire training data to generate global statistics before splitting\\nit into different splits, leaking the mean and variance of the test samples into the\\ntraining process, allowing a model to adjust its predictions for the test samples.\\nThis information isn’t available in production, so the model’s performance will likely\\ndegrade.\\nTo avoid this type of leakage, always split your data first before scaling, then use the\\nstatistics from the train split to scale all the splits. Some even suggest that we split\\nour data before any exploratory data analysis and data processing, so that we don’t\\naccidentally gain information about the test split.\\nFilling in missing data with statistics from the test split\\nOne common way to handle the missing values of a feature is to fill (input) them with\\nthe mean or median of all values present. Leakage might occur if the mean or median\\nis calculated using entire data instead of just the train split. This type of leakage is\\nsimilar to the type of leakage caused by scaling, and it can be prevented by using only\\nstatistics from the train split to fill in missing values in all the splits.\\n138 | Chapter 5: Feature Engineering', '15 Björn Barz and Joachim Denzler, “Do We Train on Test Data? Purging CIFAR of Near-Duplicates, ” Journal of\\nImaging 6, no. 6 (2020): 41.\\n16 Michael Roberts, Derek Driggs, Matthew Thorpe, Julian Gilbey, Michael Y eung, Stephan Ursprung, Angelica\\nI. Aviles-Rivero, et al. “Common Pitfalls and Recommendations for Using Machine Learning to Detect and\\nPrognosticate for COVID-19 Using Chest Radiographs and CT Scans, ” Nature Machine Intelligence 3 (2021):\\n199–217, https://oreil.ly/TzbKJ.\\nPoor handling of data duplication before splitting\\nIf you have duplicates or near-duplicates in your data, failing to remove them before\\nsplitting your data might cause the same samples to appear in both train and valida‐\\ntion/test splits. Data duplication is quite common in the industry, and has also been\\nfound in popular research datasets. For example, CIFAR-10 and CIFAR-100 are two\\npopular datasets used for computer vision research. They were released in 2009, yet it\\nwas not until 2019 that Barz and Denzler discovered that 3.3% and 10% of the images\\nfrom the test sets of the CIFAR-10 and CIFAR-100 datasets have duplicates in the\\ntraining set.15\\nData duplication can result from data collection or merging of different data sour‐\\nces. A 2021 Nature article listed data duplication as a common pitfall when using\\nML to detect COVID-19, which happened because “one dataset combined several\\nother datasets without realizing that one of the component datasets already contains\\nanother component. ”16 Data duplication can also happen because of data processing—\\nfor example, oversampling might result in duplicating certain examples.\\nTo avoid this, always check for duplicates before splitting and also after splitting just\\nto make sure. If you oversample your data, do it after splitting.\\nGroup leakage\\nA group of examples have strongly correlated labels but are divided into different\\nsplits. For example, a patient might have two lung CT scans that are a week apart,\\nwhich likely have the same labels on whether they contain signs of lung cancer, but\\none of them is in the train split and the second is in the test split. This type of leakage\\nis common for objective detection tasks that contain photos of the same object taken\\nmilliseconds apart—some of them landed in the train split while others landed in the\\ntest split. It’s hard avoiding this type of data leakage without understanding how your\\ndata was generated.\\nData Leakage | 139', 'Leakage from data generation process\\nThe example earlier about how information on whether a CT scan shows signs of\\nlung cancer is leaked via the scan machine is an example of this type of leakage.\\nDetecting this type of data leakage requires a deep understanding of the way data\\nis collected. For example, it would be very hard to figure out that the model’s poor\\nperformance in hospital B is due to its different scan machine procedure if you don’t\\nknow about different scan machines or that the procedures at the two hospitals are\\ndifferent.\\nThere’s no foolproof way to avoid this type of leakage, but you can mitigate the risk\\nby keeping track of the sources of your data and understanding how it is collected\\nand processed. Normalize your data so that data from different sources can have\\nthe same means and variances. If different CT scan machines output images with\\ndifferent resolutions, normalizing all the images to have the same resolution would\\nmake it harder for models to know which image is from which scan machine. And\\ndon’t forget to incorporate subject matter experts, who might have more contexts on\\nhow data is collected and used, into the ML design process!\\nDetecting Data Leakage\\nData leakage can happen during many steps, from generating, collecting, sampling,\\nsplitting, and processing data to feature engineering. It’s important to monitor for\\ndata leakage during the entire lifecycle of an ML project.\\nMeasure the predictive power of each feature or a set of features with respect to the\\ntarget variable (label). If a feature has unusually high correlation, investigate how this\\nfeature is generated and whether the correlation makes sense. It’s possible that two\\nfeatures independently don’t contain leakage, but two features together can contain\\nleakage. For example, when building a model to predict how long an employee will\\nstay at a company, the starting date and the end date separately doesn’t tell us much\\nabout their tenure, but both together can give us that information.\\n140 | Chapter 5: Feature Engineering', 'Do ablation studies to measure how important a feature or a set of features is to your\\nmodel. If removing a feature causes the model’s performance to deteriorate signifi‐\\ncantly, investigate why that feature is so important. If you have a massive amount of\\nfeatures, say a thousand features, it might be infeasible to do ablation studies on every\\npossible combination of them, but it can still be useful to occasionally do ablation\\nstudies with a subset of features that you suspect the most. This is another example\\nof how subject matter expertise can come in handy in feature engineering. Ablation\\nstudies can be run offline at your own schedule, so you can leverage your machines\\nduring downtime for this purpose.\\nKeep an eye out for new features added to your model. If adding a new feature\\nsignificantly improves your model’s performance, either that feature is really good or\\nthat feature just contains leaked information about labels.\\nBe very careful every time you look at the test split. If you use the test split in any\\nway other than to report a model’s final performance, whether to come up with ideas\\nfor new features or to tune hyperparameters, you risk leaking information from the\\nfuture into your training process.\\nEngineering Good Features\\nGenerally, adding more features leads to better model performance. In my experi‐\\nence, the list of features used for a model in production only grows over time.\\nHowever, more features doesn’t always mean better model performance. Having too\\nmany features can be bad both during training and serving your model for the\\nfollowing reasons:\\n• The more features you have, the more opportunities there are for data leakage.•\\n• Too many features can cause overfitting.•\\n• Too many features can increase memory required to serve a model, which, in•\\nturn, might require you to use a more expensive machine/instance to serve your\\nmodel.\\n• Too many features can increase inference latency when doing online prediction,•\\nespecially if you need to extract these features from raw data for predictions\\nonline. We’ll go deeper into online prediction in Chapter 7.\\n• Useless features become technical debts. Whenever your data pipeline changes,•\\nall the affected features need to be adjusted accordingly. For example, if one day\\nyour application decides to no longer take in information about users’ age, all\\nfeatures that use users’ age need to be updated.\\nEngineering Good Features | 141', '17 With XGBoost function get_score.\\n18 A great open source Python package for calculating SHAP can be found on GitHub.\\nIn theory, if a feature doesn’t help a model make good predictions, regularization\\ntechniques like L1 regularization should reduce that feature’s weight to 0. However,\\nin practice, it might help models learn faster if the features that are no longer useful\\n(and even possibly harmful) are removed, prioritizing good features.\\nY ou can store removed features to add them back later. Y ou can also just store general\\nfeature definitions to reuse and share across teams in an organization. When talking\\nabout feature definition management, some people might think of feature stores as\\nthe solution. However, not all feature stores manage feature definitions. We’ll discuss\\nfeature stores further in Chapter 10.\\nThere are two factors you might want to consider when evaluating whether a feature\\nis good for a model: importance to the model and generalization to unseen data.\\nFeature Importance\\nThere are many different methods for measuring a feature’s importance. If you use\\na classical ML algorithm like boosted gradient trees, the easiest way to measure\\nthe importance of your features is to use built-in feature importance functions imple‐\\nmented by XGBoost. 17 For more model-agnostic methods, you might want to look\\ninto SHAP (SHapley Additive exPlanations). 18 InterpretML is a great open source\\npackage that leverages feature importance to help you understand how your model\\nmakes predictions.\\nThe exact algorithm for feature importance measurement is complex, but intuitively,\\na feature’s importance to a model is measured by how much that model’s performance\\ndeteriorates if that feature or a set of features containing that feature is removed from\\nthe model. SHAP is great because it not only measures a feature’s importance to an\\nentire model, it also measures each feature’s contribution to a model’s specific predic‐\\ntion. Figures 5-8 and 5-9 show how SHAP can help you understand the contribution\\nof each feature to a model’s predictions.\\n142 | Chapter 5: Feature Engineering', '19 Scott Lundberg, SHAP (SHapley Additive exPlanations), GitHub repository, last accessed 2021,\\nhttps://oreil.ly/c8qqE.\\nFigure 5-8. How much each feature contributes to a model’s single prediction, measured\\nby SHAP . The value LSTAT = 4.98 contributes the most to this specific prediction.\\nSource: Scott Lundberg19\\nFigure 5-9. How much each feature contributes to a model, measured by SHAP . The\\nfeature LSTAT has the highest importance. Source: Scott Lundberg\\nEngineering Good Features | 143', '20 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Y anxin Shi, et al., “Practical Lessons from\\nPredicting Clicks on Ads at Facebook, ” in ADKDD ’14: Proceedings of the Eighth International Workshop on\\nData Mining for Online Advertising (August 2014): 1–9, https://oreil.ly/dHXeC.\\nOften, a small number of features accounts for a large portion of your model’s feature\\nimportance. When measuring feature importance for a click-through rate prediction\\nmodel, the ads team at Facebook found out that the top 10 features are responsible\\nfor about half of the model’s total feature importance, whereas the last 300 features\\ncontribute less than 1% feature importance, as shown in Figure 5-10.20\\nFigure 5-10. Boosting feature importance. X-axis corresponds to the number of features.\\nFeature importance is in log scale. Source: He et al.\\nNot only good for choosing the right features, feature importance techniques are also\\ngreat for interpretability as they help you understand how your models work under\\nthe hood.\\nFeature Generalization\\nSince the goal of an ML model is to make correct predictions on unseen data, features\\nused for the model should generalize to unseen data. Not all features generalize\\nequally. For example, for the task of predicting whether a comment is spam, the\\nidentifier of each comment is not generalizable at all and shouldn’t be used as a\\nfeature for the model. However, the identifier of the user who posts the comment,\\nsuch as username, might still be useful for a model to make predictions.\\n144 | Chapter 5: Feature Engineering', 'Measuring feature generalization is a lot less scientific than measuring feature impor‐\\ntance, and it requires both intuition and subject matter expertise on top of statistical\\nknowledge. Overall, there are two aspects you might want to consider with regards to\\ngeneralization: feature coverage and distribution of feature values.\\nCoverage is the percentage of the samples that has values for this feature in the data—\\nso the fewer values that are missing, the higher the coverage. A rough rule of thumb\\nis that if this feature appears in a very small percentage of your data, it’s not going to\\nbe very generalizable. For example, if you want to build a model to predict whether\\nsomeone will buy a house in the next 12 months and you think that the number of\\nchildren someone has will be a good feature, but you can only get this information for\\n1% of your data, this feature might not be very useful.\\nThis rule of thumb is rough because some features can still be useful even if they\\nare missing in most of your data. This is especially true when the missing values are\\nnot at random, which means having the feature or not might be a strong indication\\nof its value. For example, if a feature appears only in 1% of your data, but 99% of\\nthe examples with this feature have POSITIVE labels, this feature is useful and you\\nshould use it.\\nCoverage of a feature can differ wildly between different slices of data and even in the\\nsame slice of data over time. If the coverage of a feature differs a lot between the train\\nand test split (such as it appears in 90% of the examples in the train split but only\\nin 20% of the examples in the test split), this is an indication that your train and test\\nsplits don’t come from the same distribution. Y ou might want to investigate whether\\nthe way you split your data makes sense and whether this feature is a cause for data\\nleakage.\\nFor the feature values that are present, you might want to look into their distribution.\\nIf the set of values that appears in the seen data (such as the train split) has no overlap\\nwith the set of values that appears in the unseen data (such as the test split), this\\nfeature might even hurt your model’s performance.\\nAs a concrete example, imagine you want to build a model to estimate the time it\\nwill take for a given taxi ride. Y ou retrain this model every week, and you want to\\nuse the data from the last six days to predict the ETAs (estimated time of arrival)', 'feature might even hurt your model’s performance.\\nAs a concrete example, imagine you want to build a model to estimate the time it\\nwill take for a given taxi ride. Y ou retrain this model every week, and you want to\\nuse the data from the last six days to predict the ETAs (estimated time of arrival)\\nfor today. One of the features is DAY_OF_THE_WEEK, which you think is useful\\nbecause the traffic on weekdays is usually worse than on the weekend. This feature\\ncoverage is 100%, because it’s present in every feature. However, in the train split,\\nthe values for this feature are Monday to Saturday, whereas in the test split, the value\\nfor this feature is Sunday. If you include this feature in your model without a clever\\nscheme to encode the days, it won’t generalize to the test split, and might harm your\\nmodel’s performance.\\nEngineering Good Features | 145', 'On the other hand, HOUR_OF_THE_DAY is a great feature, because the time in the\\nday affects the traffic too, and the range of values for this feature in the train split\\noverlaps with the test split 100%.\\nWhen considering a feature’s generalization, there’s a trade-off between generaliza‐\\ntion and specificity. Y ou might realize that the traffic during an hour only changes\\ndepending on whether that hour is the rush hour. So you generate the feature\\nIS_RUSH_HOUR and set it to 1 if the hour is between 7 a.m. and 9 a.m. or between\\n4 p.m. and 6 p.m. IS_RUSH_HOUR is more generalizable but less specific than\\nHOUR_OF_THE_DAY . Using IS_RUSH_HOUR without HOUR_OF_THE_DAY\\nmight cause models to lose important information about the hour.\\nSummary\\nBecause the success of today’s ML systems still depends on their features, it’s impor‐\\ntant for organizations interested in using ML in production to invest time and effort\\ninto feature engineering.\\nHow to engineer good features is a complex question with no foolproof answers. The\\nbest way to learn is through experience: trying out different features and observing\\nhow they affect your models’ performance. It’s also possible to learn from experts. I\\nfind it extremely useful to read about how the winning teams of Kaggle competitions\\nengineer their features to learn more about their techniques and the considerations\\nthey went through.\\nFeature engineering often involves subject matter expertise, and subject matter\\nexperts might not always be engineers, so it’s important to design your workflow\\nin a way that allows nonengineers to contribute to the process.\\nHere is a summary of best practices for feature engineering:\\n• Split data by time into train/valid/test splits instead of doing it randomly.•\\n• If you oversample your data, do it after splitting.•\\n• Scale and normalize your data after splitting to avoid data leakage.•\\n• Use statistics from only the train split, instead of the entire data, to scale your•\\nfeatures and handle missing values.\\n146 | Chapter 5: Feature Engineering', '• Understand how your data is generated, collected, and processed. Involve•\\ndomain experts if possible.\\n• Keep track of your data’s lineage.•\\n• Understand feature importance to your model.•\\n• Use features that generalize well.•\\n• Remove no longer useful features from your models.•\\nWith a set of good features, we’ll move to the next part of the workflow: training ML\\nmodels. Before we move on, I just want to reiterate that moving to modeling doesn’t\\nmean we’re done with handling data or feature engineering. We are never done with\\ndata and features. In most real-world ML projects, the process of collecting data and\\nfeature engineering goes on as long as your models are in production. We need to use\\nnew, incoming data to continually improve models, which we’ll cover in Chapter 9.\\nSummary | 147', 'CHAPTER 6\\nModel Development and Offline Evaluation\\nIn Chapter 4 , we discussed how to create training data for your model, and in\\nChapter 5, we discussed how to engineer features from that training data. With the\\ninitial set of features, we’ll move to the ML algorithm part of ML systems. For me,\\nthis has always been the most fun step, as it allows me to play around with different\\nalgorithms and techniques, even the latest ones. This is also the first step where I can\\nsee all the hard work I’ve put into data and feature engineering transformed into a\\nsystem whose outputs (predictions) I can use to evaluate the success of my effort.\\nTo build an ML model, we first need to select the ML model to build. There are so\\nmany ML algorithms out there, with more actively being developed. This chapter\\nstarts with six tips for selecting the best algorithms for your task.\\nThe section that follows discusses different aspects of model development, such as\\ndebugging, experiment tracking and versioning, distributed training, and AutoML.\\nModel development is an iterative process. After each iteration, you’ll want to com‐\\npare your model’s performance against its performance in previous iterations and\\nevaluate how suitable this iteration is for production. The last section of this chapter\\nis dedicated to how to evaluate your model before deploying it to production, cover‐\\ning a range of evaluation techniques including perturbation tests, invariance tests,\\nmodel calibration, and slide-based evaluation.\\nI expect that most readers already have an understanding of common ML algorithms\\nsuch as linear models, decision trees, k-nearest neighbors, and different types of\\nneural networks. This chapter will discuss techniques surrounding these algorithms\\nbut won’t go into details of how they work. Because this chapter deals with ML\\nalgorithms, it requires a lot more ML knowledge than other chapters. If you’re not\\nfamiliar with them, I recommend taking an online course or reading a book on ML\\nalgorithms before reading this chapter. Readers wanting a quick refresh on basic ML\\n149', 'concepts might find helpful the section “Basic ML Reviews” in the book’s GitHub\\nrepository.\\nModel Development and Training\\nIn this section, we’ll discuss necessary aspects to help you develop and train your\\nmodel, including how to evaluate different ML models for your problem, creating\\nensembles of models, experiment tracking and versioning, and distributed training,\\nwhich is necessary for the scale at which models today are usually trained at. We’ll\\nend this section with the more advanced topic of AutoML—using ML to automati‐\\ncally choose a model best for your problem.\\nEvaluating ML Models\\nThere are many possible solutions to any given problem. Given a task that can\\nleverage ML in its solution, you might wonder what ML algorithm you should use\\nfor it. For example, should you start with logistic regression, an algorithm that you’re\\nalready familiar with? Or should you try out a new fancy model that is supposed to\\nbe the new state of the art for your problem? A more senior colleague mentioned that\\ngradient-boosted trees have always worked for her for this task in the past—should\\nyou listen to her advice?\\nIf you had unlimited time and compute power, the rational thing to do would be to\\ntry all possible solutions and see what is best for you. However, time and compute\\npower are limited resources, and you have to be strategic about what models you\\nselect.\\nWhen talking about ML algorithms, many people think in terms of classical ML\\nalgorithms versus neural networks. There are a lot of interests and media coverage for\\nneural networks, especially deep learning, which is understandable given that most\\nof the AI progress in the last decade happened due to neural networks getting bigger\\nand deeper.\\nThese interests and coverage might give off the impression that deep learning is\\nreplacing classical ML algorithms. However, even though deep learning is finding\\nmore use cases in production, classical ML algorithms are not going away. Many\\nrecommender systems still rely on collaborative filtering and matrix factorization.\\nTree-based algorithms, including gradient-boosted trees, still power many classifica‐\\ntion tasks with strict latency requirements.\\nEven in applications where neural networks are deployed, classic ML algorithms are\\nstill being used in tandem. For example, neural networks and decision trees might be\\nused together in an ensemble. A k-means clustering model might be used to extract', 'tion tasks with strict latency requirements.\\nEven in applications where neural networks are deployed, classic ML algorithms are\\nstill being used in tandem. For example, neural networks and decision trees might be\\nused together in an ensemble. A k-means clustering model might be used to extract\\nfeatures to input into a neural network. Vice versa, a pretrained neural network\\n150 | Chapter 6: Model Development and Offline Evaluation', '(like BERT or GPT-3) might be used to generate embeddings to input into a logistic\\nregression model.\\nWhen selecting a model for your problem, you don’t choose from every possible\\nmodel out there, but usually focus on a set of models suitable for your problem. For\\nexample, if your boss tells you to build a system to detect toxic tweets, you know\\nthat this is a text classification problem—given a piece of text, classify whether it’s\\ntoxic or not—and common models for text classification include naive Bayes, logistic\\nregression, recurrent neural networks, and transformer-based models such as BERT,\\nGPT, and their variants.\\nIf your client wants you to build a system to detect fraudulent transactions, you\\nknow that this is the classic abnormality detection problem—fraudulent transactions\\nare abnormalities that you want to detect—and common algorithms for this prob‐\\nlem are many, including k-nearest neighbors, isolation forest, clustering, and neural\\nnetworks.\\nKnowledge of common ML tasks and the typical approaches to solve them is essential\\nin this process.\\nDifferent types of algorithms require different numbers of labels as well as different\\namounts of compute power. Some take longer to train than others, whereas some\\ntake longer to make predictions. Non-neural network algorithms tend to be more\\nexplainable (e.g., what features contributed the most to an email being classified as\\nspam) than neural networks.\\nWhen considering what model to use, it’s important to consider not only the model’s\\nperformance, measured by metrics such as accuracy, F1 score, and log loss, but\\nalso its other properties, such as how much data, compute, and time it needs to\\ntrain, what’s its inference latency, and interpretability. For example, a simple logistic\\nregression model might have lower accuracy than a complex neural network, but it\\nrequires less labeled data to start, it’s much faster to train, it’s much easier to deploy,\\nand it’s also much easier to explain why it’s making certain predictions.\\nComparing ML algorithms is out of the scope of this book. No matter how good\\na comparison is, it will be outdated as soon as new algorithms come out. Back in\\n2016, LSTM-RNNs were all the rage and the backbone of the architecture seq2seq\\n(Sequence-to-Sequence) that powered many NLP tasks from machine translation to\\ntext summarization to text classification. However, just two years later, recurrent\\narchitectures were largely replaced by transformer architectures for NLP tasks.', '(Sequence-to-Sequence) that powered many NLP tasks from machine translation to\\ntext summarization to text classification. However, just two years later, recurrent\\narchitectures were largely replaced by transformer architectures for NLP tasks.\\nTo understand different algorithms, the best way is to equip yourself with basic\\nML knowledge and run experiments with the algorithms you’re interested in. To\\nkeep up to date with so many new ML techniques and models, I find it helpful to\\nmonitor trends at major ML conferences such as NeurIPS, ICLR, and ICML, as well\\nas following researchers whose work has a high signal-to-noise ratio on Twitter.\\nModel Development and Training | 151', 'Six tips for model selection\\nWithout getting into specifics of different algorithms, here are six tips that might help\\nyou decide what ML algorithms to work on next.\\nAvoid the state-of-the-art trap.    While helping companies as well as recent graduates\\nget started in ML, I usually have to spend a nontrivial amount of time steering\\nthem away from jumping straight into state-of-the-art models. I can see why people\\nwant state-of-the-art models. Many believe that these models would be the best\\nsolutions for their problems—why try an old solution if you believe that a newer\\nand superior solution exists? Many business leaders also want to use state-of-the-art\\nmodels because they want to make their businesses appear cutting edge. Developers\\nmight also be more excited getting their hands on new models than getting stuck into\\nthe same old things over and over again.\\nResearchers often only evaluate models in academic settings, which means that a\\nmodel being state of the art often means that it performs better than existing models\\non some static datasets . It doesn’t mean that this model will be fast enough or cheap\\nenough for you to implement. It doesn’t even mean that this model will perform\\nbetter than other models on your data.\\nWhile it’s essential to stay up to date with new technologies and beneficial to evaluate\\nthem for your business, the most important thing to do when solving a problem\\nis finding solutions that can solve that problem. If there’s a solution that can solve\\nyour problem that is much cheaper and simpler than state-of-the-art models, use the\\nsimpler solution.\\nStart with the simplest models.    Zen of Python states that “simple is better than com‐\\nplex, ” and this principle is applicable to ML as well. Simplicity serves three purposes.\\nFirst, simpler models are easier to deploy, and deploying your model early allows\\nyou to validate that your prediction pipeline is consistent with your training pipeline.\\nSecond, starting with something simple and adding more complex components step-\\nby-step makes it easier to understand your model and debug it. Third, the simplest\\nmodel serves as a baseline to which you can compare your more complex models.\\nSimplest models are not always the same as models with the least effort. For example,\\npretrained BERT models are complex, but they require little effort to get started\\nwith, especially if you use a ready-made implementation like the one in Hugging', 'Simplest models are not always the same as models with the least effort. For example,\\npretrained BERT models are complex, but they require little effort to get started\\nwith, especially if you use a ready-made implementation like the one in Hugging\\nFace’s Transformer. In this case, it’s not a bad idea to use the complex solution,\\ngiven that the community around this solution is well developed enough to help you\\nget through any problems you might encounter. However, you might still want to\\nexperiment with simpler solutions to ensure that pretrained BERT is indeed better\\nthan those simpler solutions for your problem. Pretrained BERT might be low effort\\nto start with, but it can be quite high effort to improve upon. Whereas if you start\\nwith a simpler model, there’ll be a lot of room for you to improve upon your model.\\n152 | Chapter 6: Model Development and Offline Evaluation', '1 Andrew Ng has a great lecture where he explains that if a learning algorithm suffers from high bias, getting\\nmore training data by itself won’t help much. Whereas if a learning algorithm suffers from high variance,\\ngetting more training data is likely to help.\\nAvoid human biases in selecting models.    Imagine an engineer on your team is assigned\\nthe task of evaluating which model is better for your problem: a gradient-boosted tree\\nor a pretrained BERT model. After two weeks, this engineer announces that the best\\nBERT model outperforms the best gradient-boosted tree by 5%. Y our team decides to\\ngo with the pretrained BERT model.\\nA few months later, however, a seasoned engineer joins your team. She decides\\nto look into gradient-boosted trees again and finds out that this time, the best\\ngradient-boosted tree outperforms the pretrained BERT model you currently have in\\nproduction. What happened?\\nThere are a lot of human biases in evaluating models. Part of the process of evaluating\\nan ML architecture is to experiment with different features and different sets of\\nhyperparameters to find the best model of that architecture. If an engineer is more\\nexcited about an architecture, they will likely spend a lot more time experimenting\\nwith it, which might result in better-performing models for that architecture.\\nWhen comparing different architectures, it’s important to compare them under com‐\\nparable setups. If you run 100 experiments for an architecture, it’s not fair to only run\\na couple of experiments for the architecture you’re evaluating it against. Y ou might\\nneed to run 100 experiments for the other architecture too.\\nBecause the performance of a model architecture depends heavily on the context\\nit’s evaluated in—e.g., the task, the training data, the test data, the hyperparameters,\\netc.—it’s extremely difficult to make claims that a model architecture is better than\\nanother architecture. The claim might be true in a context, but unlikely true for all\\npossible contexts.\\nEvaluate good performance now versus good performance later.    The best model now does\\nnot always mean the best model two months from now. For example, a tree-based\\nmodel might work better now because you don’t have a ton of data yet, but two\\nmonths from now, you might be able to double your amount of training data, and\\nyour neural network might perform much better.1\\nA simple way to estimate how your model’s performance might change with more', 'model might work better now because you don’t have a ton of data yet, but two\\nmonths from now, you might be able to double your amount of training data, and\\nyour neural network might perform much better.1\\nA simple way to estimate how your model’s performance might change with more\\ndata is to use learning curves . A learning curve of a model is a plot of its perfor‐\\nmance—e.g., training loss, training accuracy, validation accuracy—against the num‐\\nber of training samples it uses, as shown in Figure 6-1 . The learning curve won’t\\nhelp you estimate exactly how much performance gain you can get from having more\\nModel Development and Training | 153', 'training data, but it can give you a sense of whether you can expect any performance\\ngain at all from more training data.\\nFigure 6-1. The learning curves of a naive Bayes model and an SVM model. Source:\\nscikit-learn\\nA situation that I’ve encountered is when a team evaluates a simple neural network\\nagainst a collaborative filtering model for making recommendations. When evaluat‐\\ning both models offline, the collaborative filtering model outperformed. However,\\nthe simple neural network can update itself with each incoming example, whereas\\nthe collaborative filtering has to look at all the data to update its underlying matrix.\\nThe team decided to deploy both the collaborative filtering model and the simple\\nneural network. They used the collaborative filtering model to make predictions for\\nusers, and continually trained the simple neural network in production with new,\\nincoming data. After two weeks, the simple neural network was able to outperform\\nthe collaborative filtering model.\\nWhile evaluating models, you might want to take into account their potential for\\nimprovements in the near future, and how easy/difficult it is to achieve those\\nimprovements.\\nEvaluate trade-offs.    There are many trade-offs you have to make when selecting mod‐\\nels. Understanding what’s more important in the performance of your ML system will\\nhelp you choose the most suitable model.\\n154 | Chapter 6: Model Development and Offline Evaluation', 'One classic example of trade-off is the false positives and false negatives trade-off.\\nReducing the number of false positives might increase the number of false negatives,\\nand vice versa. In a task where false positives are more dangerous than false negatives,\\nsuch as fingerprint unlocking (unauthorized people shouldn’t be classified as author‐\\nized and given access), you might prefer a model that makes fewer false positives.\\nSimilarly, in a task where false negatives are more dangerous than false positives,\\nsuch as COVID-19 screening (patients with COVID-19 shouldn’t be classified as no\\nCOVID-19), you might prefer a model that makes fewer false negatives.\\nAnother example of trade-off is compute requirement and accuracy—a more com‐\\nplex model might deliver higher accuracy but might require a more powerful\\nmachine, such as a GPU instead of a CPU, to generate predictions with acceptable\\ninference latency. Many people also care about the interpretability and performance\\ntrade-off. A more complex model can give a better performance, but its results are\\nless interpretable.\\nUnderstand your model’s assumptions.    The statistician George Box said in 1976 that “all\\nmodels are wrong, but some are useful. ” The real world is intractably complex, and\\nmodels can only approximate using assumptions. Every single model comes with its\\nown assumptions. Understanding what assumptions a model makes and whether our\\ndata satisfies those assumptions can help you evaluate which model works best for\\nyour use case.\\nFollowing are some of the common assumptions. It’s not meant to be an exhaustive\\nlist, but just a demonstration:\\nPrediction assumption\\nEvery model that aims to predict an output Y from an input X makes the\\nassumption that it’s possible to predict Y based on X.\\nIID\\nNeural networks assume that the examples are independent and identically dis‐\\ntributed, which means that all the examples are independently drawn from the\\nsame joint distribution.\\nSmoothness\\nEvery supervised machine learning method assumes that there’s a set of functions\\nthat can transform inputs into outputs such that similar inputs are transformed\\ninto similar outputs. If an input X produces an output Y, then an input close to X\\nwould produce an output proportionally close to Y.\\nModel Development and Training | 155', '2 I went through the winning solutions listed on Farid Rashidi’s “Kaggle Solutions” web page. One solu‐\\ntion used 33 models (Giba, “1st Place-Winner Solution-Gilberto Titericz and Stanislav Semenov, ” Kaggle,\\nhttps://oreil.ly/z5od8).\\nTractability\\nLet X be the input and Z be the latent representation of X. Every generative\\nmodel makes the assumption that it’s tractable to compute the probability P(Z|X).\\nBoundaries\\nA linear classifier assumes that decision boundaries are linear.\\nConditional independence\\nA naive Bayes classifier assumes that the attribute values are independent of each\\nother given the class.\\nNormally distributed\\nMany statistical methods assume that data is normally distributed.\\nEnsembles\\nWhen considering an ML solution to your problem, you might want to start with\\na system that contains just one model (the process of selecting one model for your\\nproblem was discussed earlier in the chapter). After developing one single model,\\nyou might think about how to continue improving its performance. One method that\\nhas consistently given a performance boost is to use an ensemble of multiple models\\ninstead of just an individual model to make predictions. Each model in the ensemble\\nis called a base learner. For example, for the task of predicting whether an email is\\nSPAM or NOT SPAM, you might have three different models. The final prediction\\nfor each email is the majority vote of all three models. So if at least two base learners\\noutput SPAM, the email will be classified as SPAM.\\nTwenty out of 22 winning solutions on Kaggle competitions in 2021, as of August\\n2021, use ensembles. 2 As of January 2022, 20 top solutions on SQuAD 2.0 , the\\nStanford Question Answering Dataset, are ensembles, as shown in Figure 6-2.\\nEnsembling methods are less favored in production because ensembles are more\\ncomplex to deploy and harder to maintain. However, they are still common for tasks\\nwhere a small performance boost can lead to a huge financial gain, such as predicting\\nclick-through rate for ads.\\n156 | Chapter 6: Model Development and Offline Evaluation', 'Figure 6-2. As of January 2022, the top 20 solutions on SQuAD 2.0 are all ensembles\\nWe’ll go over an example to give you the intuition of why ensembling works. Imagine\\nyou have three email spam classifiers, each with an accuracy of 70%. Assuming\\nthat each classifier has an equal probability of making a correct prediction for each\\nemail, and that these three classifiers are not correlated, we’ll show that by taking the\\nmajority vote of these three classifiers, we can get an accuracy of 78.4%.\\nFor each email, each classifier has a 70% chance of being correct. The ensemble will\\nbe correct if at least two classifiers are correct. Table 6-1 shows the probabilities of\\ndifferent possible outcomes of the ensemble given an email. This ensemble will have\\nan accuracy of 0.343 + 0.441 = 0.784, or 78.4%.\\nModel Development and Training | 157', '3 Mikel Galar, Alberto Fernandez, Edurne Barrenechea, Humberto Bustince, and Francisco Herrera, “ A Review\\non Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches, ” IEEE\\nTransactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42, no. 4 (July 2012): 463–\\n84, https://oreil.ly/ZBlgE; G. Rekha, Amit Kumar Tyagi, and V . Krishna Reddy, “Solving Class Imbalance Prob‐\\nlem Using Bagging, Boosting Techniques, With and Without Using Noise Filtering Method, ” International\\nJournal of Hybrid Intelligent Systems 15, no. 2 (January 2019): 67–76, https://oreil.ly/hchzU.\\n4 Training stability here means less fluctuation in the training loss.\\nTable 6-1. Possible outcomes of the ensemble that takes the majority vote\\nfrom three classifiers\\nOutputs of three models Probability Ensemble’s output\\nAll three are correct 0.7 * 0.7 * 0.7 = 0.343 Correct\\nOnly two are correct (0.7 * 0.7 * 0.3) * 3 = 0.441 Correct\\nOnly one is correct (0.3 * 0.3 * 0.7) * 3 = 0.189 Wrong\\nNone are correct 0.3 * 0.3 * 0.3 = 0.027 Wrong\\nThis calculation only holds if the classifiers in an ensemble are uncorrelated. If all\\nclassifiers are perfectly correlated—all three of them make the same prediction for\\nevery email—the ensemble will have the same accuracy as each individual classifier.\\nWhen creating an ensemble, the less correlation there is among base learners, the\\nbetter the ensemble will be. Therefore, it’s common to choose very different types of\\nmodels for an ensemble. For example, you might create an ensemble that consists of\\none transformer model, one recurrent neural network, and one gradient-boosted tree.\\nThere are three ways to create an ensemble: bagging, boosting, and stacking. In\\naddition to helping boost performance, according to several survey papers, ensemble\\nmethods such as boosting and bagging, together with resampling, have shown to help\\nwith imbalanced datasets. 3 We’ll go over each of these three methods, starting with\\nbagging.\\nBagging\\nBagging, shortened from bootstrap aggregating , is designed to improve both the\\ntraining stability and accuracy of ML algorithms. 4 It reduces variance and helps to\\navoid overfitting.\\nGiven a dataset, instead of training one classifier on the entire dataset, you sample\\nwith replacement to create different datasets, called bootstraps, and train a classifica‐\\ntion or regression model on each of these bootstraps. Sampling with replacement', 'avoid overfitting.\\nGiven a dataset, instead of training one classifier on the entire dataset, you sample\\nwith replacement to create different datasets, called bootstraps, and train a classifica‐\\ntion or regression model on each of these bootstraps. Sampling with replacement\\nensures that each bootstrap is created independently from its peers. Figure 6-3 shows\\nan illustration of bagging.\\n158 | Chapter 6: Model Development and Offline Evaluation', '5 Leo Breiman, “Bagging Predictors, ” Machine Learning 24 (1996): 123–40, https://oreil.ly/adzJu.\\nFigure 6-3. Bagging illustration. Source: Adapted from an image by Sirakorn\\nIf the problem is classification, the final prediction is decided by the majority vote of\\nall models. For example, if 10 classifiers vote SPAM and 6 models vote NOT SPAM,\\nthe final prediction is SPAM.\\nIf the problem is regression, the final prediction is the average of all models’\\npredictions.\\nBagging generally improves unstable methods, such as neural networks, classification\\nand regression trees, and subset selection in linear regression. However, it can mildly\\ndegrade the performance of stable methods such as k-nearest neighbors.5\\nA random forest is an example of bagging. A random forest is a collection of decision\\ntrees constructed by both bagging and feature randomness, where each tree can pick\\nonly from a random subset of features to use.\\nBoosting\\nBoosting is a family of iterative ensemble algorithms that convert weak learners to\\nstrong ones. Each learner in this ensemble is trained on the same set of samples, but\\nthe samples are weighted differently among iterations. As a result, future weak learn‐\\ners focus more on the examples that previous weak learners misclassified. Figure 6-4\\nshows an illustration of boosting, which involves the steps that follow.\\nModel Development and Training | 159', 'Figure 6-4. Boosting illustration. Source: Adapted from an image by Sirakorn\\n1. Y ou start by training the first weak classifier on the original dataset.1.\\n2. Samples are reweighted based on how well the first classifier classifies them, e.g.,2.\\nmisclassified samples are given higher weight.\\n3. Train the second classifier on this reweighted dataset. Y our ensemble now con‐3.\\nsists of the first and the second classifiers.\\n4. Samples are weighted based on how well the ensemble classifies them.4.\\n5. Train the third classifier on this reweighted dataset. Add the third classifier to the5.\\nensemble.\\n6. Repeat for as many iterations as needed.6.\\n7. Form the final strong classifier as a weighted combination of the existing classifi‐7.\\ners—classifiers with smaller training errors have higher weights.\\nAn example of a boosting algorithm is a gradient boosting machine (GBM), which\\nproduces a prediction model typically from weak decision trees. It builds the model\\nin a stage-wise fashion like other boosting methods do, and it generalizes them by\\nallowing optimization of an arbitrary differentiable loss function.\\n160 | Chapter 6: Model Development and Offline Evaluation', '6 “Machine Learning Challenge Winning Solutions, ” https://oreil.ly/YjS8d.\\n7 Tianqi Chen and Tong He, “Higgs Boson Discovery with Boosted Trees, ” Proceedings of Machine Learning\\nResearch 42 (2015): 69–80, https://oreil.ly/ysBYO.\\nXGBoost, a variant of GBM, used to be the algorithm of choice for many winning\\nteams of ML competitions.6 It’s been used in a wide range of tasks from classification,\\nranking, to the discovery of the Higgs Boson. 7 However, many teams have been\\nopting for LightGBM, a distributed gradient boosting framework that allows parallel\\nlearning, which generally allows faster training on large datasets.\\nStacking\\nStacking means that you train base learners from the training data then create a meta-\\nlearner that combines the outputs of the base learners to output final predictions, as\\nshown in Figure 6-5. The meta-learner can be as simple as a heuristic: you take the\\nmajority vote (for classification tasks) or the average vote (for regression tasks) from\\nall base learners. It can be another model, such as a logistic regression model or a\\nlinear regression model.\\nFigure 6-5. A visualization of a stacked ensemble from three base learners\\nFor more great advice on how to create an ensemble, refer to the awesome ensemble\\nguide by one of Kaggle’s legendary teams, MLWave.\\nModel Development and Training | 161', 'Experiment Tracking and Versioning\\nDuring the model development process, you often have to experiment with many\\narchitectures and many different models to choose the best one for your problem.\\nSome models might seem similar to each other and differ in only one hyperparame‐\\nter—such as one model using a learning rate of 0.003 and another model using\\na learning rate of 0.002—and yet their performances are dramatically different. It’s\\nimportant to keep track of all the definitions needed to re-create an experiment and\\nits relevant artifacts. An artifact is a file generated during an experiment—examples\\nof artifacts can be files that show the loss curve, evaluation loss graph, logs, or\\nintermediate results of a model throughout a training process. This enables you\\nto compare different experiments and choose the one best suited for your needs.\\nComparing different experiments can also help you understand how small changes\\naffect your model’s performance, which, in turn, gives you more visibility into how\\nyour model works.\\nThe process of tracking the progress and results of an experiment is called experi‐\\nment tracking. The process of logging all the details of an experiment for the purpose\\nof possibly recreating it later or comparing it with other experiments is called ver‐\\nsioning. These two go hand in hand with each other. Many tools originally set out to\\nbe experiment tracking tools, such as MLflow and Weights & Biases, have grown to\\nincorporate versioning. Many tools originally set out to be versioning tools, such as\\nDVC, have also incorporated experiment tracking.\\nExperiment tracking\\nA large part of training an ML model is babysitting the learning processes. Many\\nproblems can arise during the training process, including loss not decreasing, over‐\\nfitting, underfitting, fluctuating weight values, dead neurons, and running out of\\nmemory. It’s important to track what’s going on during training not only to detect\\nand address these issues but also to evaluate whether your model is learning anything\\nuseful.\\nWhen I just started getting into ML, all I was told to track was loss and speed. Fast-\\nforward several years, and people are tracking so many things that their experiment\\ntracking boards look both beautiful and terrifying at the same time. Following is just\\na short list of things you might want to consider tracking for each experiment during\\nits training process:\\n• The loss curve corresponding to the train split and each of the eval splits.•', 'tracking boards look both beautiful and terrifying at the same time. Following is just\\na short list of things you might want to consider tracking for each experiment during\\nits training process:\\n• The loss curve corresponding to the train split and each of the eval splits.•\\n• The model performance metrics that you care about on all nontest splits, such as•\\naccuracy, F1, perplexity.\\n• The log of corresponding sample, prediction, and ground truth label. This comes in•\\nhandy for ad hoc analytics and sanity check.\\n162 | Chapter 6: Model Development and Offline Evaluation', '8 We’ll cover observability in Chapter 8.\\n9 I’m still waiting for an experiment tracking tool that integrates with Git commits and DVC commits.\\n• The speed of your model, evaluated by the number of steps per second or, if your•\\ndata is text, the number of tokens processed per second.\\n• System performance metrics  such as memory usage and CPU/GPU utilization.•\\nThey’re important to identify bottlenecks and avoid wasting system resources.\\n• The values over time of any parameter and hyperparameter whose changes can•\\naffect your model’s performance, such as the learning rate if you use a learning\\nrate schedule; gradient norms (both globally and per layer), especially if you’re\\nclipping your gradient norms; and weight norm, especially if you’re doing weight\\ndecay.\\nIn theory, it’s not a bad idea to track everything you can. Most of the time, you\\nprobably don’t need to look at most of them. But when something does happen, one\\nor more of them might give you clues to understand and/or debug your model. In\\ngeneral, tracking gives you observability into the state of your model. 8 However, in\\npractice, due to the limitations of tooling today, it can be overwhelming to track too\\nmany things, and tracking less important things can distract you from tracking what\\nis really important.\\nExperiment tracking enables comparison across experiments. By observing how a\\ncertain change in a component affects the model’s performance, you gain some\\nunderstanding into what that component does.\\nA simple way to track your experiments is to automatically make copies of all the\\ncode files needed for an experiment and log all outputs with their timestamps. 9 Using\\nthird-party experiment tracking tools, however, can give you nice dashboards and\\nallow you to share your experiments with your coworkers.\\nVersioning\\nImagine this scenario. Y ou and your team spent the last few weeks tweaking your\\nmodel, and one of the runs finally showed promising results. Y ou wanted to use it\\nfor more extensive tests, so you tried to replicate it using the set of hyperparameters\\nyou’ d noted down somewhere, only to find out that the results weren’t quite the same.\\nY ou remembered that you’ d made some changes to the code between that run and the\\nnext, so you tried your best to undo the changes from memory because your reckless\\npast self had decided that the change was too minimal to be committed. But you still\\ncouldn’t replicate the promising result because there are just too many possible ways\\nto make changes.', 'next, so you tried your best to undo the changes from memory because your reckless\\npast self had decided that the change was too minimal to be committed. But you still\\ncouldn’t replicate the promising result because there are just too many possible ways\\nto make changes.\\nModel Development and Training | 163', 'This problem could have been avoided if you versioned your ML experiments. ML\\nsystems are part code, part data, so you need to not only version your code but your\\ndata as well. Code versioning has more or less become a standard in the industry.\\nHowever, at this point, data versioning is like flossing. Everyone agrees it’s a good\\nthing to do, but few do it.\\nThere are a few reasons why data versioning is challenging. One reason is that\\nbecause data is often much larger than code, we can’t use the same strategy that\\npeople usually use to version code to version data.\\nFor example, code versioning is done by keeping track of all the changes made to a\\ncodebase. A change is known as a diff, short for difference. Each change is measured\\nby line-by-line comparison. A line of code is usually short enough for line-by-line\\ncomparison to make sense. However, a line of your data, especially if it’s stored in a\\nbinary format, can be indefinitely long. Saying that this line of 1,000,000 characters is\\ndifferent from the other line of 1,000,000 characters isn’t going to be that helpful.\\nCode versioning tools allow users to revert to a previous version of the codebase by\\nkeeping copies of all the old files. However, a dataset used might be so large that\\nduplicating it multiple times might be unfeasible.\\nCode versioning tools allow for multiple people to work on the same codebase at the\\nsame time by duplicating the codebase on each person’s local machine. However, a\\ndataset might not fit into a local machine.\\nSecond, there’s still confusion in what exactly constitutes a diff when we version data.\\nWould diffs mean changes in the content of any file in your data repository, only\\nwhen a file is removed or added, or when the checksum of the whole repository has\\nchanged?\\nAs of 2021, data versioning tools like DVC only register a diff if the checksum of the\\ntotal directory has changed and if a file is removed or added.\\nAnother confusion is in how to resolve merge conflicts: if developer 1 uses data\\nversion X to train model A and developer 2 uses data version Y to train model B, it\\ndoesn’t make sense to merge data versions X and Y to create Z, since there’s no model\\ncorresponding with Z.\\nThird, if you use user data to train your model, regulations like General Data Protec‐\\ntion Regulation (GDPR) might make versioning this data complicated. For example,\\nregulations might mandate that you delete user data if requested, making it legally', 'corresponding with Z.\\nThird, if you use user data to train your model, regulations like General Data Protec‐\\ntion Regulation (GDPR) might make versioning this data complicated. For example,\\nregulations might mandate that you delete user data if requested, making it legally\\nimpossible to recover older versions of your data.\\nAggressive experiment tracking and versioning helps with reproducibility, but\\nit doesn’t ensure reproducibility. The frameworks and hardware you use might\\n164 | Chapter 6: Model Development and Offline Evaluation', '10 Notable examples include atomic operations in CUDA where nondeterministic orders of operations lead to\\ndifferent floating point rounding errors between runs.\\nintroduce nondeterminism to your experiment results, 10 making it impossible to rep‐\\nlicate the result of an experiment without knowing everything about the environment\\nyour experiment runs in.\\nThe way we have to run so many experiments right now to find the best possible\\nmodel is the result of us treating ML as a black box. Because we can’t predict which\\nconfiguration will work best, we have to experiment with multiple configurations.\\nHowever, I hope that as the field progresses, we’ll gain more understanding into\\ndifferent models and can reason about what model will work best instead of running\\nhundreds or thousands of experiments.\\nDebugging ML Models\\nDebugging is an inherent part of developing any piece of software. ML models aren’t\\nan exception. Debugging is never fun, and debugging ML models can be especially\\nfrustrating for the following three reasons.\\nFirst, ML models fail silently, a topic we’ll cover in depth in Chapter 8 . The code\\ncompiles. The loss decreases as it should. The correct functions are called. The\\npredictions are made, but the predictions are wrong. The developers don’t notice the\\nerrors. And worse, users don’t either and use the predictions as if the application was\\nfunctioning as it should.\\nSecond, even when you think you’ve found the bug, it can be frustratingly slow\\nto validate whether the bug has been fixed. When debugging a traditional software\\nprogram, you might be able to make changes to the buggy code and see the result\\nimmediately. However, when making changes to an ML model, you might have to\\nretrain the model and wait until it converges to see whether the bug is fixed, which\\ncan take hours. In some cases, you can’t even be sure whether the bugs are fixed until\\nthe model is deployed to the users.\\nThird, debugging ML models is hard because of their cross-functional complexity.\\nThere are many components in an ML system: data, labels, features, ML algorithms,\\ncode, infrastructure, etc. These different components might be owned by different\\nteams. For example, data is managed by data engineers, labels by subject matter\\nexperts, ML algorithms by data scientists, and infrastructure by ML engineers or\\nthe ML platform team. When an error occurs, it could be because of any of these\\ncomponents or a combination of them, making it hard to know where to look or who', 'experts, ML algorithms by data scientists, and infrastructure by ML engineers or\\nthe ML platform team. When an error occurs, it could be because of any of these\\ncomponents or a combination of them, making it hard to know where to look or who\\nshould be looking into it.\\nModel Development and Training | 165', 'Here are some of the things that might cause an ML model to fail:\\nTheoretical constraints\\nAs discussed previously, each model comes with its own assumptions about the\\ndata and the features it uses. A model might fail because the data it learns from\\ndoesn’t conform to its assumptions. For example, you use a linear model for the\\ndata whose decision boundaries aren’t linear.\\nPoor implementation of model\\nThe model might be a good fit for the data, but the bugs are in the implementa‐\\ntion of the model. For example, if you use PyTorch, you might have forgotten\\nto stop gradient updates during evaluation when you should. The more com‐\\nponents a model has, the more things that can go wrong, and the harder it\\nis to figure out which goes wrong. However, with models being increasingly\\ncommoditized and more and more companies using off-the-shelf models, this is\\nbecoming less of a problem.\\nPoor choice of hyperparameters\\nWith the same model, one set of hyperparameters can give you the state-of-the-\\nart result but another set of hyperparameters might cause the model to never\\nconverge. The model is a great fit for your data, and its implementation is\\ncorrect, but a poor set of hyperparameters might render your model useless.\\nData problems\\nThere are many things that could go wrong in data collection and preprocessing\\nthat might cause your models to perform poorly, such as data samples and\\nlabels being incorrectly paired, noisy labels, features normalized using outdated\\nstatistics, and more.\\nPoor choice of features\\nThere might be many possible features for your models to learn from. Too many\\nfeatures might cause your models to overfit to the training data or cause data\\nleakage. Too few features might lack predictive power to allow your models to\\nmake good predictions.\\nDebugging should be both preventive and curative. Y ou should have healthy practices\\nto minimize the opportunities for bugs to proliferate as well as a procedure for\\ndetecting, locating, and fixing bugs. Having the discipline to follow both the best\\npractices and the debugging procedure is crucial in developing, implementing, and\\ndeploying ML models.\\n166 | Chapter 6: Model Development and Offline Evaluation', 'There is, unfortunately, still no scientific approach to debugging in ML. However,\\nthere have been a number of tried-and-true debugging techniques published by\\nexperienced ML engineers and researchers. The following are three of them. Readers\\ninterested in learning more might want to check out Andrej Karpathy’s awesome post\\n“ A Recipe for Training Neural Networks”.\\nStart simple and gradually add more components\\nStart with the simplest model and then slowly add more components to see if\\nit helps or hurts the performance. For example, if you want to build a recurrent\\nneural network (RNN), start with just one level of RNN cell before stacking\\nmultiple together or adding more regularization. If you want to use a BERT-like\\nmodel (Devlin et al. 2018), which uses both a masked language model (MLM)\\nand next sentence prediction (NSP) loss, you might want to use only the MLM\\nloss before adding NSP loss.\\nCurrently, many people start out by cloning an open source implementation of a\\nstate-of-the-art model and plugging in their own data. On the off-chance that it\\nworks, it’s great. But if it doesn’t, it’s very hard to debug the system because the\\nproblem could have been caused by any of the many components in the model.\\nOverfit a single batch\\nAfter you have a simple implementation of your model, try to overfit a small\\namount of training data and run evaluation on the same data to make sure that\\nit gets to the smallest possible loss. If it’s for image recognition, overfit on 10\\nimages and see if you can get the accuracy to be 100%, or if it’s for machine\\ntranslation, overfit on 100 sentence pairs and see if you can get to a BLEU score\\nof near 100. If it can’t overfit a small amount of data, there might be something\\nwrong with your implementation.\\nSet a random seed\\nThere are so many factors that contribute to the randomness of your model:\\nweight initialization, dropout, data shuffling, etc. Randomness makes it hard to\\ncompare results across different experiments—you have no idea if the change in\\nperformance is due to a change in the model or a different random seed. Setting\\na random seed ensures consistency between different runs. It also allows you to\\nreproduce errors and other people to reproduce your results.\\nModel Development and Training | 167', '11 For products that serve a large number of users, you also have to care about scalability in serving a model,\\nwhich is outside of the scope of an ML project so not covered in this book.\\n12 According to Wikipedia, “Out-of-core algorithms are algorithms that are designed to process data that are too\\nlarge to fit into a computer’s main memory at once” (s.v. “External memory algorithm, ” https://oreil.ly/apv5m).\\n13 Tim Salimans, Y aroslav Bulatov, and contributors, gradient-checkpointing repository, 2017,\\nhttps://oreil.ly/GTUgC.\\nDistributed Training\\nAs models are getting bigger and more resource-intensive, companies care a lot more\\nabout training at scale. 11 Expertise in scalability is hard to acquire because it requires\\nhaving regular access to massive compute resources. Scalability is a topic that merits\\na series of books. This section covers some notable issues to highlight the challenges\\nof doing ML at scale and provide a scaffold to help you plan the resources for your\\nproject accordingly.\\nIt’s common to train a model using data that doesn’t fit into memory. It’s especially\\ncommon when dealing with medical data such as CT scans or genome sequences. It\\ncan also happen with text data if you work for teams that train large language models\\n(cue OpenAI, Google, NVIDIA, Cohere).\\nWhen your data doesn’t fit into memory, your algorithms for preprocessing (e.g.,\\nzero-centering, normalizing, whitening), shuffling, and batching data will need to run\\nout of core and in parallel. 12 When a sample of your data is large, e.g., one machine\\ncan handle a few samples at a time, you might only be able to work with a small batch\\nsize, which leads to instability for gradient descent-based optimization.\\nIn some cases, a data sample is so large it can’t even fit into memory and you will\\nhave to use something like gradient checkpointing, a technique that leverages the\\nmemory footprint and compute trade-off to make your system do more computation\\nwith less memory. According to the authors of the open source package gradient-\\ncheckpointing, “For feed-forward models we were able to fit more than 10x larger\\nmodels onto our GPU, at only a 20% increase in computation time. ” 13 Even when a\\nsample fits into memory, using checkpointing can allow you to fit more samples into\\na batch, which might allow you to train your model faster.\\nData parallelism\\nIt’s now the norm to train ML models on multiple machines. The most common', 'sample fits into memory, using checkpointing can allow you to fit more samples into\\na batch, which might allow you to train your model faster.\\nData parallelism\\nIt’s now the norm to train ML models on multiple machines. The most common\\nparallelization method supported by modern ML frameworks is data parallelism:\\nyou split your data on multiple machines, train your model on all of them, and\\naccumulate gradients. This gives rise to a couple of issues.\\n168 | Chapter 6: Model Development and Offline Evaluation', '14 Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan Vaidynathan, Srinivas Sridharan, Dhiraj\\nKalamkar, Bharat Kaul, and Pradeep Dubey, “Distributed Deep Learning Using Synchronous Stochastic\\nGradient Descent, ” arXiv, February 22, 2016, https://oreil.ly/ma8Y6.\\n15 Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz, “Revisiting Distributed\\nSynchronous SGD, ” ICLR 2017, https://oreil.ly/dzVZ5; Matei Zaharia, Andy Konwinski, Anthony D. Joseph,\\nRandy Katz, and Ion Stoica, “Improving MapReduce Performance in Heterogeneous Environments, ” 8th\\nUSENIX Symposium on Operating Systems Design and Implementation, https://oreil.ly/FWswd; Aaron Har‐\\nlap, Henggang Cui, Wei Dai, Jinliang Wei, Gregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson, and Eric\\nP . Xing, “ Addressing the Straggler Problem for Iterative Convergent Parallel ML ” (SoCC ’16, Santa Clara, CA,\\nOctober 5–7, 2016), https://oreil.ly/wZgOO.\\n16 Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio Ranzato, et al.,\\n“Large Scale Distributed Deep Networks, ” NIPS 2012, https://oreil.ly/EWPun.\\n17 Jim Dowling, “Distributed TensorFlow, ” O’Reilly Media, December 19, 2017, https://oreil.ly/VYlOP.\\nA challenging problem is how to accurately and effectively accumulate gradients from\\ndifferent machines. As each machine produces its own gradient, if your model waits\\nfor all of them to finish a run—synchronous stochastic gradient descent (SGD)—\\nstragglers will cause the entire system to slow down, wasting time and resources. 14\\nThe straggler problem grows with the number of machines, as the more workers, the\\nmore likely that at least one worker will run unusually slowly in a given iteration.\\nHowever, there have been many algorithms that effectively address this problem.15\\nIf your model updates the weight using the gradient from each machine separately—\\nasynchronous SGD—gradient staleness might become a problem because the gradi‐\\nents from one machine have caused the weights to change before the gradients from\\nanother machine have come in.16\\nThe difference between synchronous SGD and asynchronous SGD is illustrated in\\nFigure 6-6.\\nFigure 6-6. Synchronous SGD versus asynchronous SGD for data parallelism. Source:\\nAdapted from an image by Jim Dowling17\\nModel Development and Training | 169', '18 Feng Niu, Benjamin Recht, Christopher Ré, and Stephen J. Wright, “Hogwild!: A Lock-Free Approach to\\nParallelizing Stochastic Gradient Descent, ” 2011, https://oreil.ly/sAEbv.\\n19 Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\\nNeelakantan, et al., “Language Models Are Few-Shot Learners, ” arXiv, May 28, 2020, https://oreil.ly/qjg2S.\\n20 Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team, “ An Empirical Model of Large-\\nBatch Training, ” arXiv, December 14, 2018, https://oreil.ly/mcjbV; Christopher J. Shallue, Jaehoon Lee,\\nJoseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George E. Dahl, “Measuring the Effects of\\nData Parallelism on Neural Network Training, ” Journal of Machine Learning Research 20 (2019): 1–49,\\nhttps://oreil.ly/YAEOM.\\nIn theory, asynchronous SGD converges but requires more steps than synchronous\\nSGD. However, in practice, when the number of weights is large, gradient updates\\ntend to be sparse, meaning most gradient updates only modify small fractions of\\nthe parameters, and it’s less likely that two gradient updates from different machines\\nwill modify the same weights. When gradient updates are sparse, gradient staleness\\nbecomes less of a problem and the model converges similarly for both synchronous\\nand asynchronous SGD.18\\nAnother problem is that spreading your model on multiple machines can cause your\\nbatch size to be very big. If a machine processes a batch size of 1,000, then 1,000\\nmachines process a batch size of 1M (OpenAI’s GPT-3 175B uses a batch size of\\n3.2M in 2020). 19 To oversimplify the calculation, if training an epoch on a machine\\ntakes 1M steps, training on 1,000 machines might take only 1,000 steps. An intuitive\\napproach is to scale up the learning rate to account for more learning at each step, but\\nwe also can’t make the learning rate too big as it will lead to unstable convergence. In\\npractice, increasing the batch size past a certain point yields diminishing returns.20\\nLast but not least, with the same model setup, the main worker sometimes uses a lot\\nmore resources than other workers. If that’s the case, to make the most use out of all\\nmachines, you need to figure out a way to balance out the workload among them.\\nThe easiest way, but not the most effective way, is to use a smaller batch size on the\\nmain worker and a larger batch size on other workers.\\nModel parallelism\\nWith data parallelism, each worker has its own copy of the whole model and does', 'The easiest way, but not the most effective way, is to use a smaller batch size on the\\nmain worker and a larger batch size on other workers.\\nModel parallelism\\nWith data parallelism, each worker has its own copy of the whole model and does\\nall the computation necessary for its copy of the model. Model parallelism is when\\ndifferent components of your model are trained on different machines, as shown\\nin Figure 6-7 . For example, machine 0 handles the computation for the first two\\nlayers while machine 1 handles the next two layers, or some machines can handle the\\nforward pass while several others handle the backward pass.\\n170 | Chapter 6: Model Development and Offline Evaluation', '21 Jure Leskovec, Mining Massive Datasets course, Stanford, lecture 13, 2020, https://oreil.ly/gZcja.\\nFigure 6-7. Data parallelism and model parallelism. Source: Adapted from an image by\\nJure Leskovec21\\nModel parallelism can be misleading because in some cases parallelism doesn’t mean\\nthat different parts of the model in different machines are executed in parallel. For\\nexample, if your model is a massive matrix and the matrix is split into two halves on\\ntwo machines, then these two halves might be executed in parallel. However, if your\\nmodel is a neural network and you put the first layer on machine 1 and the second\\nlayer on machine 2, and layer 2 needs outputs from layer 1 to execute, then machine 2\\nhas to wait for machine 1 to finish first to run.\\nPipeline parallelism is a clever technique to make different components of a model\\non different machines run more in parallel. There are multiple variants to this,\\nbut the key idea is to break the computation of each machine into multiple parts.\\nWhen machine 1 finishes the first part of its computation, it passes the result onto\\nmachine 2, then continues to the second part, and so on. Machine 2 now can execute\\nits computation on the first part while machine 1 executes its computation on the\\nsecond part.\\nModel Development and Training | 171', '22 Y anping Huang, Y oulong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, et\\nal., “GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism, ” arXiv, July 25, 2019, https://oreil.ly/wehkx.\\nTo make this concrete, consider you have four different machines and the first,\\nsecond, third, and fourth layers are on machine 1, 2, 3, and 4 respectively. With\\npipeline parallelism, each mini-batch is broken into four micro-batches. Machine\\n1 computes the first layer on the first micro-batch, then machine 2 computes the\\nsecond layer on machine 1’s results while machine 1 computes the first layer on the\\nsecond micro-batch, and so on. Figure 6-8 shows what pipeline parallelism looks like\\non four machines; each machine runs both the forward pass and the backward pass\\nfor one component of a neural network.\\nFigure 6-8. Pipeline parallelism for a neural network on four machines; each machine\\nruns both the forward pass (F) and the backward pass (B) for one component of the\\nneural network. Source: Adapted from an image by Huang et al.22\\nModel parallelism and data parallelism aren’t mutually exclusive. Many companies\\nuse both methods for better utilization of their hardware, even though the setup to\\nuse both methods can require significant engineering effort.\\nAutoML\\nThere’s a joke that a good ML researcher is someone who will automate themselves\\nout of job, designing an AI algorithm intelligent enough to design itself. It was funny\\nuntil the TensorFlow Dev Summit 2018, where Jeff Dean took the stage and declared\\nthat Google intended on replacing ML expertise with 100 times more computational\\npower, introducing AutoML to the excitement and horror of the community. Instead\\nof paying a group of 100 ML researchers/engineers to fiddle with various models and\\neventually select a suboptimal one, why not use that money on compute to search\\nfor the optimal model? A screenshot from the recording of the event is shown in\\nFigure 6-9.\\n172 | Chapter 6: Model Development and Offline Evaluation', '23 We’ll cover quantization in Chapter 7.\\nFigure 6-9. Jeff Dean unveiling Google’s AutoML at TensorFlow Dev Summit 2018\\nSoft AutoML: Hyperparameter tuning\\nAutoML refers to automating the process of finding ML algorithms to solve real-\\nworld problems. One mild form, and the most popular form, of AutoML in produc‐\\ntion is hyperparameter tuning. A hyperparameter is a parameter supplied by users\\nwhose value is used to control the learning process, e.g., learning rate, batch size,\\nnumber of hidden layers, number of hidden units, dropout probability, β1 and β2 in\\nAdam optimizer, etc. Even quantization—e.g., whether to use 32 bits, 16 bits, or 8\\nbits to represent a number or a mixture of these representations—can be considered a\\nhyperparameter to tune.23\\nWith different sets of hyperparameters, the same model can give drastically different\\nperformances on the same dataset. Melis et al. showed in their 2018 paper “On the\\nState of the Art of Evaluation in Neural Language Models”  that weaker models with\\nwell-tuned hyperparameters can outperform stronger, fancier models. The goal of\\nhyperparameter tuning is to find the optimal set of hyperparameters for a given\\nmodel within a search space—the performance of each set evaluated on a validation\\nset.\\nModel Development and Training | 173', '24 GSD is a well-documented technique. See “How Do People Come Up With All These Crazy Deep Learning\\nArchitectures?, ” Reddit, https://oreil.ly/5vEsH; “Debate About Science at Organizations like Google Brain/\\nFAIR/DeepMind, ” Reddit, https://oreil.ly/2K77r; “Grad Student Descent, ” Science Dryad, January 25, 2014,\\nhttps://oreil.ly/dIR9r; and Guy Zyskind (@GuyZys), “Grad Student Descent: the preferred #nonlinear #optimi‐\\nzation technique #machinelearning, ” Twitter, April 27, 2015, https://oreil.ly/SW1or.\\n25 auto-sklearn 2.0 also provides basic model selection capacity.\\n26 Our team at NVIDIA developed Milano, a framework-agnostic tool for automatic hyperparameter tuning\\nusing random search.\\n27 A common practice I’ve observed is to start with coarse-to-fine random search, then experiment with\\nBayesian or grid search once the search space has been significantly reduced.\\nDespite knowing its importance, many still ignore systematic approaches to hyper‐\\nparameter tuning in favor of a manual, gut-feeling approach. The most popular is\\narguably graduate student descent (GSD), a technique in which a graduate student\\nfiddles around with the hyperparameters until the model works.24\\nHowever, more and more people are adopting hyperparameter tuning as part of their\\nstandard pipelines. Popular ML frameworks either come with built-in utilities or\\nhave third-party utilities for hyperparameter tuning—for example, scikit-learn with\\nauto-sklearn,25 TensorFlow with Keras Tuner, and Ray with Tune. Popular methods\\nfor hyperparameter tuning include random search, 26 grid search, and Bayesian opti‐\\nmization.27 The book AutoML: Methods, Systems, Challenges by the AutoML group at\\nthe University of Freiburg dedicates its first chapter (which you can read online for\\nfree) to hyperparameter optimization.\\nWhen tuning hyperparameters, keep in mind that a model’s performance might be\\nmore sensitive to the change in one hyperparameter than another, and therefore\\nsensitive hyperparameters should be more carefully tuned.\\nIt’s crucial to never use your test split to tune hyperparameters.\\nChoose the best set of hyperparameters for a model based on its\\nperformance on a validation split, then report the model’s final\\nperformance on the test split. If you use your test split to tune\\nhyperparameters, you risk overfitting your model to the test split.\\nHard AutoML: Architecture search and learned optimizer\\nSome teams take hyperparameter tuning to the next level: what if we treat other com‐', 'performance on the test split. If you use your test split to tune\\nhyperparameters, you risk overfitting your model to the test split.\\nHard AutoML: Architecture search and learned optimizer\\nSome teams take hyperparameter tuning to the next level: what if we treat other com‐\\nponents of a model or the entire model as hyperparameters. The size of a convolution\\nlayer or whether or not to have a skip layer can be considered a hyperparameter.\\nInstead of manually putting a pooling layer after a convolutional layer or ReLu\\n(rectified linear unit) after linear, you give your algorithm these building blocks and\\nlet it figure out how to combine them. This area of research is known as architectural\\n174 | Chapter 6: Model Development and Offline Evaluation', '28 Barret Zoph and Quoc V . Le, “Neural Architecture Search with Reinforcement Learning, ” arXiv, November\\n5, 2016, https://oreil.ly/FhsuQ; Esteban Real, Alok Aggarwal, Y anping Huang, and Quoc V . Le, “Regularized\\nEvolution for Image Classifier Architecture Search, ” AAAI 2019, https://oreil.ly/FWYjn.\\n29 Y ou can make the search space continuous to allow differentiation, but the resulting architecture has to be\\nconverted into a discrete architecture. See “DARTS: Differentiable Architecture Search” (Liu et al. 2018).\\n30 We cover learning procedures and optimizers in more detail in the section “Basic ML Reviews” in the book’s\\nGitHub repository.\\nsearch, or neural architecture search (NAS) for neural networks, as it searches for the\\noptimal model architecture.\\nA NAS setup consists of three components:\\nA search space\\nDefines possible model architectures—i.e., building blocks to choose from and\\nconstraints on how they can be combined.\\nA performance estimation strategy\\nTo evaluate the performance of a candidate architecture without having to train\\neach candidate architecture from scratch until convergence. When we have a\\nlarge number of candidate architectures, say 1,000, training all of them until\\nconvergence can be costly.\\nA search strategy\\nTo explore the search space. A simple approach is random search—randomly\\nchoosing from all possible configurations—which is unpopular because it’s pro‐\\nhibitively expensive even for NAS. Common approaches include reinforcement\\nlearning (rewarding the choices that improve the performance estimation) and\\nevolution (adding mutations to an architecture, choosing the best-performing\\nones, adding mutations to them, and so on).28\\nFor NAS, the search space is discrete—the final architecture uses only one of the\\navailable options for each layer/operation, 29 and you have to provide the set of build‐\\ning blocks. The common building blocks are various convolutions of different sizes,\\nlinear, various activations, pooling, identity, zero, etc. The set of building blocks varies\\nbased on the base architecture, e.g., convolutional neural networks or transformers.\\nIn a typical ML training process, you have a model and then a learning procedure,\\nan algorithm that helps your model find the set of parameters that minimize a given\\nobjective function for a given set of data. The most common learning procedure for\\nneural networks today is gradient descent, which leverages an optimizer to specify', 'an algorithm that helps your model find the set of parameters that minimize a given\\nobjective function for a given set of data. The most common learning procedure for\\nneural networks today is gradient descent, which leverages an optimizer to specify\\nhow to update a model’s weights given gradient updates. 30 Popular optimizers are,\\nas you probably already know, Adam, Momentum, SGD, etc. In theory, you can\\ninclude optimizers as building blocks in NAS and search for one that works best.\\nIn practice, this is difficult to do, since optimizers are sensitive to the setting of\\nModel Development and Training | 175', '31 Luke Metz, Niru Maheswaranathan, C. Daniel Freeman, Ben Poole, and Jascha Sohl-Dickstein, “Tasks,\\nStability, Architecture, and Compute: Training More Effective Learned Optimizers, and Using Them to Train\\nThemselves, ” arXiv, September 23, 2020, https://oreil.ly/IH7eT.\\n32 Mingxing Tan and Quoc V . Le, “EfficientNet: Improving Accuracy and Efficiency through AutoML and\\nModel Scaling, ” Google AI Blog, May 29, 2019, https://oreil.ly/gonEn.\\ntheir hyperparameters, and the default hyperparameters don’t often work well across\\narchitectures.\\nThis leads to an exciting research direction: what if we replace the functions that\\nspecify the update rule with a neural network? How much to update the model’s\\nweights will be calculated by this neural network. This approach results in learned\\noptimizers, as opposed to hand-designed optimizers.\\nSince learned optimizers are neural networks, they need to be trained. Y ou can train\\nyour learned optimizer on the same dataset you’re training the rest of your neural\\nnetwork on, but this requires you to train an optimizer every time you have a task.\\nAnother approach is to train a learned optimizer once on a set of existing tasks\\n—using aggregated loss on those tasks as the loss function and existing designed\\noptimizers as the learning rule—and use it for every new task after that. For example,\\nMetz et al. constructed a set of thousands of tasks to train learned optimizers. Their\\nlearned optimizer was able to generalize to both new datasets and domains as well as\\nnew architectures.31 And the beauty of this approach is that the learned optimizer can\\nthen be used to train a better-learned optimizer, an algorithm that improves on itself.\\nWhether it’s architecture search or meta-learning learning rules, the up-front training\\ncost is expensive enough that only a handful of companies in the world can afford\\nto pursue them. However, it’s important for people interested in ML in production to\\nbe aware of the progress in AutoML for two reasons. First, the resulting architectures\\nand learned optimizers can allow ML algorithms to work off-the-shelf on multiple\\nreal-world tasks, saving production time and cost, during both training and inferenc‐\\ning. For example, EfficientNets, a family of models produced by Google’s AutoML\\nteam, surpass state-of-the-art accuracy with up to 10x better efficiency.32 Second, they\\nmight be able to solve many real-world tasks previously impossible with existing\\narchitectures and optimizers.', 'ing. For example, EfficientNets, a family of models produced by Google’s AutoML\\nteam, surpass state-of-the-art accuracy with up to 10x better efficiency.32 Second, they\\nmight be able to solve many real-world tasks previously impossible with existing\\narchitectures and optimizers.\\n176 | Chapter 6: Model Development and Offline Evaluation', '33 Samantha Murphy, “The Evolution of Facebook News Feed, ” Mashable, March 12, 2013,\\nhttps://oreil.ly/1HMXh.\\n34 Iveta Ryšavá, “What Mark Zuckerberg’s News Feed Looked Like in 2006, ” Newsfeed.org, January 14, 2016,\\nhttps://oreil.ly/XZT6Q.\\nFour Phases of ML Model Development\\nBefore we transition to model training, let’s take a look at the four phases of ML\\nmodel development. Once you’ve decided to explore ML, your strategy depends on\\nwhich phase of ML adoption you are in. There are four phases of adopting ML. The\\nsolutions from a phase can be used as baselines to evaluate the solutions from the\\nnext phase:\\nPhase 1. Before machine learning\\nIf this is your first time trying to make this type of prediction from this type\\nof data, start with non-ML solutions. Y our first stab at the problem can be the\\nsimplest heuristics. For example, to predict what letter users are going to type\\nnext in English, you can show the top three most common English letters, “e, ” “t, ”\\nand “a, ” which might get your accuracy to be 30%.\\nFacebook newsfeed was introduced in 2006 without any intelligent algorithms—\\nposts were shown in chronological order, as shown in Figure 6-10 .33 It wasn’t\\nuntil 2011 that Facebook started displaying news updates you were most interes‐\\nted in at the top of the feed.\\nFigure 6-10. Facebook newsfeed circa 2006. Source: Iveta Ryšavá34\\nModel Development and Training | 177', '35 Martin Zinkevich, “Rules of Machine Learning: Best Practices for ML Engineering, ” Google, 2019,\\nhttps://oreil.ly/YtEsN.\\n36 We’ll go in depth about how often to update your models in Chapter 9.\\n37 See the section “Business and ML Objectives” on page 26.\\nAccording to Martin Zinkevich in his magnificent “Rules of Machine Learning:\\nBest Practices for ML Engineering”: “If you think that machine learning will give\\nyou a 100% boost, then a heuristic will get you 50% of the way there. ”35 Y ou might\\neven find that non-ML solutions work fine and you don’t need ML yet.\\nPhase 2. Simplest machine learning models\\nFor your first ML model, you want to start with a simple algorithm, something\\nthat gives you visibility into its working to allow you to validate the usefulness of\\nyour problem framing and your data. Logistic regression, gradient-boosted trees,\\nk-nearest neighbors can be great for that. They are also easier to implement and\\ndeploy, which allows you to quickly build out a framework from data engineering\\nto development to deployment that you can test out and gain confidence on.\\nPhase 3. Optimizing simple models\\nOnce you have your ML framework in place, you can focus on optimizing the\\nsimple ML models with different objective functions, hyperparameter search,\\nfeature engineering, more data, and ensembles.\\nPhase 4. Complex models\\nOnce you’ve reached the limit of your simple models and your use case demands\\nsignificant model improvement, experiment with more complex models.\\nY ou’ll also want to experiment to figure out how quickly your model decays in\\nproduction (e.g., how often it’ll need to be retrained) so that you can build out\\nyour infrastructure to support this retraining requirement.36\\nModel Offline Evaluation\\nOne common but quite difficult question I often encounter when helping companies\\nwith their ML strategies is: “How do I know that our ML models are any good?” In\\none case, a company deployed ML to detect intrusions to 100 surveillance drones, but\\nthey had no way of measuring how many intrusions their system failed to detect, and\\nthey couldn’t decide if one ML algorithm was better than another for their needs.\\nLacking a clear understanding of how to evaluate your ML systems is not necessarily\\na reason for your ML project to fail, but it might make it impossible to find the\\nbest solution for your need, and make it harder to convince your managers to adopt\\nML. Y ou might want to partner with the business team to develop metrics for model', 'a reason for your ML project to fail, but it might make it impossible to find the\\nbest solution for your need, and make it harder to convince your managers to adopt\\nML. Y ou might want to partner with the business team to develop metrics for model\\nevaluation that are more relevant to your company’s business.37\\n178 | Chapter 6: Model Development and Offline Evaluation', '38 Fréchet inception distance, a common metric for measuring the quality of synthesized images. The smaller\\nthe value, the higher the quality is supposed to be.\\n39 The accuracy, in this case, would be around 0.80.\\n40 Revisit the section “Using the right evaluation metrics” on page 106 for a refresh on the asymmetry of F1.\\nIdeally, the evaluation methods should be the same during both development and\\nproduction. But in many cases, the ideal is impossible because during development,\\nyou have ground truth labels, but in production, you don’t.\\nFor certain tasks, it’s possible to infer or approximate labels in production based on\\nusers’ feedback, as covered in the section “Natural Labels” on page 91. For example,\\nfor the recommendation task, it’s possible to infer if a recommendation is good by\\nwhether users click on it. However, there are many biases associated with this.\\nFor other tasks, you might not be able to evaluate your model’s performance in\\nproduction directly and might have to rely on extensive monitoring to detect changes\\nand failures in your ML system’s performance. We’ll cover monitoring in Chapter 8.\\nOnce your model is deployed, you’ll need to continue monitoring and testing your\\nmodel in production. In this section, we’ll discuss methods to evaluate your model’s\\nperformance before it’s deployed. We’ll start with the baselines against which we will\\nevaluate our models. Then we’ll cover some of the common methods to evaluate your\\nmodel beyond overall accuracy metrics.\\nBaselines\\nSomeone once told me that her new generative model achieved the FID score of 10.3\\non ImageNet.38 I had no idea what this number meant or whether her model would\\nbe useful for my problem.\\nAnother time, I helped a company implement a classification model where the\\npositive class appears 90% of the time. An ML engineer on the team told me, all\\nexcited, that their initial model achieved an F1 score of 0.90. I asked him how it was\\ncompared to random. He had no idea. It turned out that because for his task the\\nPOSITIVE class accounts for 90% of the labels, if his model randomly outputs the\\npositive class 90% of the time, its F1 score would also be around 0.90. 39 His model\\nmight as well be making predictions at random.40\\nModel Offline Evaluation | 179', 'Evaluation metrics, by themselves, mean little. When evaluating your model, it’s\\nessential to know the baseline you’re evaluating it against. The exact baselines should\\nvary from one use case to another, but here are the five baselines that might be useful\\nacross use cases:\\nRandom baseline\\nIf our model just predicts at random, what’s the expected performance? The\\npredictions are generated at random following a specific distribution, which can\\nbe the uniform distribution or the task’s label distribution.\\nFor example, consider the task that has two labels, NEGATIVE that appears 90%\\nof the time and POSITIVE that appears 10% of the time. Table 6-2 shows the F1\\nand accuracy scores of baseline models making predictions at random. However,\\nas an exercise to see how challenging it is for most people to have an intuition\\nfor these values, try to calculate these raw numbers in your head before looking at\\nthe table.\\nTable 6-2. F1 and accuracy scores of a baseline model predicting at random\\nRandom distribution Meaning F1 Accuracy\\nUniform random Predicting each label with equal probability (50%) 0.167 0.5\\nTask’s label distribution Predicting NEGATIVE 90% of the time, and POSITIVE 10% of the time 0.1 0.82\\nSimple heuristic\\nForget ML. If you just make predictions based on simple heuristics, what perfor‐\\nmance would you expect? For example, if you want to build a ranking system to\\nrank items on a user’s newsfeed with the goal of getting that user to spend more\\ntime on the newsfeed, how much time would a user spend if you just rank all the\\nitems in reverse chronological order, showing the latest one first?\\nZero rule baseline\\nThe zero rule baseline is a special case of the simple heuristic baseline when your\\nbaseline model always predicts the most common class.\\nFor example, for the task of recommending the app a user is most likely to use\\nnext on their phone, the simplest model would be to recommend their most\\nfrequently used app. If this simple heuristic can predict the next app accurately\\n70% of the time, any model you build has to outperform it significantly to justify\\nthe added complexity.\\nHuman baseline\\nIn many cases, the goal of ML is to automate what would have been otherwise\\ndone by humans, so it’s useful to know how your model performs compared to\\nhuman experts. For example, if you work on a self-driving system, it’s crucial to\\nmeasure your system’s progress compared to human drivers, because otherwise\\n180 | Chapter 6: Model Development and Offline Evaluation', 'you might never be able to convince your users to trust this system. Even if your\\nsystem isn’t meant to replace human experts and only to aid them in improving\\ntheir productivity, it’s still important to know in what scenarios this system would\\nbe useful to humans.\\nExisting solutions\\nIn many cases, ML systems are designed to replace existing solutions, which\\nmight be business logic with a lot of if/else statements or third-party solutions.\\nIt’s crucial to compare your new model to these existing solutions. Y our ML\\nmodel doesn’t always have to be better than existing solutions to be useful. A\\nmodel whose performance is a little bit inferior can still be useful if it’s much\\neasier or cheaper to use.\\nWhen evaluating a model, it’s important to differentiate between “a good system” and\\n“a useful system. ” A good system isn’t necessarily useful, and a bad system isn’t neces‐\\nsarily useless. A self-driving vehicle might be good if it’s a significant improvement\\nfrom previous self-driving systems, but it might not be useful if it doesn’t perform at\\nleast as well as human drivers. In some cases, even if an ML system drives better than\\nan average human, people might still not trust it, which renders it not useful. On the\\nother hand, a system that predicts what word a user will type next on their phone\\nmight be considered bad if it’s much worse than a native speaker. However, it might\\nstill be useful if its predictions can help users type faster some of the time.\\nEvaluation Methods\\nIn academic settings, when evaluating ML models, people tend to fixate on their\\nperformance metrics. However, in production, we also want our models to be robust,\\nfair, calibrated, and overall make sense. We’ll introduce some evaluation methods that\\nhelp with measuring these characteristics of a model.\\nPerturbation tests\\nA group of my students wanted to build an app to predict whether someone has\\nCOVID-19 through their cough. Their best model worked great on the training data,\\nwhich consisted of two-second long cough segments collected by hospitals. However,\\nwhen they deployed it to actual users, this model’s predictions were close to random.\\nOne of the reasons is that actual users’ coughs contain a lot of noise compared to\\nthe coughs collected in hospitals. Users’ recordings might contain background music\\nor nearby chatter. The microphones they use are of varying quality. They might start\\nrecording their coughs as soon as recording is enabled or wait for a fraction of a\\nsecond.', 'the coughs collected in hospitals. Users’ recordings might contain background music\\nor nearby chatter. The microphones they use are of varying quality. They might start\\nrecording their coughs as soon as recording is enabled or wait for a fraction of a\\nsecond.\\nIdeally, the inputs used to develop your model should be similar to the inputs your\\nmodel will have to work with in production, but it’s not possible in many cases. This\\nModel Offline Evaluation | 181', '41 Other examples of noisy data include images with different lighting or texts with accidental typos or inten‐\\ntional text modifications such as typing “long” as “loooooong. ”\\n42 Khristopher J. Brooks, “Disparity in Home Lending Costs Minorities Millions, Researchers Find, ” CBS News,\\nNovember 15, 2019, https://oreil.ly/TMPVl.\\n43 It might also be mandated by law to exclude sensitive information from the model training process.\\nis especially true when data collection is expensive or difficult and the best available\\ndata you have access to for training is still very different from your real-world data.\\nThe inputs your models have to work with in production are often noisy compared\\nto inputs in development. 41 The model that performs best on training data isn’t\\nnecessarily the model that performs best on noisy data.\\nTo get a sense of how well your model might perform with noisy data, you can\\nmake small changes to your test splits to see how these changes affect your model’s\\nperformance. For the task of predicting whether someone has COVID-19 from their\\ncough, you could randomly add some background noise or randomly clip the testing\\nclips to simulate the variance in your users’ recordings. Y ou might want to choose the\\nmodel that works best on the perturbed data instead of the one that works best on the\\nclean data.\\nThe more sensitive your model is to noise, the harder it will be to maintain it,\\nsince if your users’ behaviors change just slightly, such as they change their phones,\\nyour model’s performance might degrade. It also makes your model susceptible to\\nadversarial attack.\\nInvariance tests\\nA Berkeley study found that between 2008 and 2015, 1.3 million creditworthy Black\\nand Latino applicants had their mortgage applications rejected because of their\\nraces.42 When the researchers used the income and credit scores of the rejected\\napplications but deleted the race-identifying features, the applications were accepted.\\nCertain changes to the inputs shouldn’t lead to changes in the output. In the pre‐\\nceding case, changes to race information shouldn’t affect the mortgage outcome.\\nSimilarly, changes to applicants’ names shouldn’t affect their resume screening results\\nnor should someone’s gender affect how much they should be paid. If these happen,\\nthere are biases in your model, which might render it unusable no matter how good\\nits performance is.\\nTo avoid these biases, one solution is to do the same process that helped the Berkeley', 'nor should someone’s gender affect how much they should be paid. If these happen,\\nthere are biases in your model, which might render it unusable no matter how good\\nits performance is.\\nTo avoid these biases, one solution is to do the same process that helped the Berkeley\\nresearchers discover the biases: keep the inputs the same but change the sensitive\\ninformation to see if the outputs change. Better, you should exclude the sensitive\\ninformation from the features used to train the model in the first place.43\\n182 | Chapter 6: Model Development and Offline Evaluation', '44 For more information on calibrated recommendations, check out the paper “Calibrated Recommendations”\\nby Harald Steck in 2018 based on his work at Netflix.\\nDirectional expectation tests\\nCertain changes to the inputs should, however, cause predictable changes in outputs.\\nFor example, when developing a model to predict housing prices, keeping all the\\nfeatures the same but increasing the lot size shouldn’t decrease the predicted price,\\nand decreasing the square footage shouldn’t increase it. If the outputs change in the\\nopposite expected direction, your model might not be learning the right thing, and\\nyou need to investigate it further before deploying it.\\nModel calibration\\nModel calibration is a subtle but crucial concept to grasp. Imagine that someone\\nmakes a prediction that something will happen with a probability of 70%. What this\\nprediction means is that out of all the times this prediction is made, the predicted\\noutcome matches the actual outcome 70% of the time. If a model predicts that team\\nA will beat team B with a 70% probability, and out of the 1,000 times these two\\nteams play together, team A only wins 60% of the time, then we say that this model\\nisn’t calibrated. A calibrated model should predict that team A wins with a 60%\\nprobability.\\nModel calibration is often overlooked by ML practitioners, but it’s one of the most\\nimportant properties of any predictive system. To quote Nate Silver in his book The\\nSignal and the Noise , calibration is “one of the most important tests of a forecast—\\nI would argue that it is the single most important one. ”\\nWe’ll walk through two examples to show why model calibration is important. First,\\nconsider the task of building a recommender system to recommend what movies\\nusers will likely watch next. Suppose user A watches romance movies 80% of the time\\nand comedy 20% of the time. If your recommender system shows exactly the movies\\nA will most likely watch, the recommendations will consist of only romance movies\\nbecause A is much more likely to watch romance than any other type of movies. Y ou\\nmight want a more calibrated system whose recommendations are representative of\\nusers’ actual watching habits. In this case, they should consist of 80% romance and\\n20% comedy.44\\nSecond, consider the task of building a model to predict how likely it is that a user\\nwill click on an ad. For the sake of simplicity, imagine that there are only two ads,\\nad A and ad B. Y our model predicts that this user will click on ad A with a 10%', '20% comedy.44\\nSecond, consider the task of building a model to predict how likely it is that a user\\nwill click on an ad. For the sake of simplicity, imagine that there are only two ads,\\nad A and ad B. Y our model predicts that this user will click on ad A with a 10%\\nprobability and on ad B with an 8% probability. Y ou don’t need your model to be\\ncalibrated to rank ad A above ad B. However, if you want to predict how many clicks\\nyour ads will get, you’ll need your model to be calibrated. If your model predicts that\\na user will click on ad A with a 10% probability but in reality the ad is only clicked on\\nModel Offline Evaluation | 183', '5% of the time, your estimated number of clicks will be way off. If you have another\\nmodel that gives the same ranking but is better calibrated, you might want to consider\\nthe better calibrated one.\\nTo measure a model’s calibration, a simple method is counting: you count the number\\nof times your model outputs the probability X and the frequency Y of that prediction\\ncoming true, and plot X against Y. The graph for a perfectly calibrated model will\\nhave X equal Y at all data points. In scikit-learn, you can plot the calibration curve\\nof a binary classifier with the method sklearn.calibration.calibration_curve, as\\nshown in Figure 6-11.\\nFigure 6-11. The calibration curves of different models on a toy task. The logistic\\nregression model is the best calibrated model because it directly optimizes logistic loss.\\nSource: scikit-learn\\nTo calibrate your models, a common method is Platt scaling, which is implemented\\nin scikit-learn with sklearn.calibration.CalibratedClassifierCV. Another good\\nopen source implementation by Geoff Pleiss can be found on GitHub. For readers\\nwho want to learn more about the importance of model calibration and how to\\ncalibrate neural networks, Lee Richardson and Taylor Pospisil have an excellent blog\\npost based on their work at Google.\\n184 | Chapter 6: Model Development and Offline Evaluation', 'Confidence measurement\\nConfidence measurement can be considered a way to think about the usefulness\\nthreshold for each individual prediction. Indiscriminately showing all a model’s\\npredictions to users, even the predictions that the model is unsure about, can, at\\nbest, cause annoyance and make users lose trust in the system, such as an activity\\ndetection system on your smartwatch that thinks you’re running even though you’re\\njust walking a bit fast. At worst, it can cause catastrophic consequences, such as a\\npredictive policing algorithm that flags an innocent person as a potential criminal.\\nIf you only want to show the predictions that your model is certain about, how do\\nyou measure that certainty? What is the certainty threshold at which the predictions\\nshould be shown? What do you want to do with predictions below that threshold—\\ndiscard them, loop in humans, or ask for more information from users?\\nWhile most other metrics measure the system’s performance on average, confidence\\nmeasurement is a metric for each individual sample. System-level measurement is\\nuseful to get a sense of overall performance, but sample-level metrics are crucial when\\nyou care about your system’s performance on every sample.\\nSlice-based evaluation\\nSlicing means to separate your data into subsets and look at your model’s perfor‐\\nmance on each subset separately. A common mistake that I’ve seen in many compa‐\\nnies is that they are focused too much on coarse-grained metrics like overall F1 or\\naccuracy on the entire data and not enough on sliced-based metrics. This can lead to\\ntwo problems.\\nOne is that their model performs differently on different slices of data when the\\nmodel should perform the same. For example, their data has two subgroups, one\\nmajority and one minority, and the majority subgroup accounts for 90% of the data:\\n• Model A achieves 98% accuracy on the majority subgroup but only 80% on the•\\nminority subgroup, which means its overall accuracy is 96.2%.\\n• Model B achieves 95% accuracy on the majority and 95% on the minority, which•\\nmeans its overall accuracy is 95%.\\nThese two models are compared in Table 6-3. Which model would you choose?\\nTable 6-3. Two models’ performance on the majority and minority subgroups\\nMajority accuracy Minority accuracy Overall accuracy\\nModel A 98% 80% 96.2%\\nModel B 95% 95% 95%\\nModel Offline Evaluation | 185', '45 Maggie Zhang, “Google Photos Tags Two African-Americans As Gorillas Through Facial Recognition Soft‐\\nware, ” Forbes, July 1, 2015, https://oreil.ly/VYG2j.\\nIf a company focuses only on overall metrics, they might go with model A. They\\nmight be very happy with this model’s high accuracy until, one day, their end users\\ndiscover that this model is biased against the minority subgroup because the minor‐\\nity subgroup happens to correspond to an underrepresented demographic group. 45\\nThe focus on overall performance is harmful not only because of the potential\\npublic backlash, but also because it blinds the company to huge potential model\\nimprovements. If the company sees the two models’ slice-based performance, they\\nmight follow different strategies. For example, they might decide to improve model\\nA ’s performance on the minority subgroup, which leads to improving this model’s\\nperformance overall. Or they might keep both models the same but now have more\\ninformation to make a better-informed decision on which model to deploy.\\nAnother problem is that their model performs the same on different slices of data\\nwhen the model should perform differently. Some subsets of data are more critical.\\nFor example, when you build a model for user churn prediction (predicting when a\\nuser will cancel a subscription or a service), paid users are more critical than nonpaid\\nusers. Focusing on a model’s overall performance might hurt its performance on\\nthese critical slices.\\nA fascinating and seemingly counterintuitive reason why slice-based evaluation is\\ncrucial is Simpson’s paradox, a phenomenon in which a trend appears in several\\ngroups of data but disappears or reverses when the groups are combined. This means\\nthat model B can perform better than model A on all data together, but model A\\nperforms better than model B on each subgroup separately. Consider model A ’s and\\nmodel B’s performance on group A and group B as shown in Table 6-4 . Model\\nA outperforms model B for both group A and B, but when combined, model B\\noutperforms model A.\\nTable 6-4. An example of Simpson’s paradoxa\\nGroup A Group B Overall\\nModel A 93% (81/87) 73% (192/263) 78% (273/350)\\nModel B 87% (234/270) 69% (55/80) 83% (289/350)\\na Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\\n“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave', 'Model B 87% (234/270) 69% (55/80) 83% (289/350)\\na Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\\n“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave\\nLithotripsy,” British Medical Journal  (Clinical Research Edition) 292, no. 6524 (March 1986): 879–82, https://oreil.ly/X8oWr.\\n186 | Chapter 6: Model Development and Offline Evaluation', '46 P . J. Bickel, E. A. Hammel, and J. W . O’Connell, “Sex Bias in Graduate Admissions: Data from Berkeley, ”\\nScience 187 (1975): 398–404, https://oreil.ly/TeR7E.\\nSimpson’s paradox is more common than you’ d think. In 1973, Berkeley graduate\\nstatistics showed that the admission rate for men was much higher than for women,\\nwhich caused people to suspect biases against women. However, a closer look into\\nindividual departments showed that the admission rates for women were actually\\nhigher than those for men in four out of six departments,46 as shown in Table 6-5.\\nTable 6-5. Berkeley’s 1973 graduate admission dataa\\n All Men Women\\nDepartment Applicants Admitted Applicants Admitted Applicants Admitted\\nA 933 64% 825 62% 108 82%\\nB 585 63% 560 63% 25 68 %\\nC 918 35% 325 37 % 593 34%\\nD 792 34% 417 33% 375 35 %\\nE 584 25% 191 28 % 393 24%\\nF 714 6% 373 6% 341 7 %\\nTotal 12,763 41% 8,442 44% 4,321 35%\\na Data from Bickel et al. (1975)\\nRegardless of whether you’ll actually encounter this paradox, the point here is that\\naggregation can conceal and contradict actual situations. To make informed decisions\\nregarding what model to choose, we need to take into account its performance not\\nonly on the entire data, but also on individual slices. Slice-based evaluation can give\\nyou insights to improve your model’s performance both overall and on critical data\\nand help detect potential biases. It might also help reveal non-ML problems. Once,\\nour team discovered that our model performed great overall but very poorly on\\ntraffic from mobile users. After investigating, we realized that it was because a button\\nwas half hidden on small screens (e.g., phone screens).\\nEven when you don’t think slices matter, understanding how your model performs in\\na more fine-grained way can give you confidence in your model to convince other\\nstakeholders, like your boss or your customers, to trust your ML models.\\nTo track your model’s performance on critical slices, you’ d first need to know what\\nyour critical slices are. Y ou might wonder how to discover critical slices in your data.\\nSlicing is, unfortunately, still more of an art than a science, requiring intensive data\\nexploration and analysis. Here are the three main approaches:\\nModel Offline Evaluation | 187', '47 For readers interested in learning more about UX design across cultures, Jenny Shen has a great post.\\nHeuristics-based\\nSlice your data using domain knowledge you have of the data and the task at\\nhand. For example, when working with web traffic, you might want to slice your\\ndata along dimensions like mobile versus desktop, browser type, and locations.\\nMobile users might behave very differently from desktop users. Similarly, inter‐\\nnet users in different geographic locations might have different expectations on\\nwhat a website should look like.47\\nError analysis\\nManually go through misclassified examples and find patterns among them. We\\ndiscovered our model’s problem with mobile users when we saw that most of the\\nmisclassified examples were from mobile users.\\nSlice finder\\nThere has been research to systemize the process of finding slices, including\\nChung et al. ’s “Slice Finder: Automated Data Slicing for Model Validation”  in\\n2019 and covered in Sumyea Helal’s “Subgroup Discovery Algorithms: A Survey\\nand Empirical Evaluation”  (2016). The process generally starts with generating\\nslice candidates with algorithms such as beam search, clustering, or decision,\\nthen pruning out clearly bad candidates for slices, and then ranking the candi‐\\ndates that are left.\\nKeep in mind that once you have discovered these critical slices, you will need\\nsufficient, correctly labeled data for each of these slices for evaluation. The quality of\\nyour evaluation is only as good as the quality of your evaluation data.\\nSummary\\nIn this chapter, we’ve covered the ML algorithm part of ML systems, which many\\nML practitioners consider to be the most fun part of an ML project lifecycle. With\\nthe initial models, we can bring to life (in the form of predictions) all our hard work\\nin data and feature engineering, and can finally evaluate our hypothesis (i.e., we can\\npredict the outputs given the inputs).\\nWe started with how to select ML models best suited for our tasks. Instead of going\\ninto pros and cons of each individual model architecture—which is a fool’s errand\\ngiven the growing pools of existing models—the chapter outlined the aspects you\\nneed to consider to make an informed decision on which model is best for your\\nobjectives, constraints, and requirements.\\n188 | Chapter 6: Model Development and Offline Evaluation', 'We then continued to cover different aspects of model development. We covered not\\nonly individual models but also ensembles of models, a technique widely used in\\ncompetitions and leaderboard-style research.\\nDuring the model development phase, you might experiment with many different\\nmodels. Intensive tracking and versioning of your many experiments are generally\\nagreed to be important, but many ML engineers still skip it because doing it might\\nfeel like a chore. Therefore, having tools and appropriate infrastructure to automate\\nthe tracking and versioning process is essential. We’ll cover tools and infrastructure\\nfor ML production in Chapter 10.\\nAs models today are getting bigger and consuming more data, distributed training is\\nbecoming an essential skill for ML model developers, and we discussed techniques\\nfor parallelism including data parallelism, model parallelism, and pipeline parallel‐\\nism. Making your models work on a large distributed system, like the one that runs\\nmodels with hundreds of millions, if not billions, of parameters, can be challenging\\nand require specialized system engineering expertise.\\nWe ended the chapter with how to evaluate your models to pick the best one to\\ndeploy. Evaluation metrics don’t mean much unless you have a baseline to compare\\nthem to, and we covered different types of baselines you might want to consider\\nfor evaluation. We also covered a range of evaluation techniques necessary to\\nsanity check your models before further evaluating your models in a production\\nenvironment.\\nOften, no matter how good your offline evaluation of a model is, you still can’t be sure\\nof your model’s performance in production until that model has been deployed. In\\nthe next chapter, we’ll go over how to deploy a model.\\nSummary | 189', 'CHAPTER 7\\nModel Deployment and Prediction Service\\nIn Chapters 4 through 6, we have discussed the considerations for developing an ML\\nmodel, from creating training data, extracting features, and developing the model to\\ncrafting metrics to evaluate this model. These considerations constitute the logic of\\nthe model—instructions on how to go from raw data into an ML model, as shown\\nin Figure 7-1. Developing this logic requires both ML knowledge and subject matter\\nexpertise. In many companies, this is the part of the process that is done by the ML or\\ndata science teams.\\nFigure 7-1. Different aspects that make up the ML model logic\\nIn this chapter, we’ll discuss another part in the iterative process: deploying your\\nmodel. “Deploy” is a loose term that generally means making your model run‐\\nning and accessible. During model development, your model usually runs in a\\n191', '1 We’ll cover development environments in detail in Chapter 10.\\n2 We’ll go more into containers in Chapter 9.\\n3 CS 329S: Machine Learning Systems Design at Stanford; you can see the project demos on Y ouTube.\\ndevelopment environment.1 To be deployed, your model will have to leave the devel‐\\nopment environment. Y our model can be deployed to a staging environment for\\ntesting or to a production environment to be used by your end users. In this chapter,\\nwe focus on deploying models to production environments.\\nBefore we move forward, I want to emphasize that production is a spectrum. For\\nsome teams, production means generating nice plots in notebooks to show to the\\nbusiness team. For other teams, production means keeping your models up and\\nrunning for millions of users a day. If your work is in the first scenario, your\\nproduction environment is similar to the development environment, and this chapter\\nis less relevant for you. If your work is closer to the second scenario, read on.\\nI once read somewhere on the internet: deploying is easy if you ignore all the hard\\nparts. If you want to deploy a model for your friends to play with, all you have to do\\nis to wrap your predict function in a POST request endpoint using Flask or FastAPI,\\nput the dependencies this predict function needs to run in a container, 2 and push\\nyour model and its associated container to a cloud service like AWS or GCP to expose\\nthe endpoint:\\n# Example of how to use FastAPI to turn your predict function', \"# Example of how to use FastAPI to turn your predict function \\n# into a POST endpoint\\n@app.route('/predict', methods=['POST'])\\ndef predict():\\n    X = request.get_json()['X']\\n    y = MODEL.predict(X).tolist()\\n    return json.dumps({'y': y}), 200\\nY ou can use this exposed endpoint for downstream applications: e.g., when an appli‐\\ncation receives a prediction request from a user, this request is sent to the exposed\\nendpoint, which returns a prediction. If you’re familiar with the necessary tools, you\\ncan have a functional deployment in an hour. My students, after a 10-week course,\\nwere all able to deploy an ML application as their final projects even though few have\\nhad deployment experience before.3\\nThe hard parts include making your model available to millions of users with a\\nlatency of milliseconds and 99% uptime, setting up the infrastructure so that the right\\nperson can be immediately notified when something goes wrong, figuring out what\\nwent wrong, and seamlessly deploying the updates to fix what’s wrong.\\n192 | Chapter 7: Model Deployment and Prediction Service\", '4 See the discussion on “data serialization” in the section “Data Formats” on page 53.\\nIn many companies, the responsibility of deploying models falls into the hands of\\nthe same people who developed those models. In many other companies, once a\\nmodel is ready to be deployed, it will be exported and handed off to another team\\nto deploy it. However, this separation of responsibilities can cause high overhead\\ncommunications across teams and make it slow to update your model. It also can\\nmake it hard to debug should something go wrong. We’ll discuss more on team\\nstructures in Chapter 11.\\nExporting a model means converting this model into a format that\\ncan be used by another application. Some people call this process\\n“serialization. ”4 There are two parts of a model that you can export:\\nthe model definition and the model’s parameter values. The model\\ndefinition defines the structure of your model, such as how many\\nhidden layers it has and how many units in each layer. The param‐\\neter values provide the values for these units and layers. Usually,\\nthese two parts are exported together.\\nIn TensorFlow 2, you might use tf.keras.Model.save() to export\\nyour model into TensorFlow’s SavedModel format. In PyTorch, you\\nmight use torch.onnx.export() to export your model into ONNX\\nformat.\\nRegardless of whether your job involves deploying ML models, being cognizant of\\nhow your models are used can give you an understanding of their constraints and\\nhelp you tailor them to their purposes.\\nIn this chapter, we’ll start off with some common myths about ML deployment that\\nI’ve often heard from people who haven’t deployed ML models. We’ll then discuss\\nthe two main ways a model generates and serves its predictions to users: online pre‐\\ndiction and batch prediction. The process of generating predictions is called inference.\\nWe’ll continue with where the computation for generating predictions should be\\ndone: on the device (also referred to as the edge) and the cloud. How a model serves\\nand computes the predictions influences how it should be designed, the infrastruc‐\\nture it requires, and the behaviors that users encounter.\\nModel Deployment and Prediction Service | 193', 'If you come from an academic background, some of the topics discussed in this\\nchapter might be outside your comfort zone. If an unfamiliar term comes up, take a\\nmoment to look it up. If a section becomes too dense, feel free to skip it. This chapter\\nis modular, so skipping a section shouldn’t affect your understanding of another\\nsection.\\nMachine Learning Deployment Myths\\nAs discussed in Chapter 1 , deploying an ML model can be very different from\\ndeploying a traditional software program. This difference might cause people who\\nhave never deployed a model before to either dread the process or underestimate how\\nmuch time and effort it will take. In this section, we’ll debunk some of the common\\nmyths about the deployment process, which will, hopefully, put you in a good state of\\nmind to begin the process. This section will be most helpful to people with little to no\\ndeploying experience.\\nMyth 1: You Only Deploy One or Two ML Models at a Time\\nWhen doing academic projects, I was advised to choose a small problem to focus on,\\nwhich usually led to a single model. Many people from academic backgrounds I’ve\\ntalked to tend to also think of ML production in the context of a single model. Sub‐\\nsequently, the infrastructure they have in mind doesn’t work for actual applications,\\nbecause it can only support one or two models.\\nIn reality, companies have many, many ML models. An application might have\\nmany different features, and each feature might require its own model. Consider a\\nride-sharing app like Uber. It needs a model to predict each of the following elements:\\nride demand, driver availability, estimated time of arrival, dynamic pricing, fraudu‐\\nlent transaction, customer churn, and more. Additionally, if this app operates in 20\\ncountries, until you can have models that generalize across different user-profiles,\\ncultures, and languages, each country would need its own set of models. So with 20\\ncountries and 10 models for each country, you already have 200 models. Figure 7-2\\nshows a wide range of the tasks that leverage ML at Netflix.\\n194 | Chapter 7: Model Deployment and Prediction Service', '5 Ville Tuulos, “Human-Centric Machine Learning Infrastructure @Netflix, ” InfoQ, 2018, video, 49:11,\\nhttps://oreil.ly/j4Hfx.\\n6 Wayne Cunningham, “Science at Uber: Powering Machine Learning at Uber, ” Uber Engineering Blog, Septem‐\\nber 10, 2019, https://oreil.ly/WfaCF.\\n7 Daniel Papasian and Todd Underwood, “OpML ’20—How ML Breaks: A Decade of Outages for One Large\\nML Pipeline, ” Google, 2020, video, 19:06, https://oreil.ly/HjQm0.\\n8 Lucas Bernardi, Themistoklis Mavridis, and Pablo Estevez, “150 Successful Machine Learning Models: 6\\nLessons Learned at Booking.com, ” KDD ’19: Proceedings of the 25th ACM SIGKDD International Conference\\non Knowledge Discovery & Data Mining (July 2019): 1743–51, https://oreil.ly/Ea1Ke.\\n9 “2021 Enterprise Trends in Machine Learning, ” Algorithmia, https://oreil.ly/9kdcw.\\nFigure 7-2. Different tasks that leverage ML at Netflix. Source: Ville Tuulos5\\nIn fact, Uber has thousands of models in production. 6 At any given moment, Google\\nhas thousands of models training concurrently with hundreds of billions parameters\\nin size. 7 Booking.com has 150+ models. 8 A 2021 study by Algorithmia shows that\\namong organizations with over 25,000 employees, 41% have more than 100 models in\\nproduction.9\\nMyth 2: If We Don’t Do Anything, Model Performance\\nRemains the Same\\nSoftware doesn’t age like fine wine. It ages poorly. The phenomenon in which a\\nsoftware program degrades over time even if nothing seems to have changed is\\nknown as “software rot” or “bit rot. ”\\nMachine Learning Deployment Myths | 195', '10 We’ll discuss data distribution shifts further in Chapter 8.\\n11 Christopher Null, “10 Companies Killing It at DevOps, ” TechBeacon, 2015, https://oreil.ly/JvNwu.\\n12 Qian Yu, “Machine Learning with Flink in Weibo, ” QCon 2019, video, 17:57, https://oreil.ly/RcTMv.\\n13 Josh Wills, “Instrumentation, Observability and Monitoring of Machine Learning Models, ” InfoQ 2019,\\nhttps://oreil.ly/5Ot5m.\\nML systems aren’t immune to it. On top of that, ML systems suffer from what are\\nknown as data distribution shifts, when the data distribution your model encounters\\nin production is different from the data distribution it was trained on. 10 Therefore, an\\nML model tends to perform best right after training and to degrade over time.\\nMyth 3: You Won’t Need to Update Your Models as Much\\nPeople tend to ask me: “How often should I update my models?” It’s the wrong\\nquestion to ask. The right question should be: “How often can I update my models?”\\nSince a model’s performance decays over time, we want to update it as fast as possible.\\nThis is an area of ML where we should learn from existing DevOps best practices.\\nEven back in 2015, people were already constantly pushing out updates to their\\nsystems. Etsy deployed 50 times/day, Netflix thousands of times per day, AWS every\\n11.7 seconds.11\\nWhile many companies still only update their models once a month, or even once a\\nquarter, Weibo’s iteration cycle for updating some of their ML models is 10 minutes.12\\nI’ve heard similar numbers at companies like Alibaba and ByteDance (the company\\nbehind TikTok).\\nIn the words of Josh Wills, a former staff engineer at Google and director of data\\nengineering at Slack, “We’re always trying to bring new models into production just\\nas fast as humanly possible. ”13\\nWe’ll discuss more on the frequency to retrain your models in Chapter 9.\\nMyth 4: Most ML Engineers Don’t Need to Worry About Scale\\nWhat “scale” means varies from application to application, but examples include a\\nsystem that serves hundreds of queries per second or millions of users a month.\\nY ou might argue that, if so, only a small number of companies need to worry\\nabout it. There is only one Google, one Facebook, one Amazon. That’s true, but a\\nsmall number of large companies employ the majority of the software engineering\\nworkforce. According to the Stack Overflow Developer Survey 2019, more than half\\nof the respondents worked for a company of at least 100 employees (see Figure 7-3).', 'small number of large companies employ the majority of the software engineering\\nworkforce. According to the Stack Overflow Developer Survey 2019, more than half\\nof the respondents worked for a company of at least 100 employees (see Figure 7-3).\\nThis isn’t a perfect correlation, but a company of 100 employees has a good chance of\\nserving a reasonable number of users.\\n196 | Chapter 7: Model Deployment and Prediction Service', '14 “Developer Survey Results, ” Stack Overflow, 2019, https://oreil.ly/guYIq.\\nFigure 7-3. The distribution of the size of companies where software engineers work.\\nSource: Adapted from an image by Stack Overflow14\\nI couldn’t find a survey for ML-specific roles, so I asked on Twitter and found similar\\nresults. This means that if you’re looking for an ML-related job in the industry, you’ll\\nlikely work for a company of at least 100 employees, whose ML applications likely\\nneed to be scalable. Statistically speaking, an ML engineer should care about scale.\\nBatch Prediction Versus Online Prediction\\nOne fundamental decision you’ll have to make that will affect both your end users\\nand developers working on your system is how it generates and serves its predictions\\nto end users: online or batch. The terminologies surrounding batch and online\\nprediction are still quite confusing due to the lack of standardized practices in the\\nindustry. I’ll do my best to explain the nuances of each term in this section. If you\\nfind any of the terms mentioned here too confusing, feel free to ignore them for now.\\nIf you forget everything else, there are three main modes of prediction that I hope\\nyou’ll remember:\\n• Batch prediction, which uses only batch features.•\\n• Online prediction that uses only batch features (e.g., precomputed embeddings).•\\n• Online prediction that uses both batch features and streaming features. This is•\\nalso known as streaming prediction.\\nBatch Prediction Versus Online Prediction | 197', 'Online prediction is when predictions are generated and returned as soon as requests\\nfor these predictions arrive. For example, you enter an English sentence into Goo‐\\ngle Translate and get back its French translation immediately. Online prediction is\\nalso known as on-demand prediction. Traditionally, when doing online prediction,\\nrequests are sent to the prediction service via RESTful APIs (e.g., HTTP requests—\\nsee “Data Passing Through Services” on page 73). When prediction requests are\\nsent via HTTP requests, online prediction is also known as synchronous prediction:\\npredictions are generated in synchronization with requests.\\nBatch prediction is when predictions are generated periodically or whenever triggered.\\nThe predictions are stored somewhere, such as in SQL tables or an in-memory data‐\\nbase, and retrieved as needed. For example, Netflix might generate movie recommen‐\\ndations for all of its users every four hours, and the precomputed recommendations\\nare fetched and shown to users when they log on to Netflix. Batch prediction is also\\nknown as asynchronous prediction: predictions are generated asynchronously with\\nrequests.\\nTerminology Confusion\\nThe terms “online prediction” and “batch prediction” can be con‐\\nfusing. Both can make predictions for multiple samples (in batch)\\nor one sample at a time. To avoid this confusion, people sometimes\\nprefer the terms “synchronous prediction” and “asynchronous pre‐\\ndiction. ” However, this distinction isn’t perfect either, because when\\nonline prediction leverages a real-time transport to send prediction\\nrequests to your model, the requests and predictions technically are\\nasynchronous.\\nFigure 7-4 shows a simplified architecture for batch prediction, and Figure 7-5 shows\\na simplified version of online prediction using only batch features. We’ll go over what\\nit means to use only batch features next.\\nFigure 7-4. A simplified architecture for batch prediction\\n198 | Chapter 7: Model Deployment and Prediction Service', 'Figure 7-5. A simplified architecture for online prediction that uses only batch features\\nAs discussed in Chapter 3, features computed from historical data, such as data in\\ndatabases and data warehouses, are batch features. Features computed from streaming\\ndata—data in real-time transports—are streaming features. In batch prediction, only\\nbatch features are used. In online prediction, however, it’s possible to use both batch\\nfeatures and streaming features. For example, after a user puts in an order on Door‐\\nDash, they might need the following features to estimate the delivery time:\\nBatch features\\nThe mean preparation time of this restaurant in the past\\nStreaming features\\nIn the last 10 minutes, how many other orders they have, and how many delivery\\npeople are available\\nStreaming Features Versus Online Features\\nI’ve heard the terms “streaming features” and “online features” used\\ninterchangeably. They are actually different. Online features are\\nmore general, as they refer to any feature used for online predic‐\\ntion, including batch features stored in memory.\\nA very common type of batch feature used for online prediction,\\nespecially session-based recommendations, is item embeddings.\\nItem embeddings are usually precomputed in batch and fetched\\nwhenever they are needed for online prediction. In this case,\\nembeddings can be considered online features but not streaming\\nfeatures.\\nStreaming features refer exclusively to features computed from\\nstreaming data.\\nBatch Prediction Versus Online Prediction | 199', 'A simplified architecture for online prediction that uses both streaming features and\\nbatch features is shown in Figure 7-6 . Some companies call this kind of prediction\\n“streaming prediction” to distinguish it from the kind of online prediction that\\ndoesn’t use streaming features.\\nFigure 7-6. A simplified architecture for online prediction that uses both batch features\\nand streaming features\\nHowever, online prediction and batch prediction don’t have to be mutually exclusive.\\nOne hybrid solution is that you precompute predictions for popular queries, then\\ngenerate predictions online for less popular queries. Table 7-1 summarizes the key\\npoints to consider for online prediction and batch prediction.\\nTable 7-1. Some key differences between batch prediction and online prediction\\nBatch prediction (asynchronous) Online prediction (synchronous)\\nFrequency Periodical, such as every four hours As soon as requests come\\nUseful for Processing accumulated data when you don’t need\\nimmediate results (such as recommender systems)\\nWhen predictions are needed as soon as a data sample\\nis generated (such as fraud detection)\\nOptimized for High throughput Low latency\\n200 | Chapter 7: Model Deployment and Prediction Service', '15 David Curry, “Grubhub Revenue and Usage Statistics (2022), ” Business of Apps, January 11, 2022,\\nhttps://oreil.ly/jX43M; “ Average Number of Grubhub Orders per Day Worldwide from 2011 to 2020, ” Statista,\\nhttps://oreil.ly/Tu9fm.\\n16 The URL of the entry point for a service, which, in this case, is the prediction service of your ML model.\\nIn many applications, online prediction and batch prediction are used side by side for\\ndifferent use cases. For example, food ordering apps like DoorDash and UberEats use\\nbatch prediction to generate restaurant recommendations—it’ d take too long to gen‐\\nerate these recommendations online because there are many restaurants. However,\\nonce you click on a restaurant, food item recommendations are generated using\\nonline prediction.\\nMany people believe that online prediction is less efficient, both in terms of cost\\nand performance, than batch prediction because you might not be able to batch\\ninputs together and leverage vectorization or other optimization techniques. This is\\nnot necessarily true, as we already discussed in the section “Batch Processing Versus\\nStream Processing” on page 78.\\nAlso, with online prediction, you don’t have to generate predictions for users who\\naren’t visiting your site. Imagine you run an app where only 2% of your users log in\\ndaily—e.g., in 2020, Grubhub had 31 million users and 622,000 daily orders. 15 If you\\ngenerate predictions for every user each day, the compute used to generate 98% of\\nyour predictions will be wasted.\\nFrom Batch Prediction to Online Prediction\\nTo people coming to ML from an academic background, the more natural way to\\nserve predictions is probably online. Y ou give your model an input and it generates\\na prediction as soon as it receives that input. This is likely how most people interact\\nwith their models while prototyping. This is also likely easier to do for most com‐\\npanies when first deploying a model. Y ou export your model, upload the exported\\nmodel to Amazon SageMaker or Google App Engine, and get back an exposed\\nendpoint.16 Now, if you send a request that contains an input to that endpoint, it will\\nsend back a prediction generated on that input.\\nA problem with online prediction is that your model might take too long to generate\\npredictions. Instead of generating predictions as soon as they arrive, what if you\\ncompute predictions in advance and store them in your database, and fetch them', 'send back a prediction generated on that input.\\nA problem with online prediction is that your model might take too long to generate\\npredictions. Instead of generating predictions as soon as they arrive, what if you\\ncompute predictions in advance and store them in your database, and fetch them\\nwhen requests arrive? This is exactly what batch prediction does. With this approach,\\nyou can generate predictions for multiple inputs at once, leveraging distributed\\ntechniques to process a high volume of samples efficiently.\\nBatch Prediction Versus Online Prediction | 201', '17 If a new user joins, you can give them some generic recommendations.\\nBecause the predictions are precomputed, you don’t have to worry about how long\\nit’ll take your models to generate predictions. For this reason, batch prediction can\\nalso be seen as a trick to reduce the inference latency of more complex models—the\\ntime it takes to retrieve a prediction is usually less than the time it takes to generate it.\\nBatch prediction is good for when you want to generate a lot of predictions and don’t\\nneed the results immediately. Y ou don’t have to use all the predictions generated. For\\nexample, you can make predictions for all customers on how likely they are to buy a\\nnew product, and reach out to the top 10%.\\nHowever, the problem with batch prediction is that it makes your model less\\nresponsive to users’ change preferences. This limitation can be seen even in more\\ntechnologically progressive companies like Netflix. Say you’ve been watching a lot\\nof horror movies lately, so when you first log in to Netflix, horror movies dominate\\nrecommendations. But you’re feeling bright today, so you search “comedy” and start\\nbrowsing the comedy category. Netflix should learn and show you more comedy in\\nyour list of their recommendations, right? As of writing this book, it can’t update the\\nlist until the next batch of recommendations is generated, but I have no doubt that\\nthis limitation will be addressed in the near future.\\nAnother problem with batch prediction is that you need to know what requests to\\ngenerate predictions for in advance. In the case of recommending movies for users,\\nyou know in advance how many users to generate recommendations for. 17 However,\\nfor cases when you have unpredictable queries—if you have a system to translate\\nfrom English to French, it might be impossible to anticipate every possible English\\ntext to be translated—you need to use online prediction to generate predictions as\\nrequests arrive.\\nIn the Netflix example, batch prediction causes mild inconvenience (which is tightly\\ncoupled with user engagement and retention), not catastrophic failures. There are\\nmany applications where batch prediction would lead to catastrophic failures or just\\nwouldn’t work. Examples where online prediction is crucial include high-frequency\\ntrading, autonomous vehicles, voice assistants, unlocking your phone using face or\\nfingerprints, fall detection for elderly care, and fraud detection. Being able to detect a', 'wouldn’t work. Examples where online prediction is crucial include high-frequency\\ntrading, autonomous vehicles, voice assistants, unlocking your phone using face or\\nfingerprints, fall detection for elderly care, and fraud detection. Being able to detect a\\nfraudulent transaction that happened three hours ago is still better than not detecting\\nit at all, but being able to detect it in real time can prevent the fraudulent transaction\\nfrom going through.\\n202 | Chapter 7: Model Deployment and Prediction Service', 'Batch prediction is a workaround for when online prediction isn’t cheap enough or\\nisn’t fast enough. Why generate one million predictions in advance and worry about\\nstoring and retrieving them if you can generate each prediction as needed at the exact\\nsame cost and same speed?\\nAs hardware becomes more customized and powerful and better techniques are\\nbeing developed to allow faster, cheaper online predictions, online prediction might\\nbecome the default.\\nIn recent years, companies have made significant investments to move from batch\\nprediction to online prediction. To overcome the latency challenge of online predic‐\\ntion, two components are required:\\n• A (near) real-time pipeline that can work with incoming data, extract streaming•\\nfeatures (if needed), input them into a model, and return a prediction in near real\\ntime. A streaming pipeline with real-time transport and a stream computation\\nengine can help with that.\\n• A model that can generate predictions at a speed acceptable to its end users. For•\\nmost consumer apps, this means milliseconds.\\nWe’ve discussed stream processing in Chapter 3. We’ll continue discussing the unifi‐\\ncation of the stream pipeline with the batch pipeline in the next section. Then we’ll\\ndiscuss how to speed up inference in the section “Model optimization” on page 216.\\nUnifying Batch Pipeline and Streaming Pipeline\\nBatch prediction is largely a product of legacy systems. In the last decade, big data\\nprocessing has been dominated by batch systems like MapReduce and Spark, which\\nallow us to periodically process a large amount of data very efficiently. When compa‐\\nnies started with ML, they leveraged their existing batch systems to make predictions.\\nWhen these companies want to use streaming features for their online prediction,\\nthey need to build a separate streaming pipeline. Let’s go through an example to make\\nthis more concrete.\\nBatch Prediction Versus Online Prediction | 203', 'Imagine you want to build a model to predict arrival time for an application like\\nGoogle Maps. The prediction is continually updated as a user’s trip progresses. A\\nfeature you might want to use is the average speed of all the cars in your path in the\\nlast five minutes. For training, you might use data from the last month. To extract\\nthis feature from your training data, you might want to put all your data into a\\ndataframe to compute this feature for multiple training samples at the same time.\\nDuring inference, this feature will be continually computed on a sliding window. This\\nmeans that in training this feature is computed in batch, whereas during inference\\nthis feature is computed in a streaming process.\\nHaving two different pipelines to process your data is a common cause for bugs\\nin ML production. One cause for bugs is when the changes in one pipeline aren’t\\ncorrectly replicated in the other, leading to two pipelines extracting two different\\nsets of features. This is especially common if the two pipelines are maintained by\\ntwo different teams, such as the ML team maintains the batch pipeline for training\\nwhile the deployment team maintains the stream pipeline for inference, as shown in\\nFigure 7-7.\\nFigure 7-7. Having two different pipelines for training and inference is a common source\\nfor bugs for ML in production\\nFigure 7-8 shows a more detailed but also more complex feature of the data pipeline\\nfor ML systems that do online prediction. The boxed element labeled Research is\\nwhat people are often exposed to in an academic environment.\\n204 | Chapter 7: Model Deployment and Prediction Service', '18 Shuyi Chean and Fabian Hueske, “Streaming SQL to Unify Batch & Stream Processing w/ Apache Flink\\n@Uber, ” InfoQ, https://oreil.ly/XoaNu; Yu, “Machine Learning with Flink in Weibo. ”\\nFigure 7-8. A data pipeline for ML systems that do online prediction\\nBuilding infrastructure to unify stream processing and batch processing has become a\\npopular topic in recent years for the ML community. Companies including Uber and\\nWeibo have made major infrastructure overhauls to unify their batch and stream pro‐\\ncessing pipelines by using a stream processor like Apache Flink. 18 Some companies\\nuse feature stores to ensure the consistency between the batch features used during\\ntraining and the streaming features used in prediction. We’ll discuss feature stores in\\nChapter 10.\\nBatch Prediction Versus Online Prediction | 205', '19 Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang, “ A Survey of Model Compression and Acceleration for\\nDeep Neural Networks, ” arXiv, June 14, 2020, https://oreil.ly/1eMho.\\n20 Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman, “Speeding up Convolutional Neural Networks with\\nLow Rank Expansions, ” arXiv, May 15, 2014, https://oreil.ly/4Vf4s.\\nModel Compression\\nWe’ve talked about a streaming pipeline that allows an ML system to extract stream‐\\ning features from incoming data and input them into an ML model in (near) real\\ntime. However, having a near (real-time) pipeline isn’t enough for online prediction.\\nIn the next section, we’ll discuss techniques for fast inference for ML models.\\nIf the model you want to deploy takes too long to generate predictions, there are three\\nmain approaches to reduce its inference latency: make it do inference faster, make the\\nmodel smaller, or make the hardware it’s deployed on run faster.\\nThe process of making a model smaller is called model compression, and the process\\nto make it do inference faster is called inference optimization. Originally, model\\ncompression was to make models fit on edge devices. However, making models\\nsmaller often makes them run faster.\\nWe’ll discuss inference optimization in the section “Model optimization” on page 216,\\nand we’ll discuss the landscape for hardware backends being developed specifically\\nfor running ML models faster in the section “ML on the Cloud and on the Edge” on\\npage 212. Here, we’ll discuss model compression.\\nThe number of research papers on model compression is growing. Off-the-shelf\\nutilities are proliferating. As of April 2022, Awesome Open Source has a list of “The\\nTop 168 Model Compression Open Source Projects” , and that list is growing. While\\nthere are many new techniques being developed, the four types of techniques that you\\nmight come across the most often are low-rank optimization, knowledge distillation,\\npruning, and quantization. Readers interested in a comprehensive review might want\\nto check out Cheng et al. ’s “Survey of Model Compression and Acceleration for Deep\\nNeural Networks, ” which was updated in 2020.19\\nLow-Rank Factorization\\nThe key idea behind low-rank factorization is to replace high-dimensional tensors\\nwith lower-dimensional tensors. 20 One type of low-rank factorization is compact\\nconvolutional filters, where the over-parameterized (having too many parameters)\\nconvolution filters are replaced with compact blocks to both reduce the number of', 'with lower-dimensional tensors. 20 One type of low-rank factorization is compact\\nconvolutional filters, where the over-parameterized (having too many parameters)\\nconvolution filters are replaced with compact blocks to both reduce the number of\\nparameters and increase speed.\\n206 | Chapter 7: Model Deployment and Prediction Service', '21 Forrest N. Iandola, Song Han, Matthew W . Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer,\\n“SqueezeNet: AlexNet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size, ” arXiv, November\\n4, 2016, https://oreil.ly/xs3mi.\\n22 Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco\\nAndreetto, and Hartwig Adam, “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\\nApplications, ” arXiv, April 17, 2017, https://oreil.ly/T84fD.\\nFor example, by using a number of strategies including replacing 3 × 3 convolution\\nwith 1 × 1 convolution, SqueezeNets achieves AlexNet-level accuracy on ImageNet\\nwith 50 times fewer parameters.21\\nSimilarly, MobileNets decomposes the standard convolution of size K × K × C into a\\ndepthwise convolution (K × K × 1) and a pointwise convolution (1 × 1 × C), with K\\nbeing the kernel size and C being the number of channels. This means that each new\\nconvolution uses only K2 + C instead of K2C parameters. If K = 3, this means an eight\\nto nine times reduction in the number of parameters (see Figure 7-9).22\\nFigure 7-9. Compact convolutional filters in MobileNets. The standard convolutional\\nfilters in (a) are replaced by depthwise convolution in (b) and pointwise convolution\\nin (c) to build a depthwise separable filter. Source: Adapted from an image by Howard\\net al.\\nThis method has been used to develop smaller models with significant acceleration\\ncompared to standard models. However, it tends to be specific to certain types\\nof models (e.g., compact convolutional filters are specific to convolutional neural\\nModel Compression | 207', '23 Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, “Distilling the Knowledge in a Neural Network, ” arXiv, March\\n9, 2015, https://oreil.ly/OJEPW.\\n24 Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf, “DistilBERT, a Distilled Version of BERT:\\nSmaller, Faster, Cheaper and Lighter, ” arXiv, October 2, 2019, https://oreil.ly/mQWBv.\\n25 Hence the name “pruning. ”\\nnetworks) and requires a lot of architectural knowledge to design, so it’s not widely\\napplicable to many use cases yet.\\nKnowledge Distillation\\nKnowledge distillation is a method in which a small model (student) is trained to\\nmimic a larger model or ensemble of models (teacher). The smaller model is what\\nyou’ll deploy. Even though the student is often trained after a pretrained teacher, both\\nmay also be trained at the same time. 23 One example of a distilled network used in\\nproduction is DistilBERT, which reduces the size of a BERT model by 40% while\\nretaining 97% of its language understanding capabilities and being 60% faster.24\\nThe advantage of this approach is that it can work regardless of the architectural\\ndifferences between the teacher and the student networks. For example, you can get\\na random forest as the student and a transformer as the teacher. The disadvantage\\nof this approach is that it’s highly dependent on the availability of a teacher network.\\nIf you use a pretrained model as the teacher model, training the student network\\nwill require less data and will likely be faster. However, if you don’t have a teacher\\navailable, you’ll have to train a teacher network before training a student network,\\nand training a teacher network will require a lot more data and take more time\\nto train. This method is also sensitive to applications and model architectures, and\\ntherefore hasn’t found wide usage in production.\\nPruning\\nPruning was a method originally used for decision trees where you remove sections of\\na tree that are uncritical and redundant for classification.25 As neural networks gained\\nwider adoption, people started to realize that neural networks are over-parameterized\\nand began to find ways to reduce the workload caused by the extra parameters.\\nPruning, in the context of neural networks, has two meanings. One is to remove\\nentire nodes of a neural network, which means changing its architecture and reduc‐\\ning its number of parameters. The more common meaning is to find parameters least\\nuseful to predictions and set them to 0. In this case, pruning doesn’t reduce the total', 'entire nodes of a neural network, which means changing its architecture and reduc‐\\ning its number of parameters. The more common meaning is to find parameters least\\nuseful to predictions and set them to 0. In this case, pruning doesn’t reduce the total\\nnumber of parameters, only the number of nonzero parameters. The architecture of\\nthe neural network remains the same. This helps with reducing the size of a model\\nbecause pruning makes a neural network more sparse, and sparse architecture tends\\nto require less storage space than dense structure. Experiments show that pruning\\n208 | Chapter 7: Model Deployment and Prediction Service', '26 Jonathan Frankle and Michael Carbin, “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural\\nNetworks, ” ICLR 2019, https://oreil.ly/ychdl.\\n27 Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag, “What Is the State of Neural\\nNetwork Pruning?” arXiv, March 6, 2020, https://oreil.ly/VQsC3.\\n28 Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell, “Rethinking the Value of Network\\nPruning, ” arXiv, March 5, 2019, https://oreil.ly/mB4IZ.\\n29 Michael Zhu and Suyog Gupta, “To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model\\nCompression, ” arXiv, November 13, 2017, https://oreil.ly/KBRjy.\\n30 Matthieu Courbariaux, Y oshua Bengio, and Jean-Pierre David, “BinaryConnect: Training Deep Neural Net‐\\nworks with Binary Weights During Propagations, ” arXiv, November 2, 2015, https://oreil.ly/Fwp2G; Moham‐\\nmad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi, “XNOR-Net: ImageNet Classification\\nUsing Binary Convolutional Neural Networks, ” arXiv, August 2, 2016, https://oreil.ly/gr3Ay.\\n31 Alan Boyle, Taylor Soper, and Todd Bishop, “Exclusive: Apple Acquires Xnor.ai, Edge AI Spin-out from Paul\\nAllen’s AI2, for Price in $200M Range, ” GeekWire, January 15, 2020, https://oreil.ly/HgaxC.\\ntechniques can reduce the nonzero parameter counts of trained networks by over\\n90%, decreasing storage requirements and improving computational performance of\\ninference without compromising overall accuracy. 26 In Chapter 11, we’ll discuss how\\npruning can introduce biases into your model.\\nWhile it’s generally agreed that pruning works, 27 there have been many discussions\\non the actual value of pruning. Liu et al. argued that the main value of pruning isn’t\\nin the inherited “important weights” but in the pruned architecture itself. 28 In some\\ncases, pruning can be useful as an architecture search paradigm, and the pruned\\narchitecture should be retrained from scratch as a dense model. However, Zhu et al.\\nshowed that the large sparse model after pruning outperformed the retrained dense\\ncounterpart.29\\nQuantization\\nQuantization is the most general and commonly used model compression method.\\nIt’s straightforward to do and generalizes over tasks and architectures.\\nQuantization reduces a model’s size by using fewer bits to represent its parameters.\\nBy default, most software packages use 32 bits to represent a float number (single\\nprecision floating point). If a model has 100M parameters and each requires 32 bits', 'Quantization reduces a model’s size by using fewer bits to represent its parameters.\\nBy default, most software packages use 32 bits to represent a float number (single\\nprecision floating point). If a model has 100M parameters and each requires 32 bits\\nto store, it’ll take up 400 MB. If we use 16 bits to represent a number, we’ll reduce the\\nmemory footprint by half. Using 16 bits to represent a float is called half precision.\\nInstead of using floats, you can have a model entirely in integers; each integer\\ntakes only 8 bits to represent. This method is also known as “fixed point. ” In the\\nextreme case, some have attempted the 1-bit representation of each weight (binary\\nweight neural networks), e.g., BinaryConnect and XNOR-Net. 30 The authors of the\\nXNOR-Net paper spun off Xnor.ai, a startup that focused on model compression. In\\nearly 2020, it was acquired by Apple for a reported $200M.31\\nModel Compression | 209', '32 As of October 2020, TensorFlow’s quantization aware training doesn’t actually train models with weights in\\nlower bits, but collects statistics to use for post-training quantization.\\n33 Chip Huyen, Igor Gitman, Oleksii Kuchaiev, Boris Ginsburg, Vitaly Lavrukhin, Jason Li, Vahid Noroozi,\\nand Ravi Gadde, “Mixed Precision Training for NLP and Speech Recognition with OpenSeq2Seq, ” NVIDIA\\nDevblogs, October 9, 2018, https://oreil.ly/WDT1l. It’s my post!\\n34 Shibo Wang and Pankaj Kanwar, “BFloat16: The Secret to High Performance on Cloud TPUs, ” Google Cloud\\nBlog, August 23, 2019, https://oreil.ly/ZG5p0.\\n35 Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Y aniv, and Y oshua Bengio, “Quantized Neural\\nNetworks: Training Neural Networks with Low Precision Weights and Activations, ” Journal of Machine\\nLearning Research 18 (2018): 1–30; Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,\\nAndrew Howard, Hartwig Adam, and Dmitry Kalenichenko, “Quantization and Training of Neural Networks\\nfor Efficient Integer-Arithmetic-Only Inference, ” arXiv, December 15, 2017, https://oreil.ly/sUuMT.\\nQuantization not only reduces memory footprint but also improves the computation\\nspeed. First, it allows us to increase our batch size. Second, less precision speeds up\\ncomputation, which further reduces training time and inference latency. Consider\\nthe addition of two numbers. If we perform the addition bit by bit, and each takes\\nx nanoseconds, it’ll take 32 x nanoseconds for 32-bit numbers but only 16 x nanosec‐\\nonds for 16-bit numbers.\\nThere are downsides to quantization. Reducing the number of bits to represent\\nyour numbers means that you can represent a smaller range of values. For values\\noutside that range, you’ll have to round them up and/or scale them to be in range.\\nRounding numbers leads to rounding errors, and small rounding errors can lead to\\nbig performance changes. Y ou also run the risk of rounding/scaling your numbers to\\nunder-/overflow and rendering it to 0. Efficient rounding and scaling is nontrivial to\\nimplement at a low level, but luckily, major frameworks have this built in.\\nQuantization can either happen during training (quantization aware training), 32\\nwhere models are trained in lower precision, or post-training, where models are\\ntrained in single-precision floating point and then quantized for inference. Using\\nquantization during training means that you can use less memory for each parameter,', 'where models are trained in lower precision, or post-training, where models are\\ntrained in single-precision floating point and then quantized for inference. Using\\nquantization during training means that you can use less memory for each parameter,\\nwhich allows you to train larger models on the same hardware.\\nRecently, low-precision training has become increasingly popular, with support from\\nmost modern training hardware. NVIDIA introduced Tensor Cores, processing units\\nthat support mixed-precision training. 33 Google TPUs (tensor processing units) also\\nsupport training with Bfloat16 (16-bit Brain Floating Point Format), which the\\ncompany dubbed “the secret to high performance on Cloud TPUs. ” 34 Training in\\nfixed-point is not yet as popular but has had a lot of promising results.35\\nFixed-point inference has become a standard in the industry. Some edge devi‐\\nces only support fixed-point inference. Most popular frameworks for on-device\\nML inference—Google’s TensorFlow Lite, Facebook’s PyTorch Mobile, NVIDIA ’s\\nTensorRT—offer post-training quantization for free with a few lines of code.\\n210 | Chapter 7: Model Deployment and Prediction Service', '36 Quoc Le and Kip Kaehler, “How We Scaled Bert To Serve 1+ Billion Daily Requests on CPUs, ” Roblox, May\\n27, 2020, https://oreil.ly/U01Uj.\\nCase Study\\nTo get a better understanding of how to optimize models in production, consider\\na fascinating case study from Roblox on how they scaled BERT to serve 1+ billion\\ndaily requests on CPUs.36 For many of their NLP services, they needed to handle over\\n25,000 inferences per second at a latency of under 20 ms, as shown in Figure 7-10.\\nThey started with a large BERT model with fixed shape input, then replaced BERT\\nwith DistilBERT and fixed shape input with dynamic shape input, and finally quan‐\\ntized it.\\nFigure 7-10. Latency improvement by various model compression methods. Source:\\nAdapted from an image by Le and Kaehler\\nThe biggest performance boost they got came from quantization. Converting 32-bit\\nfloating points to 8-bit integers reduces the latency 7 times and increases throughput\\n8 times.\\nThe results here seem very promising to improve latency; however, they should be\\ntaken with a grain of salt since there’s no mention of changes in output quality after\\neach performance improvement.\\nModel Compression | 211', '37 Amir Efrati and Kevin McLaughlin, “ As AWS Use Soars, Companies Surprised by Cloud Bills, ” The Infor‐\\nmation, February 25, 2019, https://oreil.ly/H9ans; Mats Bauer, “How Much Does Netflix Pay Amazon Web\\nServices Each Month?” Quora, 2020, https://oreil.ly/HtrBk.\\n38 “2021 State of Cloud Cost Report, ” Anodot, https://oreil.ly/5ZIJK.\\n39 “Burnt $72K Testing Firebase and Cloud Run and Almost Went Bankrupt, ” Hacker News, December 10, 2020,\\nhttps://oreil.ly/vsHHC; “How to Burn the Most Money with a Single Click in Azure, ” Hacker News, March\\n29, 2020, https://oreil.ly/QvCiI. We’ll discuss in more detail how companies respond to high cloud bills in the\\nsection “Public Cloud Versus Private Data Centers” on page 300.\\nML on the Cloud and on the Edge\\nAnother decision you’ll want to consider is where your model’s computation will\\nhappen: on the cloud or on the edge. On the cloud means a large chunk of compu‐\\ntation is done on the cloud, either public clouds or private clouds. On the edge\\nmeans a large chunk of computation is done on consumer devices—such as browsers,\\nphones, laptops, smartwatches, cars, security cameras, robots, embedded devices,\\nFPGAs (field programmable gate arrays), and ASICs (application-specific integrated\\ncircuits)—which are also known as edge devices.\\nThe easiest way is to package your model up and deploy it via a managed cloud\\nservice such as AWS or GCP , and this is how many companies deploy when they\\nget started in ML. Cloud services have done an incredible job to make it easy for\\ncompanies to bring ML models into production.\\nHowever, there are many downsides to cloud deployment. The first is cost. ML\\nmodels can be compute-intensive, and compute is expensive. Even back in 2018,\\nbig companies like Pinterest, Infor, and Intuit were already spending hundreds of\\nmillions of dollars on cloud bills every year. 37 That number for small and medium\\ncompanies can be between $50K and $2M a year. 38 A mistake in handling cloud\\nservices can cause startups to go bankrupt.39\\nAs their cloud bills climb, more and more companies are looking for ways to push\\ntheir computations to edge devices. The more computation is done on the edge, the\\nless is required on the cloud, and the less they’ll have to pay for servers.\\nOther than help with controlling costs, there are many properties that make edge\\ncomputing appealing. The first is that it allows your applications to run where cloud\\ncomputing cannot. When your models are on public clouds, they rely on stable', 'Other than help with controlling costs, there are many properties that make edge\\ncomputing appealing. The first is that it allows your applications to run where cloud\\ncomputing cannot. When your models are on public clouds, they rely on stable\\ninternet connections to send data to the cloud and back. Edge computing allows your\\nmodels to work in situations where there are no internet connections or where the\\nconnections are unreliable, such as in rural areas or developing countries. I’ve worked\\nwith several companies and organizations that have strict no-internet policies, which\\nmeans that whichever applications we wanted to sell them must not rely on internet\\nconnections.\\n212 | Chapter 7: Model Deployment and Prediction Service', '40 “Nearly 80% of Companies Experienced a Cloud Data Breach in Past 18 Months, ” Security, June 5, 2020,\\nhttps://oreil.ly/gA1am.\\n41 See slide #53, CS 329S’s Lecture 8: Deployment - Prediction Service, 2022, https://oreil.ly/cXTou.\\n42 “Internet of Things (IoT) and Non-IoT Active Device Connections Worldwide from 2010 to 2025, ” Statista,\\nhttps://oreil.ly/BChLN.\\nSecond, when your models are already on consumers’ devices, you can worry less\\nabout network latency. Requiring data transfer over the network (sending data to\\nthe model on the cloud to make predictions then sending predictions back to the\\nusers) might make some use cases impossible. In many cases, network latency is a\\nbigger bottleneck than inference latency. For example, you might be able to reduce\\nthe inference latency of ResNet-50 from 30 ms to 20 ms, but the network latency can\\ngo up to seconds, depending on where you are and what services you’re trying to use.\\nPutting your models on the edge is also appealing when handling sensitive user\\ndata. ML on the cloud means that your systems might have to send user data over\\nnetworks, making it susceptible to being intercepted. Cloud computing also often\\nmeans storing data of many users in the same place, which means a breach can affect\\nmany people. “Nearly 80% of companies experienced a cloud data breach in [the] past\\n18 months, ” according to Security magazine.40\\nEdge computing makes it easier to comply with regulations, like GDPR, about how\\nuser data can be transferred or stored. While edge computing might reduce privacy\\nconcerns, it doesn’t eliminate them altogether. In some cases, edge computing might\\nmake it easier for attackers to steal user data, such as they can just take the device\\nwith them.\\nTo move computation to the edge, the edge devices have to be powerful enough to\\nhandle the computation, have enough memory to store ML models and load them\\ninto memory, as well as have enough battery or be connected to an energy source to\\npower the application for a reasonable amount of time. Running a full-sized BERT on\\nyour phone, if your phone is capable of running BERT, is a very quick way to kill its\\nbattery.\\nBecause of the many benefits that edge computing has over cloud computing, com‐\\npanies are in a race to develop edge devices optimized for different ML use cases.\\nEstablished companies including Google, Apple, and Tesla have all announced their\\nplans to make their own chips. Meanwhile, ML hardware startups have raised billions', 'panies are in a race to develop edge devices optimized for different ML use cases.\\nEstablished companies including Google, Apple, and Tesla have all announced their\\nplans to make their own chips. Meanwhile, ML hardware startups have raised billions\\nof dollars to develop better AI chips.41 It’s projected that by 2025 the number of active\\nedge devices worldwide will be over 30 billion.42\\nWith so many new offerings for hardware to run ML models on, one question arises:\\nhow do we make our model run on arbitrary hardware efficiently? In the following\\nsection, we’ll discuss how to compile and optimize a model to run it on a certain\\nML on the Cloud and on the Edge | 213', '43 Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Y an, Meghan Cowan, Haichen Shen, et\\nal., “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning, ” arXiv, February 12, 2018,\\nhttps://oreil.ly/vGnkW.\\nhardware backend. In the process, we’ll introduce important concepts that you might\\nencounter when handling models on the edge, including intermediate representations\\n(IRs) and compilers.\\nCompiling and Optimizing Models for Edge Devices\\nFor a model built with a certain framework, such as TensorFlow or PyTorch, to\\nrun on a hardware backend, that framework has to be supported by the hardware\\nvendor. For example, even though TPUs were released publicly in February 2018, it\\nwasn’t until September 2020 that PyTorch was supported on TPUs. Before then, if you\\nwanted to use a TPU, you’ d have to use a framework that TPUs supported.\\nProviding support for a framework on a hardware backend is time-consuming and\\nengineering-intensive. Mapping from ML workloads to a hardware backend requires\\nunderstanding and taking advantage of that hardware’s design, and different hard‐\\nware backends have different memory layouts and compute primitives, as shown in\\nFigure 7-11.\\nFigure 7-11. Different compute primitives and memory layouts for CPU, GPU, and TPU.\\nSource: Adapted from an image by Chen et al.43\\n214 | Chapter 7: Model Deployment and Prediction Service', '44 Nowadays, many CPUs have vector instructions and some GPUs have tensor cores, which are\\ntwo-dimensional.\\nFor example, the compute primitive of CPUs used to be a number (scalar) and\\nthe compute primitive of GPUs used to be a one-dimensional vector, whereas the\\ncompute primitive of TPUs is a two-dimensional vector (tensor). 44 Performing a\\nconvolution operator will be very different with one-dimensional vectors compared\\nto two-dimensional vectors. Similarly, you’ d need to take into account different L1,\\nL2, and L3 layouts and buffer sizes to use them efficiently.\\nBecause of this challenge, framework developers tend to focus on providing support\\nto only a handful of server-class hardware, and hardware vendors tend to offer their\\nown kernel libraries for a narrow range of frameworks. Deploying ML models to new\\nhardware requires significant manual effort.\\nInstead of targeting new compilers and libraries for every new hardware backend,\\nwhat if we create a middleman to bridge frameworks and platforms? Framework\\ndevelopers will no longer have to support every type of hardware; they will only need\\nto translate their framework code into this middleman. Hardware vendors can then\\nsupport one middleman instead of multiple frameworks.\\nThis type of “middleman” is called an intermediate representation (IR). IRs lie at the\\ncore of how compilers work. From the original code for a model, compilers generate\\na series of high- and low-level IRs before generating the code native to a hardware\\nbackend so that it can run on that hardware backend, as shown in Figure 7-12.\\nFigure 7-12. A series of high- and low-level IRs between the original model code to\\nmachine code that can run on a given hardware backend\\nML on the Cloud and on the Edge | 215', '45 Shoumik Palkar, James Thomas, Deepak Narayanan, Pratiksha Thaker, Rahul Palamuttam, Parimajan Negi,\\nAnil Shanbhag, et al., “Evaluating End-to-End Optimization for Data Analytics Applications in Weld, ” Pro‐\\nceedings of the VLDB Endowment 11, no. 9 (2018): 1002–15, https://oreil.ly/ErUIo.\\nThis process is also called lowering, as in you “lower” your high-level framework code\\ninto low-level hardware-native code. It’s not translating because there’s no one-to-one\\nmapping between them.\\nHigh-level IRs are usually computation graphs of your ML models. A computation\\ngraph is a graph that describes the order in which your computation is executed.\\nReaders interested can read about computation graphs in PyTorch and TensorFlow.\\nModel optimization\\nAfter you’ve “lowered” your code to run your models into the hardware of your\\nchoice, an issue you might run into is performance. The generated machine code\\nmight be able to run on a hardware backend, but it might not be able to do so\\nefficiently. The generated code may not take advantage of data locality and hardware\\ncaches, or it may not leverage advanced features such as vector or parallel operations\\nthat could speed code up.\\nA typical ML workflow consists of many frameworks and libraries. For example,\\nyou might use pandas/dask/ray to extract features from your data. Y ou might use\\nNumPy to perform vectorization. Y ou might use a pretrained model like Hugging\\nFace’s Transformers to generate features, then make predictions using an ensemble of\\nmodels built with various frameworks like sklearn, TensorFlow, or LightGBM.\\nEven though individual functions in these frameworks might be optimized, there’s\\nlittle to no optimization across frameworks. A naive way of moving data across these\\nfunctions for computation can cause an order of magnitude slowdown in the whole\\nworkflow. A study by researchers at Stanford DAWN lab found that typical ML\\nworkloads using NumPy, pandas, and TensorFlow run 23 times slower in one thread\\ncompared to hand-optimized code.45\\nIn many companies, what usually happens is that data scientists and ML engineers\\ndevelop models that seem to be working fine in development. However, when these\\nmodels are deployed, they turn out to be too slow, so their companies hire optimiza‐\\ntion engineers to optimize their models for the hardware their models run on. An\\nexample of a job description for optimization engineers at Mythic follows:\\n216 | Chapter 7: Model Deployment and Prediction Service', 'This vision comes together in the AI Engineering team, where our expertise is used to\\ndevelop AI algorithms and models that are optimized for our hardware, as well as to\\nprovide guidance to Mythic’s hardware and compiler teams.\\nThe AI Engineering team significantly impacts Mythic by:\\n• Developing quantization and robustness AI retraining tools\\n• Investigating new features for our compiler that leverage the adaptability of neural\\nnetworks\\n• Developing new neural networks that are optimized for our hardware products\\n• Interfacing with internal and external customers to meet their development needs\\nOptimization engineers are hard to come by and expensive to hire because they\\nneed to have expertise in both ML and hardware architectures. Optimizing compilers\\n(compilers that also optimize your code) are an alternative solution, as they can\\nautomate the process of optimizing models. In the process of lowering ML model\\ncode into machine code, compilers can look at the computation graph of your ML\\nmodel and the operators it consists of—convolution, loops, cross-entropy—and find\\na way to speed it up.\\nThere are two ways to optimize your ML models: locally and globally. Locally is when\\nyou optimize an operator or a set of operators of your model. Globally is when you\\noptimize the entire computation graph end to end.\\nThere are standard local optimization techniques that are known to speed up your\\nmodel, most of them making things run in parallel or reducing memory access on\\nchips. Here are four of the common techniques:\\nVectorization\\nGiven a loop or a nested loop, instead of executing it one item at a time, execute\\nmultiple elements contiguous in memory at the same time to reduce latency\\ncaused by data I/O.\\nParallelization\\nGiven an input array (or n-dimensional array), divide it into different, independ‐\\nent work chunks, and do the operation on each chunk individually.\\nML on the Cloud and on the Edge | 217', '46 For a helpful visualization of loop tiling, see slide 33 from Colfax Research’s presentation “ Access to Caches\\nand Memory”, session 10 of their Programming and Optimization for Intel Architecture: Hands-on Work‐\\nshop series. The entire series is available at https://oreil.ly/hT1g4.\\n47 Matthias Boehm, “ Architecture of ML Systems 04 Operator Fusion and Runtime Adaptation, ” Graz University\\nof Technology, April 5, 2019, https://oreil.ly/py43J.\\nLoop tiling46\\nChange the data accessing order in a loop to leverage hardware’s memory layout\\nand cache. This kind of optimization is hardware dependent. A good access\\npattern on CPUs is not a good access pattern on GPUs.\\nOperator fusion\\nFuse multiple operators into one to avoid redundant memory access. For exam‐\\nple, two operations on the same array require two loops over that array. In a\\nfused case, it’s just one loop. Figure 7-13 shows an example of operator fusion.\\nFigure 7-13. An example of an operator fusion. Source: Adapted from an image by\\nMatthias Boehm47\\nTo obtain a much bigger speedup, you’ d need to leverage higher-level structures\\nof your computation graph. For example, a convolution neural network with the\\ncomputation graph can be fused vertically or horizontally to reduce memory access\\nand speed up the model, as shown in Figure 7-14.\\n218 | Chapter 7: Model Deployment and Prediction Service', '48 Shashank Prasanna, Prethvi Kashinkunti, and Fausto Milletari, “TensorRT 3: Faster TensorFlow Inference and\\nVolta Support, ” NVIDIA Developer, December 4, 2017, https://oreil.ly/d9h98. CBR stands for “convolution,\\nbias, and ReLU. ”\\nFigure 7-14. Vertical and horizontal fusion of the computation graph of a convolution\\nneural network. Source: Adapted from an image by TensorRT team48\\nML on the Cloud and on the Edge | 219', '49 This is also why you shouldn’t read too much into benchmarking results, such as MLPerf ’s results. A popular\\nmodel running really fast on a type of hardware doesn’t mean an arbitrary model will run really fast on that\\nhardware. It might just be that this model is over-optimized.\\nUsing ML to optimize ML models\\nAs hinted by the previous section with the vertical and horizontal fusion for a convo‐\\nlutional neural network, there are many possible ways to execute a given computation\\ngraph. For example, given three operators A, B, and C, you can fuse A with B, fuse B\\nwith C, or fuse A, B, and C all together.\\nTraditionally, framework and hardware vendors hire optimization engineers who,\\nbased on their experience, come up with heuristics on how to best execute the\\ncomputation graph of a model. For example, NVIDIA might have an engineer or a\\nteam of engineers who focus exclusively on how to make ResNet-50 run really fast on\\ntheir DGX A100 server.49\\nThere are a couple of drawbacks to hand-designed heuristics. First, they’re nonopti‐\\nmal. There’s no guarantee that the heuristics an engineer comes up with are the best\\npossible solution. Second, they are nonadaptive. Repeating the process on a new\\nframework or a new hardware architecture requires an enormous amount of effort.\\nThis is complicated by the fact that model optimization is dependent on the operators\\nits computation graph consists of. Optimizing a convolution neural network is differ‐\\nent from optimizing a recurrent neural network, which is different from optimizing\\na transformer. Hardware vendors like NVIDIA and Google focus on optimizing\\npopular models like ResNet-50 and BERT for their hardware. But what if you, as an\\nML researcher, come up with a new model architecture? Y ou might need to optimize\\nit yourself to show that it’s fast first before it’s adopted and optimized by hardware\\nvendors.\\nIf you don’t have ideas for good heuristics, one possible solution might be to try\\nall possible ways to execute a computation graph, record the time they need to run,\\nthen pick the best one. However, given a combinatorial number of possible paths,\\nexploring them all would be intractable. Luckily, approximating the solutions to\\nintractable problems is what ML is good at. What if we use ML to narrow down the\\nsearch space so we don’t have to explore that many paths, and predict how long a\\npath will take so that we don’t have to wait for the entire computation graph to finish\\nexecuting?', 'intractable problems is what ML is good at. What if we use ML to narrow down the\\nsearch space so we don’t have to explore that many paths, and predict how long a\\npath will take so that we don’t have to wait for the entire computation graph to finish\\nexecuting?\\nTo estimate how much time a path through a computation graph will take to run\\nturns out to be difficult, as it requires making a lot of assumptions about that graph.\\nIt’s much easier to focus on a small part of the graph.\\n220 | Chapter 7: Model Deployment and Prediction Service', 'If you use PyTorch on GPUs, you might have seen torch.backends.cudnn.bench\\nmark=True. When this is set to True, cuDNN autotune will be enabled. cuDNN auto‐\\ntune searches over a predetermined set of options to execute a convolution operator\\nand then chooses the fastest way. cuDNN autotune, despite its effectiveness, only\\nworks for convolution operators. A much more general solution is autoTVM, which\\nis part of the open source compiler stack TVM. autoTVM works with subgraphs\\ninstead of just an operator, so the search spaces it works with are much more\\ncomplex. The way autoTVM works is quite complicated, but in simple terms:\\n1. It first breaks your computation graph into subgraphs.1.\\n2. It predicts how big each subgraph is.2.\\n3. It allocates time to search for the best possible path for each subgraph.3.\\n4. It stitches the best possible way to run each subgraph together to execute the4.\\nentire graph.\\nautoTVM measures the actual time it takes to run each path it goes down, which\\ngives it ground truth data to train a cost model to predict how long a future path\\nwill take. The pro of this approach is that because the model is trained using the data\\ngenerated during runtime, it can adapt to any type of hardware it runs on. The con is\\nthat it takes more time for the cost model to start improving. Figure 7-15 shows the\\nperformance gain that autoTVM gave compared to cuDNN for the model ResNet-50\\non NVIDIA TITAN X.\\nWhile the results of ML-powered compilers are impressive, they come with a catch:\\nthey can be slow. Y ou go through all the possible paths and find the most optimized\\nones. This process can take hours, even days for complex ML models. However, it’s\\na one-time operation, and the results of your optimization search can be cached and\\nused to both optimize existing models and provide a starting point for future tuning\\nsessions. Y ou optimize your model once for one hardware backend then run it on\\nmultiple devices of that same hardware type. This sort of optimization is ideal when\\nyou have a model ready for production and target hardware to run inference on.\\nML on the Cloud and on the Edge | 221', '50 Chen et al., “TVM: An Automated End-to-End Optimizing Compiler for Deep Learning. ”\\nFigure 7-15. Speedup achieved by autoTVM over cuDNN for ResNet-50 on NVIDIA\\nTITAN X. It takes ~70 trials for autoTVM to outperform cuDNN. Source: Chen et al.50\\nML in Browsers\\nWe’ve been talking about how compilers can help us generate machine-native code\\nrun models on certain hardware backends. It is, however, possible to generate code\\nthat can run on just any hardware backends by running that code in browsers. If\\nyou can run your model in a browser, you can run your model on any device that\\nsupports browsers: MacBooks, Chromebooks, iPhones, Android phones, and more.\\nY ou wouldn’t need to care what chips those devices use. If Apple decides to switch\\nfrom Intel chips to ARM chips, it’s not your problem.\\nWhen talking about browsers, many people think of JavaScript. There are tools that\\ncan help you compile your models into JavaScript, such as TensorFlow.js, Synaptic,\\nand brain.js. However, JavaScript is slow, and its capacity as a programming language\\nis limited for complex logics such as extracting features from data.\\n222 | Chapter 7: Model Deployment and Prediction Service', '51 Wasmer, https://oreil.ly/dTRxr; Awesome Wasm, https://oreil.ly/hlIFb.\\n52 Can I Use _____?, https://oreil.ly/slI05.\\n53 Abhinav Jangda, Bobby Powers, Emery D. Berger, and Arjun Guha, “Not So Fast: Analyzing the Performance\\nof WebAssembly vs. Native Code, ” USENIX, https://oreil.ly/uVzrX.\\nA more promising approach is WebAssembly (W ASM). W ASM is an open standard\\nthat allows you to run executable programs in browsers. After you’ve built your\\nmodels in scikit-learn, PyTorch, TensorFlow, or whatever frameworks you’ve used,\\ninstead of compiling your models to run on specific hardware, you can compile your\\nmodel to W ASM. Y ou get back an executable file that you can just use with JavaScript.\\nW ASM is one of the most exciting technological trends I’ve seen in the last couple of\\nyears. It’s performant, easy to use, and has an ecosystem that is growing like wildfire.51\\nAs of September 2021, it’s supported by 93% of devices worldwide.52\\nThe main drawback of W ASM is that because W ASM runs in browsers, it’s slow.\\nEven though W ASM is already much faster than JavaScript, it’s still slow compared to\\nrunning code natively on devices (such as iOS or Android apps). A study by Jangda et\\nal. showed that applications compiled to W ASM run slower than native applications\\nby an average of 45% (on Firefox) to 55% (on Chrome).53\\nSummary\\nCongratulations, you’ve finished possibly one of the most technical chapters in this\\nbook! The chapter is technical because deploying ML models is an engineering\\nchallenge, not an ML challenge.\\nWe’ve discussed different ways to deploy a model, comparing online prediction with\\nbatch prediction, and ML on the edge with ML on the cloud. Each way has its own\\nchallenges. Online prediction makes your model more responsive to users’ changing\\npreferences, but you have to worry about inference latency. Batch prediction is a\\nworkaround for when your models take too long to generate predictions, but it makes\\nyour model less flexible.\\nSimilarly, doing inference on the cloud is easy to set up, but it becomes impractical\\nwith network latency and cloud cost. Doing inference on the edge requires having\\nedge devices with sufficient compute power, memory, and battery.\\nHowever, I believe that most of these challenges are due to the limitations of the\\nhardware that ML models run on. As hardware becomes more powerful and opti‐\\nmized for ML, I believe that ML systems will transition to making online prediction\\non-device, illustrated in Figure 7-16.\\nSummary | 223', 'Figure 7-16. As hardware becomes more powerful, ML models will move to online and\\non the edge\\nI used to think that an ML project is done after the model is deployed, and I hope\\nthat I’ve made clear in this chapter that I was seriously mistaken. Moving the model\\nfrom the development environment to the production environment creates a whole\\nnew host of problems. The first is how to keep that model in production. In the\\nnext chapter, we’ll discuss how our models might fail in production, and how to\\ncontinually monitor models to detect issues and address them as fast as possible.\\n224 | Chapter 7: Model Deployment and Prediction Service', '1 This seems to be a fairly common pattern for inventory prediction. Eugene Y an wrote about a similar story to\\nillustrate the problem of degenerate feedback loops in his article “6 Little-Known Challenges After Deploying\\nMachine Learning” (2021).\\nCHAPTER 8\\nData Distribution Shifts and Monitoring\\nLet’s start the chapter with a story I was told by an executive that many readers might\\nbe able to relate to. About two years ago, his company hired a consulting firm to\\ndevelop an ML model to help them predict how many of each grocery item they’ d\\nneed next week, so they could restock the items accordingly. The consulting firm took\\nsix months to develop the model. When the consulting firm handed the model over,\\nhis company deployed it and was very happy with its performance. They could finally\\nboast to their investors that they were an AI-powered company.\\nHowever, a year later, their numbers went down. The demand for some items was\\nconsistently being overestimated, which caused the extra items to expire. At the same\\ntime, the demand for some items was consistently being underestimated, leading to\\nlost sales.1 Initially, his inventory team manually changed the model’s predictions to\\ncorrect the patterns they noticed, but eventually, the model’s predictions had become\\nso bad that they could no longer use it. They had three options: pay the same\\nconsulting firm an obscene amount of money to update the model, pay another\\nconsulting firm even more money because this firm would need time to get up to\\nspeed, or hire an in-house team to maintain the model onward.\\n225', 'His company learned the hard way an important lesson that the rest of the industry\\nis also discovering: deploying a model isn’t the end of the process. A model’s perfor‐\\nmance degrades over time in production. Once a model has been deployed, we still\\nhave to continually monitor its performance to detect issues as well as deploy updates\\nto fix these issues.\\nIn this chapter and the next, we’ll cover the necessary topics to help you keep a\\nmodel in production. We’ll start by covering reasons why ML models that perform\\ngreat during development fail in production. Then, we’ll take a deep dive into one\\nespecially prevalent and thorny issue that affects almost all ML models in production:\\ndata distribution shifts. This occurs when the data distribution in production differs\\nand diverges from the data distribution the model was exposed to during training.\\nWe’ll continue with how to monitor for distribution shifts. In the next chapter, we’ll\\ncover how to continually update your models in production to adapt to shifts in data\\ndistributions.\\nCauses of ML System Failures\\nBefore we identify the cause of ML system failures, let’s briefly discuss what an ML\\nsystem failure is. A failure happens when one or more expectations of the system is\\nviolated. In traditional software, we mostly care about a system’s operational expecta‐\\ntions: whether the system executes its logic within the expected operational metrics,\\ne.g., latency and throughput.\\nFor an ML system, we care about both its operational metrics and its ML perfor‐\\nmance metrics. For example, consider an English-French machine translation system.\\nIts operational expectation might be that, given an English sentence, the system\\nreturns a French translation within a one-second latency. Its ML performance expect‐\\nation is that the returned translation is an accurate translation of the original English\\nsentence 99% of the time.\\nIf you enter an English sentence into the system and don’t get back a translation, the\\nfirst expectation is violated, so this is a system failure.\\n226 | Chapter 8: Data Distribution Shifts and Monitoring', '2 This is one of the reasons why many companies are hesitant to use products by startups, and why many\\ncompanies prefer to use open source software. When a product you use is no longer maintained by its\\ncreators, if that product is open source, at least you’ll be able to access the codebase and maintain it yourself.\\nIf you get back a translation that isn’t correct, it’s not necessarily a system failure\\nbecause the accuracy expectation allows some margin of error. However, if you keep\\nentering different English sentences into the system and keep getting back wrong\\ntranslations, the second expectation is violated, which makes it a system failure.\\nOperational expectation violations are easier to detect, as they’re usually accompa‐\\nnied by an operational breakage such as a timeout, a 404 error on a webpage, an\\nout-of-memory error, or a segmentation fault. However, ML performance expectation\\nviolations are harder to detect as doing so requires measuring and monitoring the\\nperformance of ML models in production. In the preceding example of the English-\\nFrench machine translation system, detecting whether the returned translations are\\ncorrect 99% of the time is difficult if we don’t know what the correct translations\\nare supposed to be. There are countless examples of Google Translate’s painfully\\nwrong translations being used by users because they aren’t aware that these are wrong\\ntranslations. For this reason, we say that ML systems often fail silently.\\nTo effectively detect and fix ML system failures in production, it’s useful to under‐\\nstand why a model, after proving to work well during development, would fail in pro‐\\nduction. We’ll examine two types of failures: software system failures and ML-specific\\nfailures.\\nSoftware System Failures\\nSoftware system failures are failures that would have happened to non-ML systems.\\nHere are some examples of software system failures:\\nDependency failure\\nA software package or a codebase that your system depends on breaks, which\\nleads your system to break. This failure mode is common when the dependency\\nis maintained by a third party, and especially common if the third party that\\nmaintains the dependency no longer exists.2\\nDeployment failure\\nFailures caused by deployment errors, such as when you accidentally deploy the\\nbinaries of an older version of your model instead of the current version, or when\\nyour systems don’t have the right permissions to read or write certain files.\\nCauses of ML System Failures | 227', '3 Cosmic rays can cause your hardware to break down (Wikipedia, s.v. “Soft error, ” https://oreil.ly/4cvNg).\\n4 Daniel Papasian and Todd Underwood, “How ML Breaks: A Decade of Outages for One Large ML Pipeline, ”\\nGoogle, July 17, 2020, video, 19:06, https://oreil.ly/WGabN. A non-ML failure might still be indirectly due\\nto ML. For example, a server can crash for non-ML systems, but because ML systems tend to require more\\ncompute power, it might cause this server to crash more often.\\n5 The peak of my career: Elon Musk agreed with me.\\nHardware failures\\nWhen the hardware that you use to deploy your model, such as CPUs or GPUs,\\ndoesn’t behave the way it should. For example, the CPUs you use might overheat\\nand break down.3\\nDowntime or crashing\\nIf a component of your system runs from a server somewhere, such as AWS or a\\nhosted service, and that server is down, your system will also be down.\\nJust because some failures are not specific to ML doesn’t mean they’re not important\\nfor ML engineers to understand. In 2020, Daniel Papasian and Todd Underwood,\\ntwo ML engineers at Google, looked at 96 cases where a large ML pipeline at Google\\nbroke. They reviewed data from over the previous 15 years to determine the causes\\nand found out that 60 out of these 96 failures happened due to causes not directly\\nrelated to ML. 4 Most of the issues are related to distributed systems, e.g., where the\\nworkflow scheduler or orchestrator makes a mistake, or related to the data pipeline,\\ne.g., where data from multiple sources is joined incorrectly or the wrong data struc‐\\ntures are being used.\\nAddressing software system failures requires not ML skills, but traditional software\\nengineering skills, and addressing them is beyond the scope of this book. Because of\\nthe importance of traditional software engineering skills in deploying ML systems,\\nML engineering is mostly engineering, not ML. 5 For readers interested in learning\\nhow to make ML systems reliable from the software engineering perspective, I highly\\nrecommend the book Reliable Machine Learning , published by O’Reilly with Todd\\nUnderwood as one of the authors.\\nA reason for the prevalence of software system failures is that because ML adoption\\nin the industry is still nascent, tooling around ML production is limited and best\\npractices are not yet well developed or standardized. However, as toolings and best\\npractices for ML production mature, there are reasons to believe that the proportion', 'in the industry is still nascent, tooling around ML production is limited and best\\npractices are not yet well developed or standardized. However, as toolings and best\\npractices for ML production mature, there are reasons to believe that the proportion\\nof software system failures will decrease and the proportion of ML-specific failures\\nwill increase.\\n228 | Chapter 8: Data Distribution Shifts and Monitoring', '6 Back when in-person academic conferences were still a thing, I often heard researchers arguing about whose\\nmodels can generalize better. “My model generalizes better than your model” is the ultimate flex.\\n7 Masashi Sugiyama and Motoaki Kawanabe, Machine Learning in Non-stationary Environments: Introduction to\\nCovariate Shift Adaptation (Cambridge, MA: MIT Press, 2012).\\nML-Specific Failures\\nML-specific failures are failures specific to ML systems. Examples include data collec‐\\ntion and processing problems, poor hyperparameters, changes in the training pipe‐\\nline not correctly replicated in the inference pipeline and vice versa, data distribution\\nshifts that cause a model’s performance to deteriorate over time, edge cases, and\\ndegenerate feedback loops.\\nIn this chapter, we’ll focus on addressing ML-specific failures. Even though they\\naccount for a small portion of failures, they can be more dangerous than non-ML\\nfailures as they’re hard to detect and fix, and they can prevent ML systems from\\nbeing used altogether. We’ve covered data problems in great detail in Chapter 4 ,\\nhyperparameter tuning in Chapter 6, and the danger of having two separate pipelines\\nfor training and inference in Chapter 7. In this chapter, we’ll discuss three new but\\nvery common problems that arise after a model has been deployed: production data\\ndiffering from training data, edge cases, and degenerate feedback loops.\\nProduction data differing from training data\\nWhen we say that an ML model learns from the training data, it means that the\\nmodel learns the underlying distribution of the training data with the goal of leverag‐\\ning this learned distribution to generate accurate predictions for unseen data—data\\nthat it didn’t see during training. We’ll go into what this means mathematically in the\\nsection “Data Distribution Shifts”  on page 237. When the model is able to generate\\naccurate predictions for unseen data, we say that this model “generalizes to unseen\\ndata. ”6 The test data that we use to evaluate a model during development is supposed\\nto represent unseen data, and the model’s performance on the test data is supposed to\\ngive us an idea of how well the model will generalize.\\nOne of the first things I learned in ML courses is that it’s essential for the training data\\nand the unseen data to come from a similar distribution. The assumption is that the\\nunseen data comes from a stationary distribution that is the same as the training data', 'One of the first things I learned in ML courses is that it’s essential for the training data\\nand the unseen data to come from a similar distribution. The assumption is that the\\nunseen data comes from a stationary distribution that is the same as the training data\\ndistribution. If the unseen data comes from a different distribution, the model might\\nnot generalize well.7\\nCauses of ML System Failures | 229', '8 John Mcquaid, “Limits to Growth: Can AI’s Voracious Appetite for Data Be Tamed?” Undark, October 18,\\n2021, https://oreil.ly/LSjVD.\\n9 The chief technology officer (CTO) of a monitoring service company told me that, in his estimate, 80% of the\\ndrifts captured by his service are caused by human errors.\\nThis assumption is incorrect in most cases for two reasons. First, the underlying\\ndistribution of the real-world data is unlikely to be the same as the underlying distri‐\\nbution of the training data. Curating a training dataset that can accurately represent\\nthe data that a model will encounter in production turns out to be very difficult. 8\\nReal-world data is multifaceted and, in many cases, virtually infinite, whereas training\\ndata is finite and constrained by the time, compute, and human resources available\\nduring the dataset creation and processing. There are many different selection and\\nsampling biases, as discussed in Chapter 4, that can happen and make real-world data\\ndiverge from training data. The divergence can be something as minor as real-world\\ndata using a different type of encoding of emojis. This type of divergence leads to a\\ncommon failure mode known as the train-serving skew : a model that does great in\\ndevelopment but performs poorly when deployed.\\nSecond, the real world isn’t stationary. Things change. Data distributions shift. In\\n2019, when people searched for Wuhan, they likely wanted to get travel information,\\nbut since COVID-19, when people search for Wuhan, they likely want to know\\nabout the place where COVID-19 originated. Another common failure mode is that a\\nmodel does great when first deployed, but its performance degrades over time as the\\ndata distribution changes. This failure mode needs to be continually monitored and\\ndetected for as long as a model remains in production.\\nWhen I use COVID-19 as an example that causes data shifts, some people have the\\nimpression that data shifts only happen because of unusual events, which implies\\nthey don’t happen often. Data shifts happen all the time, suddenly, gradually, or\\nseasonally. They can happen suddenly because of a specific event, such as when your\\nexisting competitors change their pricing policies and you have to update your price\\npredictions in response, or when you launch your product in a new region, or when a\\ncelebrity mentions your product, which causes a surge in new users, and so on. They\\ncan happen gradually because social norms, cultures, languages, trends, industries,', 'predictions in response, or when you launch your product in a new region, or when a\\ncelebrity mentions your product, which causes a surge in new users, and so on. They\\ncan happen gradually because social norms, cultures, languages, trends, industries,\\netc. just change over time. They can also happen due to seasonal variations, such as\\npeople might be more likely to request rideshares in the winter when it’s cold and\\nsnowy than in the spring.\\nDue to the complexity of ML systems and the poor practices in deploying them,\\na large percentage of what might look like data shifts on monitoring dashboards\\nare caused by internal errors, 9 such as bugs in the data pipeline, missing values\\nincorrectly inputted, inconsistencies between the features extracted during training\\nand inference, features standardized using statistics from the wrong subset of data,\\n230 | Chapter 8: Data Distribution Shifts and Monitoring', '10 This means the self-driving car is a bit safer than an average human driver. As of 2019, the ratio of\\ntraffic-related fatalities per 100,000 licensed drivers was 15.8, or 0.0158% (“Fatality Rate per 100,000 Licensed\\nDrivers in the U.S. from 1990 to 2019, ” Statista, 2021, https://oreil.ly/w3wYh).\\n11 Rodney Brooks, “Edge Cases for Self Driving Cars, ” Robots, AI, and Other Stuff, June 17, 2017, https://\\noreil.ly/Nyp4F; Lance Eliot, “Whether Those Endless Edge or Corner Cases Are the Long-Tail Doom for AI\\nSelf-Driving Cars, ” Forbes, July 13, 2021, https://oreil.ly/L2Sbp; Kevin McAllister, “Self-Driving Cars Will Be\\nShaped by Simulated, Location Data, ” Protocol, March 25, 2021, https://oreil.ly/tu8hs.\\n12 e-discovery, or electronic discovery, refers to discovery in legal proceedings, such as litigation, government\\ninvestigations, or Freedom of Information Act requests, where the information sought is in electronic format.\\nwrong model version, or bugs in the app interface that force users to change their\\nbehaviors.\\nSince this is an error mode that affects almost all ML models, we’ll cover this in detail\\nin the section “Data Distribution Shifts” on page 237.\\nEdge cases\\nImagine there existed a self-driving car that can drive you safely 99.99% of the time,\\nbut the other 0.01% of the time, it might get into a catastrophic accident that can\\nleave you permanently injured or even dead.10 Would you use that car?\\nIf you’re tempted to say no, you’re not alone. An ML model that performs well on\\nmost cases but fails on a small number of cases might not be usable if these failures\\ncause catastrophic consequences. For this reason, major self-driving car companies\\nare focusing on making their systems work on edge cases.11\\nEdge cases are the data samples so extreme that they cause the model to make\\ncatastrophic mistakes. Even though edge cases generally refer to data samples drawn\\nfrom the same distribution, if there is a sudden increase in the number of data\\nsamples in which your model doesn’t perform well, it could be an indication that the\\nunderlying data distribution has shifted.\\nAutonomous vehicles are often used to illustrate how edge cases can prevent an\\nML system from being deployed. But this is also true for any safety-critical applica‐\\ntion such as medical diagnosis, traffic control, e-discovery, 12 etc. It can also be true\\nfor non-safety-critical applications. Imagine a customer service chatbot that gives', 'ML system from being deployed. But this is also true for any safety-critical applica‐\\ntion such as medical diagnosis, traffic control, e-discovery, 12 etc. It can also be true\\nfor non-safety-critical applications. Imagine a customer service chatbot that gives\\nreasonable responses to most of the requests, but sometimes, it spits out outrageously\\nracist or sexist content. This chatbot will be a brand risk for any company that wants\\nto use it, thus rendering it unusable.\\nCauses of ML System Failures | 231', 'Edge Cases and Outliers\\nY ou might wonder about the differences between an outlier and an edge case. The\\ndefinition of what makes an edge case varies by discipline. In ML, because of its\\nrecent adoption in production, edge cases are still being discovered, which makes\\ntheir definition contentious.\\nIn this book, outliers refer to data: an example that differs significantly from other\\nexamples. Edge cases refer to performance: an example where a model performs\\nsignificantly worse than other examples. An outlier can cause a model to perform\\nunusually poorly, which makes it an edge case. However, not all outliers are edge\\ncases. For example, a person jaywalking on a highway is an outlier, but it’s not an edge\\ncase if your self-driving car can accurately detect that person and decide on a motion\\nresponse appropriately.\\nDuring model development, outliers can negatively affect your model’s performance,\\nas shown in Figure 8-1 . In many cases, it might be beneficial to remove outliers\\nas it helps your model to learn better decision boundaries and generalize better to\\nunseen data. However, during inference, you don’t usually have the option to remove\\nor ignore the queries that differ significantly from other queries. Y ou can choose\\nto transform it—for example, when you enter “mechin learnin” into Google Search,\\nGoogle might ask if you mean “machine learning. ” But most likely you’ll want to\\ndevelop a model so that it can perform well even on unexpected inputs.\\nFigure 8-1. The image on the left shows the decision boundary when there’s no outlier.\\nThe image on the right shows the decision boundary when there’s one outlier, which is\\nvery different from the decision boundary in the first case, and probably less accurate.\\n232 | Chapter 8: Data Distribution Shifts and Monitoring', '13 Ray Jiang, Silvia Chiappa, Tor Lattimore, András György, and Pushmeet Kohli, “Degenerate Feedback Loops\\nin Recommender Systems, ” arXiv, February 27, 2019, https://oreil.ly/b9G7o.\\n14 This is related to “survivorship bias. ”\\nDegenerate feedback loops\\nIn the section “Natural Labels” on page 91, we discussed a feedback loop as the time\\nit takes from when a prediction is shown until the time feedback on the prediction is\\nprovided. The feedback can be used to extract natural labels to evaluate the model’s\\nperformance and train the next iteration of the model.\\nA degenerate feedback loop can happen when the predictions themselves influence the\\nfeedback, which, in turn, influences the next iteration of the model. More formally, a\\ndegenerate feedback loop is created when a system’s outputs are used to generate the\\nsystem’s future inputs, which, in turn, influence the system’s future outputs. In ML,\\na system’s predictions can influence how users interact with the system, and because\\nusers’ interactions with the system are sometimes used as training data to the same\\nsystem, degenerate feedback loops can occur and cause unintended consequences.\\nDegenerate feedback loops are especially common in tasks with natural labels from\\nusers, such as recommender systems and ads click-through-rate prediction.\\nTo make this concrete, imagine you build a system to recommend to users songs\\nthat they might like. The songs that are ranked high by the system are shown first\\nto users. Because they are shown first, users click on them more, which makes the\\nsystem more confident that these recommendations are good. In the beginning, the\\nrankings of two songs, A and B, might be only marginally different, but because A\\nwas originally ranked a bit higher, it showed up higher in the recommendation list,\\nmaking users click on A more, which made the system rank A even higher. After a\\nwhile, A ’s ranking became much higher than B’s.13 Degenerate feedback loops are one\\nreason why popular movies, books, or songs keep getting more popular, which makes\\nit hard for new items to break into popular lists. This type of scenario is incredibly\\ncommon in production, and it’s heavily researched. It goes by many different names,\\nincluding “exposure bias, ” “popularity bias, ” “filter bubbles, ” and sometimes “echo\\nchambers. ”\\nHere’s another example to drive the danger of degenerative feedback loops home.\\nImagine building a resume-screening model to predict whether someone with a', 'including “exposure bias, ” “popularity bias, ” “filter bubbles, ” and sometimes “echo\\nchambers. ”\\nHere’s another example to drive the danger of degenerative feedback loops home.\\nImagine building a resume-screening model to predict whether someone with a\\ncertain resume is qualified for the job. The model finds that feature X accurately\\npredicts whether someone is qualified, so it recommends resumes with feature X. Y ou\\ncan replace X with features like “went to Stanford, ” “worked at Google, ” or “identifies\\nas male. ” Recruiters only interview people whose resumes are recommended by the\\nmodel, which means they only interview candidates with feature X, which means\\nthe company only hires candidates with feature X. This, in turn, makes the model\\nput even more weight on feature X. 14 Having visibility into how your model makes\\nCauses of ML System Failures | 233', '15 Erik Brynjolfsson, Yu (Jeffrey) Hu, and Duncan Simester, “Goodbye Pareto Principle, Hello Long Tail:\\nThe Effect of Search Costs on the Concentration of Product Sales, ” Management Science 57, no. 8 (2011):\\n1373–86, https://oreil.ly/tGhHi; Daniel Fleder and Kartik Hosanagar, “Blockbuster Culture’s Next Rise or\\nFall: The Impact of Recommender Systems on Sales Diversity, ” Management Science 55, no. 5 (2009), https://\\noreil.ly/Zwkh8; Himan Abdollahpouri, Robin Burke, and Bamshad Mobasher, “Managing Popularity Bias in\\nRecommender Systems with Personalized Re-ranking, ” arXiv, January 22, 2019, https://oreil.ly/jgYLr.\\n16 Patrick John Chia, Jacopo Tagliabue, Federico Bianchi, Chloe He, and Brian Ko, “Beyond NDCG: Behavioral\\nTesting of Recommender Systems with RecList, ” arXiv, November 18, 2021, https://oreil.ly/7GfHk.\\npredictions—such as measuring the importance of each feature for the model, as\\ndiscussed in Chapter 5—can help detect the bias toward feature X in this case.\\nLeft unattended, degenerate feedback loops can cause your model to perform subop‐\\ntimally at best. At worst, they can perpetuate and magnify biases embedded in data,\\nsuch as biasing against candidates without feature X.\\nDetecting degenerate feedback loops.    If degenerate feedback loops are so bad, how do\\nwe know if a feedback loop in a system is degenerate? When a system is offline,\\ndegenerate feedback loops are difficult to detect. Degenerate loops result from user\\nfeedback, and a system won’t have users until it’s online (i.e., deployed to users).\\nFor the task of recommender systems, it’s possible to detect degenerate feedback\\nloops by measuring the popularity diversity of a system’s outputs even when the\\nsystem is offline. An item’s popularity can be measured based on how many times it\\nhas been interacted with (e.g., seen, liked, bought, etc.) in the past. The popularity\\nof all the items will likely follow a long-tail distribution: a small number of items\\nare interacted with a lot, while most items are rarely interacted with at all. Various\\nmetrics such as aggregate diversity and average coverage of long-tail items  proposed\\nby Brynjolfsson et al.  (2011), Fleder and Hosanagar (2009), and Abdollahpouri et al.\\n(2019) can help you measure the diversity of the outputs of a recommender system. 15\\nLow scores mean that the outputs of your system are homogeneous, which might be\\ncaused by popularity bias.\\nIn 2021, Chia et al. went a step further and proposed the measurement of hit rate', '(2019) can help you measure the diversity of the outputs of a recommender system. 15\\nLow scores mean that the outputs of your system are homogeneous, which might be\\ncaused by popularity bias.\\nIn 2021, Chia et al. went a step further and proposed the measurement of hit rate\\nagainst popularity. They first divided items into buckets based on their popularity—\\ne.g., bucket 1 consists of items that have been interacted with less than 100 times,\\nbucket 2 consists of items that have been interacted with more than 100 times but\\nless than 1,000 times, etc. Then they measured the prediction accuracy of a recom‐\\nmender system for each of these buckets. If a recommender system is much better at\\nrecommending popular items than recommending less popular items, it likely suffers\\nfrom popularity bias. 16 Once your system is in production and you notice that its\\npredictions become more homogeneous over time, it likely suffers from degenerate\\nfeedback loops.\\n234 | Chapter 8: Data Distribution Shifts and Monitoring', '17 Catherine Wang, “Why TikTok Made Its User So Obsessive? The AI Algorithm That Got Y ou Hooked, ”\\nTowards Data Science, June 7, 2020, https://oreil.ly/J7nJ9.\\n18 Gediminas Adomavicius and Y oungOk Kwon, “Improving Aggregate Recommendation Diversity Using\\nRanking-Based Techniques, ” IEEE Transactions on Knowledge and Data Engineering 24, no. 5 (May 2012):\\n896–911, https://oreil.ly/0JjUV.\\n19 Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak, and Thorsten Joachims,\\n“Recommendations as Treatments: Debiasing Learning and Evaluation, ” arXiv, February 17, 2016,\\nhttps://oreil.ly/oDPSK.\\nCorrecting degenerate feedback loops.    Because degenerate feedback loops are a com‐\\nmon problem, there are many proposed methods on how to correct them. In this\\nchapter, we’ll discuss two methods. The first one is to use randomization, and the\\nsecond one is to use positional features.\\nWe’ve discussed that degenerate feedback loops can cause a system’s outputs to be\\nmore homogeneous over time. Introducing randomization in the predictions can\\nreduce their homogeneity. In the case of recommender systems, instead of showing\\nthe users only the items that the system ranks highly for them, we show users random\\nitems and use their feedback to determine the true quality of these items. This is the\\napproach that TikTok follows. Each new video is randomly assigned an initial pool of\\ntraffic (which can be up to hundreds of impressions). This pool of traffic is used to\\nevaluate each video’s unbiased quality to determine whether it should be moved to a\\nbigger pool of traffic or be marked as irrelevant.17\\nRandomization has been shown to improve diversity, but at the cost of user experi‐\\nence.18 Showing our users completely random items might cause users to lose interest\\nin our product. An intelligent exploration strategy, such as those discussed in the\\nsection “Contextual bandits as an exploration strategy” on page 289, can help increase\\nitem diversity with acceptable prediction accuracy loss. Schnabel et al. use a small\\namount of randomization and causal inference techniques to estimate the unbiased\\nvalue of each song.19 They were able to show that this algorithm was able to correct a\\nrecommender system to make recommendations fair to creators.\\nWe’ve also discussed that degenerate feedback loops are caused by users’ feedback\\non predictions, and users’ feedback on a prediction is biased based on where it is', 'recommender system to make recommendations fair to creators.\\nWe’ve also discussed that degenerate feedback loops are caused by users’ feedback\\non predictions, and users’ feedback on a prediction is biased based on where it is\\nshown. Consider the preceding recommender system example, where each time you\\nrecommend five songs to users. Y ou realize that the top recommended song is much\\nmore likely to be clicked on compared to the other four songs. Y ou are unsure\\nwhether your model is exceptionally good at picking the top song, or whether users\\nclick on any song as long as it’s recommended on top.\\nCauses of ML System Failures | 235', 'If the position in which a prediction is shown affects its feedback in any way, you\\nmight want to encode the position information using positional features. Positional\\nfeatures can be numerical (e.g., positions are 1, 2, 3,...) or Boolean (e.g., whether a\\nprediction is shown in the first position or not). Note that “positional features” are\\ndifferent from “positional embeddings” mentioned in Chapter 5.\\nHere is a naive example to show how to use positional features. During training,\\nyou add “whether a song is recommended first” as a feature to your training data, as\\nshown in Table 8-1. This feature allows your model to learn how much being a top\\nrecommendation influences how likely a song is clicked on.\\nTable 8-1. Adding positional features to your training data to mitigate\\ndegenerate feedback loops\\nID Song Genre Year Artist User 1st Position Click\\n1 Shallow Pop 2020 Lady Gaga listenr32 False No\\n2 Good Vibe Funk 2019 Funk Overlord listenr32 False No\\n3 Beat It Rock 1989 Michael Jackson fancypants False No\\n4 In Bloom Rock 1991 Nirvana fancypants True Yes\\n5 Shallow Pop 2020 Lady Gaga listenr32 True Yes\\nDuring inference, you want to predict whether a user will click on a song regardless\\nof where the song is recommended, so you might want to set the 1st Position feature\\nto be False. Then you look at the model’s predictions for various songs for each user\\nand can choose the order in which to show each song.\\nThis is a naive example because doing this alone might not be enough to combat\\ndegenerate feedback loops. A more sophisticated approach would be to use two\\ndifferent models. The first model predicts the probability that the user will see and\\nconsider a recommendation taking into account the position at which that recom‐\\nmendation will be shown. The second model then predicts the probability that the\\nuser will click on the item given that they saw and considered it. The second model\\ndoesn’t concern positions at all.\\n236 | Chapter 8: Data Distribution Shifts and Monitoring', '20 Jeffrey C. Schlimmer and Richard H. Granger, Jr., “Incremental Learning from Noisy Data, ” Machine Learning\\n1 (1986): 317–54, https://oreil.ly/FxFQi.\\nData Distribution Shifts\\nIn the previous section, we discussed common causes for ML system failures. In this\\nsection, we’ll zero in onto one especially sticky cause of failures: data distribution\\nshifts, or data shifts for short. Data distribution shift refers to the phenomenon in\\nsupervised learning when the data a model works with changes over time, which\\ncauses this model’s predictions to become less accurate as time passes. The distri‐\\nbution of the data the model is trained on is called the source distribution. The\\ndistribution of the data the model runs inference on is called the target distribution.\\nEven though discussions around data distribution shift have only become common\\nin recent years with the growing adoption of ML in the industry, data distribution\\nshift in systems that learned from data has been studied as early as in 1986. 20 There’s\\nalso a book on dataset distribution shifts, Dataset Shift in Machine Learning  by\\nQuiñonero-Candela et al., published by MIT Press in 2008.\\nTypes of Data Distribution Shifts\\nWhile data distribution shift is often used interchangeably with concept drift and\\ncovariate shift and occasionally label shift, these are three distinct subtypes of data\\nshift. Note that this discussion on different types of data shifts is math-heavy and\\nmostly useful from a research perspective: to develop efficient algorithms to detect\\nand address data shifts requires understanding the causes of those shifts. In produc‐\\ntion, when encountering a distribution shift, data scientists don’t usually stop to\\nwonder what type of shift it is. They mostly care about what they can do to handle\\nthis shift. If you find this discussion dense, feel free to skip to the section “General\\nData Distribution Shifts” on page 241.\\nTo understand what concept drift, covariate shift, and label shift mean, we first need\\nto define a couple of mathematical notations. Let’s call the inputs to a model X and\\nits outputs Y. We know that in supervised learning, the training data can be viewed\\nas a set of samples from the joint distribution P(X, Y), and then ML usually models\\nP(Y|X). This joint distribution P(X, Y) can be decomposed in two ways:\\n• P(X, Y) = P(Y|X)P(X)•\\n• P(X, Y) = P(X|Y)P(Y)•\\nData Distribution Shifts | 237', '21 Y ou might wonder what about the case when P(X|Y) changes but P(Y) remains the same, as in the second\\ndecomposition. I’ve never encountered any research in this setting. I asked a couple of researchers who\\nspecialize in data shifts about it, and they also told me that setting would be too difficult to study.\\n22 Wouter M. Kouw and Marco Loog, “ An Introduction to Domain Adaptation and Transfer Learning, ” arXiv,\\nDecember 31, 2018, https://oreil.ly/VKSVP.\\n23 “Breast Cancer Risk in American Women, ” National Cancer Institute, https://oreil.ly/BFP3U.\\nP(Y|X) denotes the conditional probability of an output given an input—for example,\\nthe probability of an email being spam given the content of the email. P(X) denotes\\nthe probability density of the input. P(Y) denotes the probability density of the\\noutput. Label shift, covariate shift, and concept drift are defined as follows:\\nCovariate shift\\nWhen P(X) changes but P(Y|X) remains the same. This refers to the first decom‐\\nposition of the joint distribution.\\nLabel shift\\nWhen P(Y) changes but P(X|Y) remains the same. This refers to the second\\ndecomposition of the joint distribution.\\nConcept drift\\nWhen P(Y|X) changes but P(X) remains the same. This refers to the first decom‐\\nposition of the joint distribution.21\\nIf you find this confusing, don’t panic. We’ll go over examples in the following section\\nto illustrate their differences.\\nCovariate shift\\nCovariate shift is one of the most widely studied forms of data distribution shift. 22 In\\nstatistics, a covariate is an independent variable that can influence the outcome of a\\ngiven statistical trial but which is not of direct interest. Consider that you are running\\nan experiment to determine how locations affect the housing prices. The housing\\nprice variable is your direct interest, but you know the square footage affects the\\nprice, so the square footage is a covariate. In supervised ML, the label is the variable\\nof direct interest, and the input features are covariate variables.\\nMathematically, covariate shift is when P(X) changes, but P(Y|X) remains the same,\\nwhich means that the distribution of the input changes, but the conditional probabil‐\\nity of an output given an input remains the same.\\nTo make this concrete, consider the task of detecting breast cancer. Y ou know that the\\nrisk of breast cancer is higher for women over the age of 40, 23 so you have a variable\\n“age” as your input. Y ou might have more women over the age of 40 in your training', 'To make this concrete, consider the task of detecting breast cancer. Y ou know that the\\nrisk of breast cancer is higher for women over the age of 40, 23 so you have a variable\\n“age” as your input. Y ou might have more women over the age of 40 in your training\\ndata than in your inference data, so the input distributions differ for your training\\nand inference data. However, for an example with a given age, such as above 40, the\\n238 | Chapter 8: Data Distribution Shifts and Monitoring', '24 Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernard\\nSchölkopf, “Covariate Shift by Kernel Mean Matching, ” Journal of Machine Learning Research (2009),\\nhttps://oreil.ly/s49MI.\\n25 Sugiyama and Kawanabe, Machine Learning in Non-stationary Environments.\\n26 Tongtong Fang, Nan Lu, Gang Niu, and Masashi Sugiyama, “Rethinking Importance Weighting for Deep\\nLearning under Distribution Shift, ” NeurIPS Proceedings 2020, https://oreil.ly/GzJ1r; Gretton et al., “Covariate\\nShift by Kernel Mean Matching. ”\\nprobability that this example has breast cancer is constant. So P(Y|X), the probability\\nof having breast cancer given age over 40, is the same.\\nDuring model development, covariate shifts can happen due to biases during the data\\nselection process, which could result from difficulty in collecting examples for certain\\nclasses. For example, suppose that to study breast cancer, you get data from a clinic\\nwhere women go to test for breast cancer. Because people over 40 are encouraged by\\ntheir doctors to get checkups, your data is dominated by women over 40. For this\\nreason, covariate shift is closely related to the sample selection bias problem.24\\nCovariate shifts can also happen because the training data is artificially altered to\\nmake it easier for your model to learn. As discussed in Chapter 4, it’s hard for ML\\nmodels to learn from imbalanced datasets, so you might want to collect more samples\\nof the rare classes or oversample your data on the rare classes to make it easier for\\nyour model to learn the rare classes.\\nCovariate shift can also be caused by the model’s learning process, especially through\\nactive learning. In Chapter 4, we defined active learning as follows: instead of ran‐\\ndomly selecting samples to train a model on, we use the samples most helpful to that\\nmodel according to some heuristics. This means that the training input distribution\\nis altered by the learning process to differ from the real-world input distribution, and\\ncovariate shifts are a by-product.25\\nIn production, covariate shift usually happens because of major changes in the envi‐\\nronment or in the way your application is used. Imagine you have a model to predict\\nhow likely a free user will be to convert to a paid user. The income level of the user\\nis a feature. Y our company’s marketing department recently launched a campaign that\\nattracts users from a demographic more affluent than your current demographic. The', 'how likely a free user will be to convert to a paid user. The income level of the user\\nis a feature. Y our company’s marketing department recently launched a campaign that\\nattracts users from a demographic more affluent than your current demographic. The\\ninput distribution into your model has changed, but the probability that a user with a\\ngiven income level will convert remains the same.\\nIf you know in advance how the real-world input distribution will differ from your\\ntraining input distribution, you can leverage techniques such as importance weighting\\nto train your model to work for the real-world data. Importance weighting consists\\nof two steps: estimate the density ratio between the real-world input distribution and\\nthe training input distribution, then weight the training data according to this ratio\\nand train an ML model on this weighted data.26\\nData Distribution Shifts | 239', '27 Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon, “On Learning Invariant Rep‐\\nresentations for Domain Adaptation, ” Proceedings of Machine Learning Research 97 (2019): 7523–32,\\nhttps://oreil.ly/ZxYWD.\\nHowever, because we don’t know in advance how the distribution will change in the\\nreal world, it’s very difficult to preemptively train your models to make them robust\\nto new, unknown distributions. There has been research that attempts to help models\\nlearn representations of latent variables that are invariant across data distributions, 27\\nbut I’m not aware of their adoption in the industry.\\nLabel shift\\nLabel shift, also known as prior shift, prior probability shift, or target shift, is when\\nP(Y) changes but P(X|Y) remains the same. Y ou can think of this as the case when the\\noutput distribution changes but, for a given output , the input distribution stays the\\nsame.\\nRemember that covariate shift is when the input distribution changes. When the\\ninput distribution changes, the output distribution also changes, resulting in both\\ncovariate shift and label shift happening at the same time. Consider the preceding\\nbreast cancer example for covariate shift. Because there are more women over 40 in\\nour training data than in our inference data, the percentage of POSITIVE labels is\\nhigher during training. However, if you randomly select person A with breast cancer\\nfrom your training data and person B with breast cancer from your test data, A and B\\nhave the same probability of being over 40. This means that P(X|Y), or probability of\\nage over 40 given having breast cancer, is the same. So this is also a case of label shift.\\nHowever, not all covariate shifts result in label shifts. It’s a subtle point, so we’ll\\nconsider another example. Imagine that there is now a preventive drug that every\\nwoman takes that helps reduce their chance of getting breast cancer. The probability\\nP(Y|X) reduces for women of all ages, so it’s no longer a case of covariate shift.\\nHowever, given a person with breast cancer, the age distribution remains the same, so\\nthis is still a case of label shift.\\nBecause label shift is closely related to covariate shift, methods for detecting and\\nadapting models to label shifts are similar to covariate shift adaptation methods. We’ll\\ndiscuss them more later in this chapter.\\n240 | Chapter 8: Data Distribution Shifts and Monitoring', '28 Y ou can think of this as the case where both P(X) and P(Y|X) change.\\nConcept drift\\nConcept drift, also known as posterior shift, is when the input distribution remains\\nthe same but the conditional distribution of the output given an input changes. Y ou\\ncan think of this as “same input, different output. ” Consider you’re in charge of a\\nmodel that predicts the price of a house based on its features. Before COVID-19, a\\nthree-bedroom apartment in San Francisco could cost $2,000,000. However, at the\\nbeginning of COVID-19, many people left San Francisco, so the same apartment\\nwould cost only $1,500,000. So even though the distribution of house features\\nremains the same, the conditional distribution of the price of a house given its\\nfeatures has changed.\\nIn many cases, concept drifts are cyclic or seasonal. For example, rideshare prices will\\nfluctuate on weekdays versus weekends, and flight ticket prices rise during holiday\\nseasons. Companies might have different models to deal with cyclic and seasonal\\ndrifts. For example, they might have one model to predict rideshare prices on week‐\\ndays and another model for weekends.\\nGeneral Data Distribution Shifts\\nThere are other types of changes in the real world that, even though not well studied\\nin research, can still degrade your models’ performance.\\nOne is feature change , such as when new features are added, older features are\\nremoved, or the set of all possible values of a feature changes. 28 For example, your\\nmodel was using years for the “age” feature, but now it uses months, so the range\\nof this feature’s values has drifted. One time, our team realized that our model’s\\nperformance plummeted because a bug in our pipeline caused a feature to become\\nNaNs (short for “not a number”).\\nLabel schema change is when the set of possible values for Y change. With label shift,\\nP(Y) changes but P(X|Y) remains the same. With label schema change, both P(Y) and\\nP(X|Y) change. A schema describes the structure of the data, so the label schema of\\na task describes the structure of the labels of that task. For example, a dictionary that\\nmaps from a class to an integer value, such as {“POSITIVE”: 0, “NEGATIVE”: 1}, is a\\nschema.\\nData Distribution Shifts | 241', '29 If you use a neural network using softmax as your last layer for your classification tax, the dimension of this\\nsoftmax layer is [number_of_hidden_units × number_of_classes]. When the number of classes changes, the\\nnumber of parameters in your softmax layer changes.\\n30 Y ou don’t need ground truth labels if you use an unsupervised learning method, but the vast majority of\\napplications today are supervised.\\nWith regression tasks, label schema change could happen because of changes in the\\npossible range of label values. Imagine you’re building a model to predict someone’s\\ncredit score. Originally, you used a credit score system that ranged from 300 to 850,\\nbut you switched to a new system that ranges from 250 to 900.\\nWith classification tasks, label schema change could happen because you have new\\nclasses. For example, suppose you are building a model to diagnose diseases and\\nthere’s a new disease to diagnose. Classes can also become outdated or more fine-\\ngrained. Imagine that you’re in charge of a sentiment analysis model for tweets\\nthat mention your brand. Originally, your model predicted only three classes: POSI‐\\nTIVE, NEGATIVE, and NEUTRAL. However, your marketing department realized\\nthe most damaging tweets are the angry ones, so they wanted to break the NEG‐\\nATIVE class into two classes: SAD and ANGRY . Instead of having three classes,\\nyour task now has four classes. When the number of classes changes, your mod‐\\nel’s structure might change, 29 and you might need to both relabel your data and\\nretrain your model from scratch. Label schema change is especially common with\\nhigh-cardinality tasks—tasks with a high number of classes—such as product or\\ndocumentation categorization.\\nThere’s no rule that says that only one type of shift should happen at one time. A\\nmodel might suffer from multiple types of drift, which makes handling them a lot\\nmore difficult.\\nDetecting Data Distribution Shifts\\nData distribution shifts are only a problem if they cause your model’s performance\\nto degrade. So the first idea might be to monitor your model’s accuracy-related met‐\\nrics—accuracy, F1 score, recall, AUC-ROC, etc.—in production to see whether they\\nhave changed. “Change” here usually means “decrease, ” but if my model’s accuracy\\nsuddenly goes up or fluctuates significantly for no reason that I’m aware of, I’ d want\\nto investigate.\\nAccuracy-related metrics work by comparing the model’s predictions to ground truth', 'have changed. “Change” here usually means “decrease, ” but if my model’s accuracy\\nsuddenly goes up or fluctuates significantly for no reason that I’m aware of, I’ d want\\nto investigate.\\nAccuracy-related metrics work by comparing the model’s predictions to ground truth\\nlabels.30 During model development, you have access to labels, but in production,\\nyou don’t always have access to labels, and even if you do, labels will be delayed, as\\ndiscussed in the section “Natural Labels” on page 91. Having access to labels within a\\nreasonable time window will vastly help with giving you visibility into your model’s\\nperformance.\\n242 | Chapter 8: Data Distribution Shifts and Monitoring', '31 Hamel Husain gave a great lecture on why TensorFlow Extended’s skew detection is so bad for CS 329S:\\nMachine Learning Systems Design (Stanford, 2022). Y ou can find the video on Y ouTube.\\nWhen ground truth labels are unavailable or too delayed to be useful, we can monitor\\nother distributions of interest instead. The distributions of interest are the input\\ndistribution P(X), the label distribution P(Y), and the conditional distributions P(X|\\nY) and P(Y|X).\\nWhile we don’t need to know the ground truth labels Y to monitor the input distri‐\\nbution, monitoring the label distribution and both of the conditional distributions\\nrequire knowing Y. In research, there have been efforts to understand and detect\\nlabel shifts without labels from the target distribution. One such effort is Black\\nBox Shift Estimation  by Lipton et al. (2018). However, in the industry, most drift\\ndetection methods focus on detecting changes in the input distribution, especially the\\ndistributions of features, as we discuss in detail in this chapter.\\nStatistical methods\\nIn industry, a simple method many companies use to detect whether the two distri‐\\nbutions are the same is to compare their statistics like min, max, mean, median,\\nvariance, various quantiles (such as 5th, 25th, 75th, or 95th quantile), skewness,\\nkurtosis, etc. For example, you can compute the median and variance of the values\\nof a feature during inference and compare them to the metrics computed during\\ntraining. As of October 2021, even TensorFlow Extended’s built-in data validation\\ntools use only summary statistics to detect the skew between the training and serving\\ndata and shifts between different days of training data. This is a good start, but these\\nmetrics are far from sufficient. 31 Mean, median, and variance are only useful with\\nthe distributions for which the mean/median/variance are useful summaries. If those\\nmetrics differ significantly, the inference distribution might have shifted from the\\ntraining distribution. However, if those metrics are similar, there’s no guarantee that\\nthere’s no shift.\\nA more sophisticated solution is to use a two-sample hypothesis test, shortened as\\ntwo-sample test. It’s a test to determine whether the difference between two popu‐\\nlations (two sets of data) is statistically significant. If the difference is statistically\\nsignificant, then the probability that the difference is a random fluctuation due to', 'two-sample test. It’s a test to determine whether the difference between two popu‐\\nlations (two sets of data) is statistically significant. If the difference is statistically\\nsignificant, then the probability that the difference is a random fluctuation due to\\nsampling variability is very low, and, therefore, the difference is caused by the fact\\nthat these two populations come from two distinct distributions. If you consider the\\ndata from yesterday to be the source population and the data from today to be the\\ntarget population and they are statistically different, it’s likely that the underlying data\\ndistribution has shifted between yesterday and today.\\nData Distribution Shifts | 243', '32 I. M. Chakravarti, R. G. Laha, and J. Roy, Handbook of Methods of Applied Statistics, vol. 1, Techniques of\\nComputation, Descriptive Methods, and Statistical Inference (New Y ork: Wiley, 1967).\\n33 Eric Feigelson and G. Jogesh Babu, “Beware the Kolmogorov-Smirnov Test!” Center for Astrostatistics, Penn\\nState University, https://oreil.ly/7AHcT.\\n34 Eric Breck, Marty Zinkevich, Neoklis Polyzotis, Steven Whang, and Sudip Roy, “Data Validation for Machine\\nLearning, ” Proceedings of SysML, 2019, https://oreil.ly/xoneh.\\n35 Li Bu, Cesare Alippi, and Dongbin Zhao, “ A pdf-Free Change Detection Test Based on Density Difference\\nEstimation, ” IEEE Transactions on Neural Networks and Learning Systems 29, no. 2 (February 2018): 324–34,\\nhttps://oreil.ly/RD8Uy. The authors claim that the method works on multidimensional inputs.\\n36 Stephan Rabanser, Stephan Günnemann, and Zachary C. Lipton, “Failing Loudly: An Empirical Study of\\nMethods for Detecting Dataset Shift, ” arXiv, October 29, 2018, https://oreil.ly/HxAwV.\\nA caveat is that just because the difference is statistically significant doesn’t mean that\\nit is practically important. However, a good heuristic is that if you are able to detect\\nthe difference from a relatively small sample, then it is probably a serious difference.\\nIf it takes a huge number of samples to detect, then the difference is probably not\\nworth worrying about.\\nA basic two-sample test is the Kolmogorov–Smirnov test, also known as the K-S\\nor KS test. 32 It’s a nonparametric statistical test, which means it doesn’t require any\\nparameters of the underlying distribution to work. It doesn’t make any assumption\\nabout the underlying distribution, which means it can work for any distribution.\\nHowever, one major drawback of the KS test is that it can only be used for one-\\ndimensional data. If your model’s predictions and labels are one-dimensional (scalar\\nnumbers), then the KS test is useful to detect label or prediction shifts. However, it\\nwon’t work for high-dimensional data, and features are usually high-dimensional. 33\\nKS tests can also be expensive and produce too many false positive alerts.34\\nAnother test is Least-Squares Density Difference, an algorithm that is based on the\\nleast squares density-difference estimation method. 35 There is also MMD, Maximum\\nMean Discrepancy  (Gretton et al. 2012), a kernel-based technique for multivariate\\ntwo-sample testing and its variant Learned Kernel MMD  (Liu et al. 2020). MMD', 'least squares density-difference estimation method. 35 There is also MMD, Maximum\\nMean Discrepancy  (Gretton et al. 2012), a kernel-based technique for multivariate\\ntwo-sample testing and its variant Learned Kernel MMD  (Liu et al. 2020). MMD\\nis popular in research, but as of writing this book, I’m not aware of any company\\nthat is using it in the industry. Alibi Detect is a great open source package with the\\nimplementations of many drift detection algorithms, as shown in Figure 8-2.\\nBecause two-sample tests often work better on low-dimensional data than on high-\\ndimensional data, it’s highly recommended that you reduce the dimensionality of\\nyour data before performing a two-sample test on it.36\\n244 | Chapter 8: Data Distribution Shifts and Monitoring', '37 Manuel Baena-García, José del Campo-Ávila, Raúl Fidalgo, Albert Bifet, Ricard Gavaldà, and Rafael Morales-\\nBueno, “Early Drift Detection Method, ” 2006, https://oreil.ly/Dnv0s.\\n38 Nandini Ramanan, Rasool Tahmasbi, Marjorie Sayer, Deokwoo Jung, Shalini Hemachandran, and Clau‐\\ndionor Nunes Coelho Jr., “Real-time Drift Detection on Time-series Data, ” arXiv, October 12, 2021,\\nhttps://oreil.ly/xmdqW.\\nFigure 8-2. Some drift detection algorithms implemented by Alibi Detect. Source: Screen‐\\nshot of the project’s GitHub repository\\nTime scale windows for detecting shifts\\nNot all types of shifts are equal—some are harder to detect than others. For example,\\nshifts happen at different rates, and abrupt changes are easier to detect than slow,\\ngradual changes.37 Shifts can also happen across two dimensions: spatial or temporal.\\nSpatial shifts are shifts that happen across access points, such as your application gets\\na new group of users or your application is now served on a different type of device.\\nTemporal shifts are shifts that happen over time. To detect temporal shifts, a common\\napproach is to treat input data to ML applications as time-series data.38\\nData Distribution Shifts | 245', 'When dealing with temporal shifts, the time scale window of the data we look at\\naffects the shifts we can detect. If your data has a weekly cycle, then a time scale of\\nless than a week won’t detect the cycle. Consider the data in Figure 8-3 . If we use\\ndata from day 9 to day 14 as the source distribution, then day 15 looks like a shift.\\nHowever, if we use data from day 1 to day 14 as the source distribution, then all data\\npoints from day 15 are likely being generated by that same distribution. As illustrated\\nby this example, detecting temporal shifts is hard when shifts are confounded by\\nseasonal variation.\\nFigure 8-3. Whether a distribution has drifted over time depends on the time scale\\nwindow specified\\nWhen computing running statistics over time, it’s important to differentiate between\\ncumulative and sliding statistics. Sliding statistics are computed within a single time\\nscale window, e.g., an hour. Cumulative statistics are continually updated with more\\ndata. This means, for the beginning of each time scale window, the sliding accuracy\\nis reset, whereas the cumulative sliding accuracy is not. Because cumulative statistics\\ncontain information from previous time windows, they might obscure what happens\\nin a specific time window. Figure 8-4 shows an example of how cumulative accuracy\\ncan hide the sudden dip in accuracy between hours 16 and 18.\\n246 | Chapter 8: Data Distribution Shifts and Monitoring', '39 I’m working on a solution that can handle the minute granularity level.\\n40 Thanks Goku Mohandas for sharing this tip on the MLOps Discord server.\\nFigure 8-4. Cumulative accuracy hides the sudden dip in accuracy between hours 16 and\\n18. Source: Adapted from an image by MadeWithML\\nWorking with data in the temporal space makes things so much more complicated,\\nrequiring knowledge of time-series analysis techniques such as time-series decompo‐\\nsitions that are beyond the scope of this book. For readers interested in time-series\\ndecomposition, Lyft engineering has a great case study on how they decompose their\\ntime-series data to deal with the seasonality of the market.\\nAs of today, many companies use the distribution of the training data as the base\\ndistribution and monitor the production data distribution at a certain granularity\\nlevel, such as hourly and daily.39 The shorter your time scale window, the faster you’ll\\nbe able to detect changes in your data distribution. However, too short a time scale\\nwindow can lead to false alarms of shifts, like the example in Figure 8-3.\\nSome platforms, especially those dealing with real-time data analytics such as moni‐\\ntoring, provide a merge operation that allows merging statistics from shorter time\\nscale windows to create statistics for larger time scale windows. For example, you can\\ncompute the data statistics you care about hourly, then merge these hourly statistics\\nchunks into daily views.\\nMore advanced monitoring platforms even attempt a root cause analysis (RCA) fea‐\\nture that automatically analyzes statistics across various time window sizes to detect\\nexactly the time window where a change in data happened.40\\nData Distribution Shifts | 247', '41 As Han-chung Lee, one early reviewer, pointed out, this is also because smaller companies don’t have enough\\ndata on their models. When you don’t have a lot of data, it’s better to have a time-based regimen than to\\noverfit your regime to insufficient data.\\n42 Kun Zhang, Bernhard Schölkopf, Krikamol Muandet, and Zhikun Wang, “Domain Adaptation under Tar‐\\nget and Conditional Shift, ” Proceedings of the 30th International Conference on Machine Learning (2013),\\nhttps://oreil.ly/C123l.\\n43 Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon, “On Learning Invariant Rep‐\\nresentations for Domain Adaptation, ” Proceedings of Machine Learning Research 97 (2019): 7523–32,\\nhttps://oreil.ly/W78hH.\\n44 Zachary C. Lipton, Yu-Xiang Wang, and Alex Smola, “Detecting and Correcting for Label Shift with Black\\nBox Predictors, ” arXiv, February 12, 2018, https://oreil.ly/zKSlj.\\nAddressing Data Distribution Shifts\\nHow companies address data shifts depends on how sophisticated their ML infra‐\\nstructure setups are. At one end of the spectrum, we have companies that have just\\nstarted with ML and are still working on getting ML models into production, so\\nthey might not have gotten to the point where data shifts are catastrophic to them.\\nHowever, at some point in the future—maybe three months, maybe six months—they\\nmight realize that their initial deployed models have degraded to the point that they\\ndo more harm than good. They will then need to adapt their models to the shifted\\ndistributions or to replace them with other solutions.\\nAt the same time, many companies assume that data shifts are inevitable, so they\\nperiodically retrain their models—once a month, once a week, or once a day—regard‐\\nless of the extent of the shift. How to determine the optimal frequency to retrain\\nyour models is an important decision that many companies still determine based on\\ngut feelings instead of experimental data. 41 We’ll discuss more about the retraining\\nfrequency in Chapter 9.\\nTo make a model work with a new distribution in production, there are three main\\napproaches. The first is the approach that currently dominates research: train models\\nusing massive datasets. The hope here is that if the training dataset is large enough,\\nthe model will be able to learn such a comprehensive distribution that whatever data\\npoints the model will encounter in production will likely come from this distribution.\\nThe second approach, less popular in research, is to adapt a trained model to a target', 'the model will be able to learn such a comprehensive distribution that whatever data\\npoints the model will encounter in production will likely come from this distribution.\\nThe second approach, less popular in research, is to adapt a trained model to a target\\ndistribution without requiring new labels. Zhang et al. (2013) used causal interpreta‐\\ntions together with kernel embedding of conditional and marginal distributions to\\ncorrect models’ predictions for both covariate shifts and label shifts without using\\nlabels from the target distribution. 42 Similarly, Zhao et al. (2020) proposed domain-\\ninvariant representation learning: an unsupervised domain adaptation technique\\nthat can learn data representations invariant to changing distributions. 43 However,\\nthis area of research is heavily underexplored and hasn’t found wide adoption in\\nindustry.44\\n248 | Chapter 8: Data Distribution Shifts and Monitoring', '45 Some monitoring vendors claim that their solutions are able to detect not only when your model should be\\nretrained, but also what data to retrain on. I haven’t been able to verify the validity of these claims.\\nThe third approach is what is usually done in the industry today: retrain your model\\nusing the labeled data from the target distribution. However, retraining your model is\\nnot so straightforward. Retraining can mean retraining your model from scratch on\\nboth the old and new data or continuing training the existing model on new data. The\\nlatter approach is also called fine-tuning.\\nIf you want to retrain your model, there are two questions. First, whether to train\\nyour model from scratch (stateless retraining) or continue training it from the last\\ncheckpoint (stateful training). Second, what data to use: data from the last 24 hours,\\nlast week, last 6 months, or from the point when data has started to drift. Y ou might\\nneed to run experiments to figure out which retraining strategy works best for you.45\\nIn this book, we use “retraining” to refer to both training from scratch and fine-\\ntuning. We’ll discuss more about retraining strategy in the next chapter.\\nReaders familiar with data shift literature might often see data shifts mentioned along\\nwith domain adaptation and transfer learning. If you consider a distribution to be a\\ndomain, then the question of how to adapt your model to new distributions is similar\\nto the question of how to adapt your model to different domains.\\nSimilarly, if you consider learning a joint distribution P(X, Y) as a task, then adapting\\na model trained on one joint distribution for another joint distribution can be framed\\nas a form of transfer learning. As discussed in Chapter 4, transfer learning refers to\\nthe family of methods where a model developed for a task is reused as the starting\\npoint for a model on a second task. The difference is that with transfer learning, you\\ndon’t retrain the base model from scratch for the second task. However, to adapt your\\nmodel to a new distribution, you might need to retrain your model from scratch.\\nAddressing data distribution shifts doesn’t have to start after the shifts have hap‐\\npened. It’s possible to design your system to make it more robust to shifts. A system\\nuses multiple features, and different features shift at different rates. Consider that\\nyou’re building a model to predict whether a user will download an app. Y ou might', 'pened. It’s possible to design your system to make it more robust to shifts. A system\\nuses multiple features, and different features shift at different rates. Consider that\\nyou’re building a model to predict whether a user will download an app. Y ou might\\nbe tempted to use that app’s ranking in the app store as a feature since higher-ranking\\napps tend to be downloaded more. However, app ranking changes very quickly. Y ou\\nmight want to instead bucket each app’s ranking into general categories such as top\\n10, between 11 and 100, between 101 and 1,000, between 1,001 and 10,000, and\\nso on. At the same time, an app’s categories might change a lot less frequently, but\\nthey might have less power to predict whether a user will download that app. When\\nchoosing features for your models, you might want to consider the trade-off between\\nthe performance and the stability of a feature: a feature might be really good for\\naccuracy but deteriorate quickly, forcing you to train your model more often.\\nData Distribution Shifts | 249', 'Y ou might also want to design your system to make it easier for it to adapt to\\nshifts. For example, housing prices might change a lot faster in major cities like\\nSan Francisco than in rural Arizona, so a housing price prediction model serving\\nrural Arizona might need to be updated less frequently than a model serving San\\nFrancisco. If you use the same model to serve both markets, you’ll have to use data\\nfrom both markets to update your model at the rate demanded by San Francisco.\\nHowever, if you use a separate model for each market, you can update each of them\\nonly when necessary.\\nBefore we move on to the next section, I want to reiterate that not all performance\\ndegradation of models in production requires ML solutions. Many ML failures today\\nare still caused by human errors. If your model failure is caused by human errors,\\nyou’ d first need to find those errors to fix them. Detecting a data shift is hard, but\\ndetermining what causes a shift can be even harder.\\nMonitoring and Observability\\nAs the industry realized that many things can go wrong with an ML system, many\\ncompanies started investing in monitoring and observability for their ML systems in\\nproduction.\\nMonitoring and observability are sometimes used exchangeably, but they are differ‐\\nent. Monitoring refers to the act of tracking, measuring, and logging different metrics\\nthat can help us determine when something goes wrong. Observability means setting\\nup our system in a way that gives us visibility into our system to help us investigate\\nwhat went wrong. The process of setting up our system in this way is also called\\n“instrumentation. ” Examples of instrumentation are adding timers to your functions,\\ncounting NaNs in your features, tracking how inputs are transformed through your\\nsystems, logging unusual events such as unusually long inputs, etc. Observability is\\npart of monitoring. Without some level of observability, monitoring is impossible.\\nMonitoring is all about metrics. Because ML systems are software systems, the first\\nclass of metrics you’ d need to monitor are the operational metrics. These metrics are\\ndesigned to convey the health of your systems. They are generally divided into three\\nlevels: the network the system is run on, the machine the system is run on, and the\\napplication that the system runs. Examples of these metrics are latency; throughput;\\nthe number of prediction requests your model receives in the last minute, hour,', 'levels: the network the system is run on, the machine the system is run on, and the\\napplication that the system runs. Examples of these metrics are latency; throughput;\\nthe number of prediction requests your model receives in the last minute, hour,\\nday; the percentage of requests that return with a 2xx code; CPU/GPU utilization;\\nmemory utilization; etc. No matter how good your ML model is, if the system is\\ndown, you’re not going to benefit from it.\\nLet’s look at an example. One of the most important characteristics of a software\\nsystem in production is availability—how often the system is available to offer\\nreasonable performance to users. This characteristic is measured by uptime, the\\n250 | Chapter 8: Data Distribution Shifts and Monitoring', '46 “ Amazon Compute Service Level Agreement, ” Amazon Web Services, last updated August 24, 2021,\\nhttps://oreil.ly/5bjx9.\\npercentage of time a system is up. The conditions to determine whether a system\\nis up are defined in the service level objectives (SLOs) or service level agreements\\n(SLAs). For example, an SLA may specify that the service is considered to be up if it\\nhas a median latency of less than 200 ms and a 99th percentile under 2 s.\\nA service provider might offer an SLA that specifies their uptime guarantee, such as\\n99.99% of the time, and if this guarantee is not met, they’ll give their customers back\\nmoney. For example, as of October 2021, AWS EC2 service offers a monthly uptime\\npercentage of at least 99.99% (four nines), and if the monthly uptime percentage is\\nlower than that, they’ll give you back a service credit toward future EC2 payments. 46\\nA 99.99% monthly uptime means the service is only allowed to be down a little over 4\\nminutes a month, and 99.999% means only 26 seconds a month!\\nHowever, for ML systems, the system health extends beyond the system uptime. If\\nyour ML system is up but its predictions are garbage, your users aren’t going to be\\nhappy. Another class of metrics you’ d want to monitor are ML-specific metrics that\\ntell you the health of your ML models.\\nML-Specific Metrics\\nWithin ML-specific metrics, there are generally four artifacts to monitor: a model’s\\naccuracy-related metrics, predictions, features, and raw inputs. These are artifacts\\ngenerated at four different stages of an ML system pipeline, as shown in Figure 8-5.\\nThe deeper into the pipeline an artifact is, the more transformations it has gone\\nthrough, which makes a change in that artifact more likely to be caused by errors\\nin one of those transformations. However, the more transformations an artifact has\\ngone through, the more structured it’s become and the closer it is to the metrics you\\nactually care about, which makes it easier to monitor. We’ll look at each of these\\nartifacts in detail in the following sections.\\nFigure 8-5. The more transformations an artifact has gone through, the more likely its\\nchanges are to be caused by errors in one of those transformations\\nMonitoring and Observability | 251', '47 Be careful when using the completion rate as a metric to optimize for, as it might bias your recommender\\nsystem toward short videos.\\nMonitoring accuracy-related metrics\\nIf your system receives any type of user feedback for the predictions it makes—\\nclick, hide, purchase, upvote, downvote, favorite, bookmark, share, etc.—you should\\ndefinitely log and track it. Some feedback can be used to infer natural labels, which\\ncan then be used to calculate your model’s accuracy-related metrics. Accuracy-related\\nmetrics are the most direct metrics to help you decide whether a model’s performance\\nhas degraded.\\nEven if the feedback can’t be used to infer natural labels directly, it can be used to\\ndetect changes in your ML model’s performance. For example, when you’re building\\na system to recommend to users what videos to watch next on Y ouTube, you want to\\ntrack not only whether the users click on a recommended video (click-through rate),\\nbut also the duration of time users spend on that video and whether they complete\\nwatching it (completion rate). If, over time, the click-through rate remains the same\\nbut the completion rate drops, it might mean that your recommender system is\\ngetting worse.47\\nIt’s also possible to engineer your system so that you can collect users’ feedback.\\nFor example, Google Translate has the option for users to upvote or downvote a\\ntranslation, as shown in Figure 8-6. If the number of downvotes the system receives\\nsuddenly goes up, there might be issues. These downvotes can also be used to guide\\nthe labeling process, such as getting human experts to generate new translations for\\nthe samples with downvotes, to train the next iteration of their models.\\nFigure 8-6. Google Translate allows users to upvote or downvote a translation. These\\nvotes will be used to evaluate their translation model’s quality as well as to guide the\\nlabeling process.\\nMonitoring predictions\\nPrediction is the most common artifact to monitor. If it’s a regression task, each\\nprediction is a continuous value (e.g., the predicted price of a house), and if it’s\\na classification task, each prediction is a discrete value corresponding to the predic‐\\nted category. Because each prediction is usually just a number (low dimension),\\n252 | Chapter 8: Data Distribution Shifts and Monitoring', 'predictions are easy to visualize, and their summary statistics are straightforward to\\ncompute and interpret.\\nY ou can monitor predictions for distribution shifts. Because predictions are low\\ndimensional, it’s also easier to compute two-sample tests to detect whether the\\nprediction distribution has shifted. Prediction distribution shifts are also a proxy\\nfor input distribution shifts. Assuming that the function that maps from input to\\noutput doesn’t change—the weights and biases of your model haven’t changed—then\\na change in the prediction distribution generally indicates a change in the underlying\\ninput distribution.\\nY ou can also monitor predictions for anything odd happening, such as predicting an\\nunusual number of False in a row. There could be a long delay between predictions\\nand ground truth labels, as discussed in the section “Natural Labels” on page 91.\\nChanges in accuracy-related metrics might not become obvious for days or weeks,\\nwhereas a model predicting all False for 10 minutes can be detected immediately.\\nMonitoring features\\nML monitoring solutions in the industry focus on tracking changes in features, both\\nthe features that a model uses as inputs and the intermediate transformations from\\nraw inputs into final features. Feature monitoring is appealing because compared to\\nraw input data, features are well structured following a predefined schema. The first\\nstep of feature monitoring is feature validation: ensuring that your features follow an\\nexpected schema. The expected schemas are usually generated from training data or\\nfrom common sense. If these expectations are violated in production, there might be\\na shift in the underlying distribution. For example, here are some of the things you\\ncan check for a given feature:\\n• If the min, max, or median values of a feature are within an acceptable range•\\n• If the values of a feature satisfy a regular expression format•\\n• If all the values of a feature belong to a predefined set•\\n• If the values of a feature are always greater than the values of another feature•\\nBecause features are often organized into tables—each column representing a feature\\nand each row representing a data sample—feature validation is also known as table\\ntesting or table validation. Some call them unit tests for data. There are many open\\nsource libraries that help you do basic feature validation, and the two most common\\nare Great Expectations and Deequ, which is by AWS. Figure 8-7 shows some of the', 'testing or table validation. Some call them unit tests for data. There are many open\\nsource libraries that help you do basic feature validation, and the two most common\\nare Great Expectations and Deequ, which is by AWS. Figure 8-7 shows some of the\\nbuilt-in feature validation functions by Great Expectations and an example of how to\\nuse them.\\nMonitoring and Observability | 253', 'Figure 8-7. Some of the built-in feature validation functions by Great Expectations and\\nan example of how to use them. Source: Adapted from content in the Great Expectations\\nGitHub repository\\nBeyond basic feature validation, you can also use two-sample tests to detect whether\\nthe underlying distribution of a feature or a set of features has shifted. Since a\\nfeature or a set of features can be high-dimensional, you might need to reduce their\\ndimension before performing the test on them, which can make the test less effective.\\nThere are four major concerns when doing feature monitoring:\\nA company might have hundreds of models in production, and each model uses hun‐\\ndreds, if not thousands, of features.\\nEven something as simple as computing summary statistics for all these features\\nevery hour can be expensive, not only in terms of compute required but also\\nmemory used. Tracking, i.e., constantly computing, too many metrics can also\\nslow down your system and increase both the latency that your users experience\\nand the time it takes for you to detect anomalies in your system.\\nWhile tracking features is useful for debugging purposes, it’s not very useful for detecting\\nmodel performance degradation.\\nIn theory, a small distribution shift can cause catastrophic failure, but in practice,\\nan individual feature’s minor changes might not harm the model’s performance\\nat all. Feature distributions shift all the time, and most of these changes are\\n254 | Chapter 8: Data Distribution Shifts and Monitoring', '48 Rabanser, Günnemann, and Lipton, “Failing Loudly. ”\\nbenign.48 If you want to be alerted whenever a feature seems to have drifted,\\nyou might soon be overwhelmed by alerts and realize that most of these alerts\\nare false positives. This can cause a phenomenon called “alert fatigue” where the\\nmonitoring team stops paying attention to the alerts because they are so frequent.\\nThe problem of feature monitoring becomes the problem of trying to decide\\nwhich feature shifts are critical and which are not.\\nFeature extraction is often done in multiple steps (such as filling missing values and\\nstandardization), using multiple libraries (such as pandas, Spark), on multiple services\\n(such as BigQuery or Snowflake).\\nY ou might have a relational database as an input to the feature extraction process\\nand a NumPy array as the output. Even if you detect a harmful change in a\\nfeature, it might be impossible to detect whether this change is caused by a\\nchange in the underlying input distribution or whether it’s caused by an error in\\none of the multiple processing steps.\\nThe schema that your features follow can change over time.\\nIf you don’t have a way to version your schemas and map each of your features\\nto its expected schema, the cause of the reported alert might be due to the\\nmismatched schema rather than a change in the data.\\nThese concerns are not to dismiss the importance of feature monitoring; changes\\nin the feature space are a useful source of signals to understand the health of your\\nML systems. Hopefully, thinking about these concerns can help you choose a feature\\nmonitoring solution that works for you.\\nMonitoring raw inputs\\nAs discussed in the previous section, a change in the features might be caused by\\nproblems in processing steps and not by changes in data. What if we monitor the\\nraw inputs before they are processed? The raw input data might not be easier to\\nmonitor, as it can come from multiple sources in different formats, following multiple\\nstructures. The way many ML workflows are set up today also makes it impossible\\nfor ML engineers to get direct access to raw input data, as the raw input data is often\\nmanaged by a data platform team who processes and moves the data to a location\\nlike a data warehouse, and the ML engineers can only query for data from that data\\nwarehouse where the data is already partially processed. Therefore, monitoring raw\\ninputs is often a responsibility of the data platform team, not the data science or ML', 'like a data warehouse, and the ML engineers can only query for data from that data\\nwarehouse where the data is already partially processed. Therefore, monitoring raw\\ninputs is often a responsibility of the data platform team, not the data science or ML\\nteam. Therefore, it’s out of scope for this book.\\nMonitoring and Observability | 255', '49 Ian Malpass, “Measure Anything, Measure Everything, ” Code as Craft, February 15, 2011,\\nhttps://oreil.ly/3KF1K.\\n50 Andrew Morgan, “Data Engineering in Badoo: Handling 20 Billion Events Per Day, ” InfoQ, August 9, 2019,\\nhttps://oreil.ly/qnnuV.\\n51 Charity Majors, “Observability—A 3-Y ear Retrospective, ” The New Stack, August 6, 2019, https://oreil.ly/Logby.\\nSo far, we’ve discussed different types of metrics to monitor, from operational metrics\\ngenerally used for software systems to ML-specific metrics that help you keep track of\\nthe health of your ML models. In the next section, we’ll discuss the toolbox you can\\nuse to help with metrics monitoring.\\nMonitoring Toolbox\\nMeasuring, tracking, and interpreting metrics for complex systems is a nontrivial\\ntask, and engineers rely on a set of tools to help them do so. It’s common for\\nthe industry to herald metrics, logs, and traces as the three pillars of monitoring.\\nHowever, I find their differentiations murky. They seem to be generated from the\\nperspective of people who develop monitoring systems: traces are a form of logs and\\nmetrics can be computed from logs. In this section, I’ d like to focus on the set of tools\\nfrom the perspective of users of the monitoring systems: logs, dashboards, and alerts.\\nLogs\\nTraditional software systems rely on logs to record events produced at runtime. An\\nevent is anything that can be of interest to the system developers, either at the time\\nthe event happens or later for debugging and analysis purposes. Examples of events\\nare when a container starts, the amount of memory it takes, when a function is called,\\nwhen that function finishes running, the other functions that this function calls, the\\ninput and output of that function, etc. Also, don’t forget to log crashes, stack traces,\\nerror codes, and more. In the words of Ian Malpass at Etsy, “If it moves, we track it. ”49\\nThey also track things that haven’t changed yet, in case they’ll move later.\\nThe number of logs can grow very large very quickly. For example, back in 2019,\\nthe dating app Badoo was handling 20 billion events a day. 50 When something goes\\nwrong, you’ll need to query your logs for the sequence of events that caused it, a\\nprocess that can feel like searching for a needle in a haystack.\\nIn the early days of software deployment, an application might be one single service.\\nWhen something happened, you knew where that happened. But today, a system', 'process that can feel like searching for a needle in a haystack.\\nIn the early days of software deployment, an application might be one single service.\\nWhen something happened, you knew where that happened. But today, a system\\nmight consist of many different components: containers, schedulers, microservices,\\npolyglot persistence, mesh routing, ephemeral auto-scaling instances, serverless\\nLambda functions. A request may do 20–30 hops from when it’s sent until when\\na response is received. The hard part might not be in detecting when something\\nhappened, but where the problem was.51\\n256 | Chapter 8: Data Distribution Shifts and Monitoring', '52 “Log Management Market Size, Share and Global Market Forecast to 2026, ” MarketsandMarkets, 2021,\\nhttps://oreil.ly/q0xgh.\\n53 For readers unfamiliar with stream processing, please refer to the section “Batch Processing Versus Stream\\nProcessing” on page 78.\\nWhen we log an event, we want to make it as easy as possible for us to find it later.\\nThis practice with microservice architecture is called distributed tracing. We want\\nto give each process a unique ID so that, when something goes wrong, the error\\nmessage will (hopefully) contain that ID. This allows us to search for the log messages\\nassociated with it. We also want to record with each event all the metadata necessary:\\nthe time when it happens, the service where it happens, the function that is called, the\\nuser associated with the process, if any, etc.\\nBecause logs have grown so large and so difficult to manage, there have been many\\ntools developed to help companies manage and analyze logs. The log management\\nmarket is estimated to be worth USD 2.3 billion in 2021, and it’s expected to grow to\\nUSD 4.1 billion by 2026.52\\nAnalyzing billions of logged events manually is futile, so many companies use ML\\nto analyze logs. An example use case of ML in log analysis is anomaly detection:\\nto detect abnormal events in your system. A more sophisticated model might even\\nclassify each event in terms of its priorities such as usual, abnormal, exception, error,\\nand fatal.\\nAnother use case of ML in log analysis is that when a service fails, it might be helpful\\nto know the probability of related services being affected. This could be especially\\nuseful when the system is under cyberattack.\\nMany companies process logs in batch processes. In this scenario, you collect a large\\nnumber of logs, then periodically query over them looking for specific events using\\nSQL or process them using a batch process like in a Spark or Hadoop or Hive cluster.\\nThis makes the processing of logs efficient because you can leverage distributed and\\nMapReduce processes to increase your processing throughput. However, because you\\nprocess your logs periodically, you can only discover problems periodically.\\nTo discover anomalies in your logs as soon as they happen, you want to process your\\nevents as soon as they are logged. This makes log processing a stream processing\\nproblem.53 Y ou can use real-time transport such as Kafka or Amazon Kinesis to\\ntransport events as they are logged. To search for events with specific characteristics', 'events as soon as they are logged. This makes log processing a stream processing\\nproblem.53 Y ou can use real-time transport such as Kafka or Amazon Kinesis to\\ntransport events as they are logged. To search for events with specific characteristics\\nin real time, you can leverage a streaming SQL engine like KSQL or Flink SQL.\\nMonitoring and Observability | 257', 'Dashboards\\nA picture is worth a thousand words. A series of numbers might mean nothing to\\nyou, but visualizing them on a graph might reveal the relationships among these\\nnumbers. Dashboards to visualize metrics are critical for monitoring.\\nAnother use of dashboards is to make monitoring accessible to nonengineers. Moni‐\\ntoring isn’t just for the developers of a system, but also for nonengineering stakehold‐\\ners including product managers and business developers.\\nEven though graphs can help a lot with understanding metrics, they aren’t sufficient\\non their own. Y ou still need experience and statistical knowledge. Consider the two\\ngraphs in Figure 8-8. The only thing that is obvious from these graphs is that the loss\\nfluctuates a lot. If there’s a distribution shift in any of these two graphs, I can’t tell. It’s\\neasier to plot a graph to draw a wiggling line than to understand what this wiggly line\\nmeans.\\nFigure 8-8. Graphs are useful for making sense of numbers, but they aren’t sufficient\\nExcessive metrics on a dashboard can also be counterproductive, a phenomenon\\nknown as dashboard rot . It’s important to pick the right metrics or abstract out\\nlower-level metrics to compute higher-level signals that make better sense for your\\nspecific tasks.\\n258 | Chapter 8: Data Distribution Shifts and Monitoring', 'Alerts\\nWhen our monitoring system detects something suspicious, it’s necessary to alert the\\nright people about it. An alert consists of the following three components:\\nAn alert policy\\nThis describes the condition for an alert. Y ou might want to create an alert when\\na metric breaches a threshold, optionally over a certain duration. For example,\\nyou might want to be notified when a model’s accuracy is under 90%, or that the\\nHTTP response latency is higher than a second for at least 10 minutes.\\nNotification channels\\nThese describe who is to be notified when the condition is met. The alerts will\\nbe shown in the monitoring service you employ, such as Amazon CloudWatch\\nor GCP Cloud Monitoring, but you also want to reach responsible people when\\nthey’re not on these monitoring services. For example, you might configure your\\nalerts to be sent to an email address such as mlops-monitoring@ [your company\\nemail domain], or to post to a Slack channel such as #mlops-monitoring or to\\nPagerDuty.\\nA description of the alert\\nThis helps the alerted person understand what’s going on. The description should\\nbe as detailed as possible, such as:\\n## Recommender model accuracy below 90%\\n${timestamp}: This alert originated from the service ${service-name}\\nDepending on the audience of the alert, it’s often necessary to make the alert\\nactionable by providing mitigation instructions or a runbook, a compilation of\\nroutine procedures and operations that might help with handling the alert.\\nAlert fatigue is a real phenomenon, as discussed previously in this chapter. Alert\\nfatigue can be demoralizing—nobody likes to be awakened in the middle of the night\\nfor something outside of their responsibilities. It’s also dangerous—being exposed to\\ntrivial alerts can desensitize people to critical alerts. It’s important to set meaningful\\nconditions so that only critical alerts are sent out.\\nObservability\\nSince the mid-2010s, the industry has started embracing the term “observability”\\ninstead of “monitoring. ” Monitoring makes no assumption about the relationship\\nbetween the internal state of a system and its outputs. Y ou monitor the external\\noutputs of the system to figure out when something goes wrong inside the system—\\nthere’s no guarantee that the external outputs will help you figure out what goes\\nwrong.\\nMonitoring and Observability | 259', '54 Suman Karumuri, Franco Solleza, Stan Zdonik, and Nesime Tatbul, “Towards Observability Data Manage‐\\nment at Scale, ” ACM SIGMOD Record 49, no. 4 (December 2020): 18–23, https://oreil.ly/oS5hn.\\nIn the early days of software deployment, software systems were simple enough that\\nmonitoring external outputs was sufficient for software maintenance. A system used\\nto consist of only a few components, and a team used to have control over the entire\\ncodebase. If something went wrong, it was possible to make changes to the system to\\ntest and figure out what went wrong.\\nHowever, software systems have grown significantly more complex over the last\\ndecade. Today, a software system consists of many components. Many of these com‐\\nponents are services run by other companies—cue all cloud native services—which\\nmeans that a team doesn’t even have control of the inside of all the components of\\ntheir system. When something goes wrong, a team can no longer just break apart\\ntheir system to find out. The team has to rely on external outputs of their system to\\nfigure out what’s going on internally.\\nObservability is a term used to address this challenge. It’s a concept drawn from con‐\\ntrol theory, and it refers to bringing “better visibility into understanding the complex\\nbehavior of software using [outputs] collected from the system at run time. ”54\\nTelemetry\\nA system’s outputs collected at runtime are also called telemetry. Telemetry is another\\nterm that has emerged in the software monitoring industry over the last decade. The\\nword “telemetry” comes from the Greek roots tele, meaning “remote, ” and metron,\\nmeaning “measure. ” So telemetry basically means “remote measures. ” In the monitor‐\\ning context, it refers to logs and metrics collected from remote components such as\\ncloud services or applications run on customer devices.\\nIn other words, observability makes an assumption stronger than traditional moni‐\\ntoring: that the internal states of a system can be inferred from knowledge of its\\nexternal outputs. Internal states can be current states, such as “the GPU utilization\\nright now, ” and historical states, such as “the average GPU utilization over the last\\nday. ”\\nWhen something goes wrong with an observable system, we should be able to figure\\nout what went wrong by looking at the system’s logs and metrics without having to\\nship new code to the system. Observability is about instrumenting your system in a', 'day. ”\\nWhen something goes wrong with an observable system, we should be able to figure\\nout what went wrong by looking at the system’s logs and metrics without having to\\nship new code to the system. Observability is about instrumenting your system in a\\nway to ensure that sufficient information about a system’s runtime is collected and\\nanalyzed.\\n260 | Chapter 8: Data Distribution Shifts and Monitoring', '55 See the section “Feature Importance” on page 142.\\nMonitoring centers around metrics, and metrics are usually aggregated. Observability\\nallows more fine-grain metrics, so that you can know not only when a model’s\\nperformance degrades but also for what types of inputs or what subgroups of users\\nor over what period of time the model degrades. For example, you should be able\\nto query your logs for the answers to questions like: “show me all the users for\\nwhich model A returned wrong predictions over the last hour, grouped by their zip\\ncodes” or “show me the outliers requests in the last 10 minutes” or “show me all the\\nintermediate outputs of this input through the system. ” To achieve this, you need to\\nhave logged your system’s outputs using tags and other identifying keywords to allow\\nthese outputs to later be sliced and diced along different dimensions of your data.\\nIn ML, observability encompasses interpretability. Interpretability helps us under‐\\nstand how an ML model works, and observability helps us understand how the entire\\nML system, which includes the ML model, works. For example, when a model’s\\nperformance degrades over the last hour, being able to interpret which feature con‐\\ntributes the most to all the wrong predictions made over the last hour will help with\\nfiguring out what went wrong with the system and how to fix it.55\\nIn this section, we’ve discussed multiple aspects of monitoring, from what data to\\nmonitor and what metrics to keep track of to different tools for monitoring and\\nobservability. Even though monitoring is a powerful concept, it’s inherently passive.\\nY ou wait for a shift to happen to detect it. Monitoring helps unearth the problem\\nwithout correcting it. In the next section, we’ll introduce continual learning, a para‐\\ndigm that can actively help you update your models to address shifts.\\nSummary\\nThis might have been the most challenging chapter for me to write in this book. The\\nreason is that despite the importance of understanding how and why ML systems fail\\nin production, the literature surrounding it is limited. We usually think of research\\npreceding production, but this is an area of ML where research is still trying to catch\\nup with production.\\nTo understand failures of ML systems, we differentiated between two types of failures:\\nsoftware systems failures (failures that also happen to non-ML systems) and ML-\\nspecific failures. Even though the majority of ML failures today are non-ML-specific,', 'up with production.\\nTo understand failures of ML systems, we differentiated between two types of failures:\\nsoftware systems failures (failures that also happen to non-ML systems) and ML-\\nspecific failures. Even though the majority of ML failures today are non-ML-specific,\\nas tooling and infrastructure around MLOps matures, this might change.\\nWe discussed three major causes of ML-specific failures: production data differing\\nfrom training data, edge cases, and degenerate feedback loops. The first two causes\\nare related to data, whereas the last cause is related to system design because it\\nhappens when the system’s outputs influence the same system’s input.\\nSummary | 261', 'We zeroed into one failure that has gathered much attention in recent years: data\\ndistribution shifts. We looked into three types of shifts: covariate shift, label shift,\\nand concept drift. Even though studying distribution shifts is a growing subfield of\\nML research, the research community hasn’t yet found a standard narrative. Different\\npapers call the same phenomena by different names. Many studies are still based\\non the assumption that we know in advance how the distribution will shift or have\\nthe labels for the data from both the source distribution and the target distribution.\\nHowever, in reality, we don’t know what the future data will be like, and obtaining\\nlabels for new data might be costly, slow, or just infeasible.\\nTo be able to detect shifts, we need to monitor our deployed systems. Monitoring is\\nan important set of practices for any software engineering system in production, not\\njust ML, and it’s an area of ML where we should learn as much as we can from the\\nDevOps world.\\nMonitoring is all about metrics. We discussed different metrics we need to monitor:\\noperational metrics—the metrics that should be monitored with any software systems\\nsuch as latency, throughput, and CPU utilization—and ML-specific metrics. Moni‐\\ntoring can be applied to accuracy-related metrics, predictions, features, and/or raw\\ninputs.\\nMonitoring is hard because even if it’s cheap to compute metrics, understanding\\nmetrics isn’t straightforward. It’s easy to build dashboards to show graphs, but it’s\\nmuch more difficult to understand what a graph means, whether it shows signs\\nof drift, and, if there’s drift, whether it’s caused by an underlying data distribution\\nchange or by errors in the pipeline. An understanding of statistics might be required\\nto make sense of the numbers and graphs.\\nDetecting model performance’s degradation in production is the first step. The next\\nstep is how to adapt our systems to changing environments, which we’ll discuss in the\\nnext chapter.\\n262 | Chapter 8: Data Distribution Shifts and Monitoring', 'CHAPTER 9\\nContinual Learning and Test in Production\\nIn Chapter 8 , we discussed various ways an ML system can fail in production.\\nWe focused on one especially thorny problem that has generated much discussion\\namong both researchers and practitioners: data distribution shifts. We also discussed\\nmultiple monitoring techniques and tools to detect data distribution shifts.\\nThis chapter is a continuation of this discussion: how do we adapt our models to data\\ndistribution shifts? The answer is by continually updating our ML models. We’ll start\\nwith a discussion on what continual learning is and its challenges—spoiler: continual\\nlearning is largely an infrastructural problem. Then we’ll lay out a four-stage plan to\\nmake continual learning a reality.\\nAfter you’ve set up your infrastructure to allow you to update your models as fre‐\\nquently as you want, you might want to consider the question that I’ve been asked by\\nalmost every single ML engineer I’ve met: “How often should I retrain my models?”\\nThis question is the focus of the next section of the book.\\nIf the model is retrained to adapt to the changing environment, evaluating it on\\na stationary test set isn’t enough. We’ll cover a seemingly terrifying but necessary\\nconcept: test in production. This process is a way to test your systems with live data\\nin production to ensure that your updated model indeed works without catastrophic\\nconsequences.\\nTopics in this chapter and the previous chapter are tightly coupled. Test in production\\nis complementary to monitoring. If monitoring means passively keeping track of\\nthe outputs of whatever model is being used, test in production means proactively\\nchoosing which model to produce outputs so that we can evaluate it. The goal of\\nboth monitoring and test in production is to understand a model’s performance and\\nfigure out when to update it. The goal of continual learning is to safely and efficiently\\n263', '1 Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou, “Overcoming Catastrophic Forgetting\\nwith Hard Attention to the Task, ” arXiv, January 4, 2018, https://oreil.ly/P95EZ.\\nautomate the update. All of these concepts allow us to design an ML system that is\\nmaintainable and adaptable to changing environments.\\nThis is the chapter I’m most excited to write about, and I hope that I can get you\\nexcited about it too!\\nContinual Learning\\nWhen hearing “continual learning, ” many people think of the training paradigm\\nwhere a model updates itself with every incoming sample in production. Very few\\ncompanies actually do that. First, if your model is a neural network, learning with\\nevery incoming sample makes it susceptible to catastrophic forgetting. Catastrophic\\nforgetting refers to the tendency of a neural network to completely and abruptly\\nforget previously learned information upon learning new information.1\\nSecond, it can make training more expensive—most hardware backends today were\\ndesigned for batch processing, so processing only one sample at a time causes a huge\\nwaste of compute power and is unable to exploit data parallelism.\\nCompanies that employ continual learning in production update their models in\\nmicro-batches. For example, they might update the existing model after every 512\\nor 1,024 examples—the optimal number of examples in each micro-batch is task\\ndependent.\\nThe updated model shouldn’t be deployed until it’s been evaluated. This means that\\nyou shouldn’t make changes to the existing model directly. Instead, you create a rep‐\\nlica of the existing model and update this replica on new data, and only replace the\\nexisting model with the updated replica if the updated replica proves to be better. The\\nexisting model is called the champion model, and the updated replica, the challenger.\\nThis process is shown in Figure 9-1. This is an oversimplification of the process for\\nthe sake of understanding. In reality, a company might have multiple challengers at\\nthe same time, and handling the failed challenger is a lot more sophisticated than\\nsimply discarding it.\\n264 | Chapter 9: Continual Learning and Test in Production', '2 It’s “stateful training” instead of “stateful retraining” because there’s no re-training here. The model continues\\ntraining from the last state.\\nFigure 9-1. A simplification of how continual learning might work in production. In\\nreality, the process of handling the failed challenger is a lot more sophisticated than\\nsimply discarding it.\\nStill, the term “continual learning” makes people imagine updating models very\\nfrequently, such as every 5 or 10 minutes. Many people argue that most companies\\ndon’t need to update their models that frequently because of two reasons. First, they\\ndon’t have enough traffic (i.e., enough new data) for that retraining schedule to make\\nsense. Second, their models don’t decay that fast. I agree with them. If changing the\\nretraining schedule from a week to a day gives no return and causes more overhead,\\nthere’s no need to do it.\\nStateless Retraining Versus Stateful Training\\nHowever, continual learning isn’t about the retraining frequency, but the manner\\nin which the model is retrained. Most companies do stateless retraining—the model\\nis trained from scratch each time. Continual learning means also allowing stateful\\ntraining—the model continues training on new data. 2 Stateful training is also known\\nas fine-tuning or incremental learning. The difference between stateless retraining\\nand stateful training is visualized in Figure 9-2.\\nContinual Learning | 265', '3 Alex Egg, “Online Learning for Recommendations at Grubhub, ” arXiv, July 15, 2021, https://oreil.ly/FBBUw.\\nFigure 9-2. Stateless retraining versus stateful training\\nStateful training allows you to update your model with less data. Training a model\\nfrom scratch tends to require a lot more data than fine-tuning the same model. For\\nexample, if you retrain your model from scratch, you might need to use all data\\nfrom the last three months. However, if you fine-tune your model from yesterday’s\\ncheckpoint, you only need to use data from the last day.\\nGrubhub found out that stateful training allows their models to converge faster and\\nrequire much less compute power. Going from daily stateless retraining to daily\\nstateful training reduced their training compute cost 45 times and increased their\\npurchase-through rate by 20%.3\\nOne beautiful property that is often overlooked is that with stateful training, it might\\nbe possible to avoid storing data altogether. In the traditional stateless retraining, a\\ndata sample might be reused during multiple training iterations of a model, which\\nmeans that data needs to be stored. This isn’t always possible, especially for data with\\nstrict privacy requirements. In the stateful training paradigm, each model update is\\ntrained using only the fresh data, so a data sample is used only once for training, as\\nshown in Figure 9-2. This means that it’s possible to train your model without having\\nto store data in permanent storage, which helps eliminate many concerns about data\\nprivacy. However, this is overlooked because today’s let’s-keep-track-of-everything\\npractice still makes many companies reluctant to throw away data.\\n266 | Chapter 9: Continual Learning and Test in Production', '4 Mu Li, Li Zhou, Zichao Y ang, Aaron Li, Fei Xia, David G. Andersen, and Alexander Smola, “Parameter Server\\nfor Distributed Machine Learning” (NIPS Workshop on Big Learning, Lake Tahoe, CA, 2013), https://oreil.ly/\\nxMmru.\\n5 Jonathan Raiman, Susan Zhang, and Christy Dennison, “Neural Network Surgery with Sets, ” arXiv, December\\n13, 2019, https://oreil.ly/SU0F1.\\nStateful training doesn’t mean no training from scratch. The companies that have\\nmost successfully used stateful training also occasionally train their model from\\nscratch on a large amount of data to calibrate it. Alternatively, they might also train\\ntheir model from scratch in parallel with stateful training and then combine both\\nupdated models using techniques such as parameter server.4\\nOnce your infrastructure is set up to allow both stateless retraining and stateful\\ntraining, the training frequency is just a knob to twist. Y ou can update your models\\nonce an hour, once a day, or whenever a distribution shift is detected. How to find the\\noptimal retraining schedule will be discussed in the section “How Often to Update\\nY our Models” on page 279.\\nContinual learning is about setting up infrastructure in a way that allows you, a data\\nscientist or ML engineer, to update your models whenever it is needed, whether from\\nscratch or fine-tuning, and to deploy this update quickly.\\nY ou might wonder: stateful training sounds cool, but how does this work if I want to\\nadd a new feature or another layer to my model? To answer this, we must differentiate\\ntwo types of model updates:\\nModel iteration\\nA new feature is added to an existing model architecture or the model architec‐\\nture is changed.\\nData iteration\\nThe model architecture and features remain the same, but you refresh this model\\nwith new data.\\nAs of today, stateful training is mostly applied for data iteration, as changing your\\nmodel architecture or adding a new feature still requires training the resulting model\\nfrom scratch. There has been research showing that it might be possible to bypass\\ntraining from scratch for model iteration by using techniques such as knowledge\\ntransfer (Google, 2015) and model surgery  (OpenAI, 2019). According to OpenAI,\\n“Surgery transfers trained weights from one network to another after a selection\\nprocess to determine which sections of the model are unchanged and which must be\\nre-initialized. ”5 Several large research labs have experimented with this; however, I’m\\nnot aware of any clear results in the industry.', '“Surgery transfers trained weights from one network to another after a selection\\nprocess to determine which sections of the model are unchanged and which must be\\nre-initialized. ”5 Several large research labs have experimented with this; however, I’m\\nnot aware of any clear results in the industry.\\nContinual Learning | 267', '6 This type of problem is also called “dynamic pricing. ”\\nTerminology Ambiguity\\nI use the term “continual learning” instead of “online learning” because when I\\nsay “online learning, ” people usually think of online education. If you enter “online\\nlearning” on Google, the top results will likely be about online courses.\\nSome people use “online learning” to refer to the specific setting where a model learns\\nfrom each incoming new sample. In this setting, continual learning is a generalization\\nof online learning.\\nI also use the term “continual learning” instead of “continuous learning. ” Continuous\\nlearning refers to the regime in which your model continuously learns with each\\nincoming sample, whereas with continual learning, the learning is done in a series of\\nbatches or micro-batches.\\nContinuous learning is sometimes used to refer to continuous delivery of ML, which\\nis closely related to continual learning as both help companies to speed up the itera‐\\ntion cycle of their ML models. However, the difference is that “continuous learning, ”\\nwhen used in this sense, is from the DevOps perspective about setting up the pipeline\\nfor continuous delivery, whereas “continual learning” is from the ML perspective.\\nDue to the ambiguity of the term “continuous learning, ” I hope that the community\\ncan stay away from this term altogether.\\nWhy Continual Learning?\\nWe discussed that continual learning is about setting up infrastructure so that you can\\nupdate your models and deploy these changes as fast as you want. But why would you\\nneed the ability to update your models as fast as you want?\\nThe first use case of continual learning is to combat data distribution shifts, especially\\nwhen the shifts happen suddenly. Imagine you’re building a model to determine\\nthe prices for a ride-sharing service like Lyft. 6 Historically, the ride demand on a\\nThursday evening in this particular neighborhood is slow, so the model predicts low\\nride prices, which makes it less appealing for drivers to get on the road. However,\\non this Thursday evening, there’s a big event in the neighborhood, and suddenly the\\nride demand surges. If your model can’t respond to this change quickly enough by\\nincreasing its price prediction and mobilizing more drivers to that neighborhood,\\nriders will have to wait a long time for a ride, which causes negative user experience.\\nThey might even switch to a competitor, which causes you to lose revenue.', 'increasing its price prediction and mobilizing more drivers to that neighborhood,\\nriders will have to wait a long time for a ride, which causes negative user experience.\\nThey might even switch to a competitor, which causes you to lose revenue.\\nAnother use case of continual learning is to adapt to rare events. Imagine you work\\nfor an ecommerce website like Amazon. Black Friday is an important shopping event\\n268 | Chapter 9: Continual Learning and Test in Production', '7 Jon Russell, “ Alibaba Acquires German Big Data Startup Data Artisans for $103M, ” TechCrunch, January\\n8, 2019, https://oreil.ly/4tf5c. An early reviewer mentioned that it’s also possible that the main goal of this\\nacquisition was to increase Alibaba’s open source footprint, which is tiny compared to other tech giants.\\n8 The problem is also equally challenging if you want your model to figure out when to recommend a new\\nmovie that no one has watched and given feedback on yet.\\n9 Lucas Bernardi, Jaap Kamps, Julia Kiseleva, and Melanie J. I. Müller, “The Continuous Cold Start Problem in\\ne-Commerce Recommender Systems, ” arXiv, August 5, 2015, https://oreil.ly/GWUyD.\\n10 Jacopo Tagliabue, Ciro Greco, Jean-Francis Roy, Bingqing Yu, Patrick John Chia, Federico Bianchi,\\nand Giovanni Cassani, “SIGIR 2021 E-Commerce Workshop Data Challenge, ” arXiv, April 19, 2021,\\nhttps://oreil.ly/8QxmS.\\nthat happens only once a year. There’s no way you will be able to gather enough\\nhistorical data for your model to be able to make accurate predictions on how your\\ncustomers will behave throughout Black Friday this year. To improve performance,\\nyour model should learn throughout the day with fresh data. In 2019, Alibaba\\nacquired Data Artisans, the team leading the development of the stream processing\\nframework Apache Flink, for $103 million so that the team could help them adapt\\nFlink for ML use cases. 7 Their flagship use case was making better recommendations\\non Singles Day, a shopping occasion in China similar to Black Friday in the US.\\nA huge challenge for ML production today that continual learning can help overcome\\nis the continuous cold start problem. The cold start problem arises when your model\\nhas to make predictions for a new user without any historical data. For example, to\\nrecommend to a user what movies they might want to watch next, a recommender\\nsystem often needs to know what that user has watched before. But if that user is new,\\nyou won’t have their watch history and will have to generate them something generic,\\ne.g., the most popular movies on your site right now.8\\nContinuous cold start is a generalization of the cold start problem, 9 as it can happen\\nnot just with new users but also with existing users. For example, it can happen\\nbecause an existing user switches from a laptop to a mobile phone, and their behavior\\non a phone is different from their behavior on a laptop. It can happen because users', 'not just with new users but also with existing users. For example, it can happen\\nbecause an existing user switches from a laptop to a mobile phone, and their behavior\\non a phone is different from their behavior on a laptop. It can happen because users\\nare not logged in—most news sites don’t require readers to log in to read.\\nIt can also happen when a user visits a service so infrequently that whatever historical\\ndata the service has about this user is outdated. For example, most people only book\\nhotels and flights a few times a year. Coveo, a company that provides search engine\\nand recommender systems to ecommerce websites, found that it is common for an\\necommerce site to have more than 70% of their shoppers visit their site less than\\nthree times a year.10\\nIf your model doesn’t adapt quickly enough, it won’t be able to make recommenda‐\\ntions relevant to these users until the next time the model is updated. By that time,\\nthese users might have already left the service because they don’t find anything\\nrelevant to them.\\nContinual Learning | 269', '11 Catherine Wang, “Why TikTok Made Its User So Obsessive? The AI Algorithm That Got Y ou Hooked, ”\\nTowards Data Science, June 7, 2020, https://oreil.ly/BDWf8.\\n12 See the section “Data Passing Through Real-Time Transport” on page 74.\\nIf we could make our models adapt to each user within their visiting session, the\\nmodels would be able to make accurate, relevant predictions to users even on their\\nfirst visit. TikTok, for example, has successfully applied continual learning to adapt\\ntheir recommender system to each user within minutes. Y ou download the app and,\\nafter a few videos, TikTok’s algorithms are able to predict with high accuracy what\\nyou want to watch next. 11 I don’t think everyone should try to build something\\nas addictive as TikTok, but it’s proof that continual learning can unlock powerful\\npredictive potential.\\n“Why continual learning?” should be rephrased as “why not continual learning?”\\nContinual learning is a superset of batch learning, as it allows you to do everything\\nthe traditional batch learning can do. But continual learning also allows you to unlock\\nuse cases that batch learning can’t.\\nIf continual learning takes the same effort to set up and costs the same to do as batch\\nlearning, there’s no reason not to do continual learning. As of writing this book, there\\nare still a lot of challenges in setting up continual learning, as we’ll go deeper into in\\nthe following section. However, MLOps tooling for continual learning is maturing,\\nwhich means, one day not too far in the future, it might be as easy to set up continual\\nlearning as batch learning.\\nContinual Learning Challenges\\nEven though continual learning has many use cases and many companies have\\napplied it with great success, continual learning still has many challenges. In this\\nsection, we’ll discuss three major challenges: fresh data access, evaluation, and\\nalgorithms.\\nFresh data access challenge\\nThe first challenge is the challenge to get fresh data. If you want to update your\\nmodel every hour, you need new data every hour. Currently, many companies pull\\nnew training data from their data warehouses. The speed at which you can pull data\\nfrom your data warehouses depends on the speed at which this data is deposited into\\nyour data warehouses. The speed can be slow, especially if data comes from multiple\\nsources. An alternative is to allow pull data before it’s deposited into data warehouses,\\ne.g., directly from real-time transports such as Kafka and Kinesis that transport data', 'your data warehouses. The speed can be slow, especially if data comes from multiple\\nsources. An alternative is to allow pull data before it’s deposited into data warehouses,\\ne.g., directly from real-time transports such as Kafka and Kinesis that transport data\\nfrom applications to data warehouses,12 as shown in Figure 9-3.\\n270 | Chapter 9: Continual Learning and Test in Production', 'Figure 9-3. Pulling data directly from real-time transports, before it’s deposited into data\\nwarehouses, can allow you to access fresher data\\nBeing able to pull fresh data isn’t enough. If your model needs labeled data to\\nupdate, as most models today do, this data will need to be labeled as well. In many\\napplications, the speed at which a model can be updated is bottlenecked by the speed\\nat which data is labeled.\\nThe best candidates for continual learning are tasks where you can get natural labels\\nwith short feedback loops. Examples of these tasks are dynamic pricing (based on\\nestimated demand and availability), estimating time of arrival, stock price prediction,\\nads click-through prediction, and recommender systems for online content like\\ntweets, songs, short videos, articles, etc.\\nHowever, these natural labels are usually not generated as labels, but rather as behav‐\\nioral activities that need to be extracted into labels. Let’s walk through an example\\nto make this clear. If you run an ecommerce website, your application might register\\nthat at 10:33 p.m., user A clicks on the product with the ID of 32345. Y our system\\nneeds to look back into the logs to see if this product ID was ever recommended to\\nthis user, and if yes, then what query prompted this recommendation, so that your\\nsystem can match this query to this recommendation and label this recommendation\\nas a good recommendation, as shown in Figure 9-4.\\nFigure 9-4. A simplification of the process of extracting labels from user feedback\\nThe process of looking back into the logs to extract labels is called label computation.\\nIt can be quite costly if the number of logs is large. Label computation can be done\\nwith batch processing: e.g., waiting for logs to be deposited into data warehouses\\nContinual Learning | 271', '13 See the section “Batch Processing Versus Stream Processing” on page 78.\\n14 Tyler Akidau, “Snowflake Streaming: Now Hiring! Help Design and Build the Future of Big Data and Stream\\nProcessing, ” Snowflake blog, October 26, 2020, https://oreil.ly/Knh2Y.\\n15 Arjun Narayan, “Materialize Raises a $60M Series C, Bringing Total Funding to Over $100M, ” Materialize,\\nSeptember 30, 2021, https://oreil.ly/dqxRb.\\n16 Khristopher J. Brooks, “Disparity in Home Lending Costs Minorities Millions, Researchers Find, ” CBS News,\\nNovember 15, 2019, https://oreil.ly/SpZ1N; Lee Brown, “Tesla Driver Killed in Crash Posted Videos Driving\\nWithout His Hands on the Wheel, ” New York Post, May 16, 2021, https://oreil.ly/uku9S; “ A Tesla Driver Is\\nCharged in a Crash Involving Autopilot That Killed 2 People, ” NPR, January 18, 2022, https://oreil.ly/WWaRA.\\nfirst before running a batch job to extract all labels from logs at once. However, as\\ndiscussed previously, this means that we’ d need to wait for data to be deposited first,\\nthen wait for the next batch job to run. A much faster approach would be to leverage\\nstream processing to extract labels from the real-time transports directly.13\\nIf your model’s speed iteration is bottlenecked by labeling speed, it’s also possible to\\nspeed up the labeling process by leveraging programmatic labeling tools like Snorkel\\nto generate fast labels with minimal human intervention. It might also be possible to\\nleverage crowdsourced labels to quickly annotate fresh data.\\nGiven that tooling around streaming is still nascent, architecting an efficient\\nstreaming-first infrastructure for accessing fresh data and extracting fast labels from\\nreal-time transports can be engineering-intensive and costly. The good news is that\\ntooling around streaming is growing fast. Confluent, the platform built on top of\\nKafka, is a $16 billion company as of October 2021. In late 2020, Snowflake started a\\nteam focusing on streaming.14 As of September 2021, Materialize has raised $100 mil‐\\nlion to develop a streaming SQL database.15 As tooling around streaming matures, it’ll\\nbe much easier and cheaper for companies to develop a streaming-first infrastructure\\nfor ML.\\nEvaluation challenge\\nThe biggest challenge of continual learning isn’t in writing a function to continually\\nupdate your model—you can do that by writing a script! The biggest challenge is\\nin making sure that this update is good enough to be deployed. In this book, we’ve', 'for ML.\\nEvaluation challenge\\nThe biggest challenge of continual learning isn’t in writing a function to continually\\nupdate your model—you can do that by writing a script! The biggest challenge is\\nin making sure that this update is good enough to be deployed. In this book, we’ve\\ndiscussed how ML systems make catastrophic failures in production, from millions of\\nminorities being unjustly denied loans, to drivers who trust autopilot too much being\\ninvolved in fatal crashes.16\\nThe risks for catastrophic failures amplify with continual learning. First, the more\\nfrequently you update your models, the more opportunities there are for updates to\\nfail.\\nSecond, continual learning makes your models more susceptible to coordinated\\nmanipulation and adversarial attack. Because your models learn online from\\n272 | Chapter 9: Continual Learning and Test in Production', '17 James Vincent, “Twitter Taught Microsoft’s Friendly AI Chatbot to Be a Racist Asshole in Less Than a Day, ”\\nThe Verge, May 24, 2016, https://oreil.ly/NJEVF.\\n18 Their fraud detection system consists of multiple ML models.\\n19 In the section “Bandits” on page 287, we’ll learn about how bandits can be used as a more data-efficient\\nalternative to A/B testing.\\nreal-world data, it makes it easier for users to input malicious data to trick mod‐\\nels into learning wrong things. In 2016, Microsoft released Tay, a chatbot capable\\nof learning through “casual and playful conversation” on Twitter. As soon as Tay\\nlaunched, trolls started tweeting the bot racist and misogynist remarks. The bot soon\\nbegan to post inflammatory and offensive tweets, causing Microsoft to shut down the\\nbot 16 hours after its launch.17\\nTo avoid similar or worse incidents, it’s crucial to thoroughly test each of your model\\nupdates to ensure its performance and safety before deploying the updates to a\\nwider audience. We already discussed model offline evaluation in Chapter 6, and will\\ndiscuss online evaluation (test in production) in this chapter.\\nWhen designing the evaluation pipeline for continual learning, keep in mind that\\nevaluation takes time, which can be another bottleneck for model update frequency.\\nFor example, a major online payment company I worked with has an ML system\\nto detect fraudulent transactions. 18 The fraud patterns change quickly, so they’ d like\\nto update their system quickly to adapt to the changing patterns. They can’t deploy\\nthe new model before it’s been A/B tested against the current model. However, due\\nto the imbalanced nature of the task—most transactions aren’t fraud—it takes them\\napproximately two weeks to see enough fraud transactions to be able to accurately\\nassess which model is better.19 Therefore, they can only update their system every two\\nweeks.\\nAlgorithm challenge\\nCompared to the fresh data challenge and the evaluation, this is a “softer” challenge\\nas it only affects certain algorithms and certain training frequencies. To be precise,\\nit only affects matrix-based and tree-based models that want to be updated very fast\\n(e.g., hourly).\\nTo illustrate this point, consider two different models: a neural network and a matrix-\\nbased model, such as a collaborative filtering model. The collaborative filtering model\\nuses a user-item matrix and a dimension reduction technique.\\nY ou can update the neural network model with a data batch of any size. Y ou can even', 'based model, such as a collaborative filtering model. The collaborative filtering model\\nuses a user-item matrix and a dimension reduction technique.\\nY ou can update the neural network model with a data batch of any size. Y ou can even\\nperform the update step with just one data sample. However, if you want to update\\nthe collaborative filtering model, you first need to use the entire dataset to build the\\nuser-item matrix before performing dimensionality reduction on it. Of course, you\\ncan apply dimensionality reduction to your matrix each time you update the matrix\\nContinual Learning | 273', '20 Some people call this setting “learning with partial information, ” but learning with partial information refers\\nto another setting, as outlined in the paper “Subspace Learning with Partial Information” by Gonen et al.\\n(2016).\\n21 Pedro Domingos and Geoff Hulten, “Mining High-Speed Data Streams, ” in Proceedings of the Sixth\\nInternational Conference on Knowledge Discovery and Data Mining (Boston: ACM Press, 2000), 71–80;\\nAlbert Bifet and Ricard Gavaldà, “ Adaptive Parameter-free Learning from Evolving Data Streams, ” 2009,\\nhttps://oreil.ly/XIMpl.\\n22 Zohar Karnin, Kevin Lang, and Edo Liberty, “Optimal Quantile Approximation in Streams, ” arXiv, March 17,\\n2016, https://oreil.ly/bUu4H.\\nwith a new data sample, but if your matrix is large, the dimensionality reduction step\\nwould be too slow and expensive to perform frequently. Therefore, this model is less\\nsuitable for learning with a partial dataset than the preceding neural network model.20\\nIt’s much easier to adapt models like neural networks than matrix-based and tree-\\nbased models to the continual learning paradigm. However, there have been algo‐\\nrithms to create tree-based models that can learn from incremental amounts of data,\\nmost notably Hoeffding Tree and its variants Hoeffding Window Tree and Hoeffding\\nAdaptive Tree,21 but their uses aren’t yet widespread.\\nNot only does the learning algorithm need to work with partial datasets, but the\\nfeature extract code has to as well. We discussed in the section “Scaling” on page 126\\nthat it’s often necessary to scale your features using statistics such as the min, max,\\nmedian, and variance. To compute these statistics for a dataset, you often need to do\\na pass over the entire dataset. When your model can only see a small subset of data at\\na time, in theory, you can compute these statistics for each subset of data. However,\\nthis means that these statistics will fluctuate a lot between different subsets. The\\nstatistics computed from one subset might differ wildly from the next subset, making\\nit difficult for the model trained on one subset to generalize to the next subset.\\nTo keep these statistics stable across different subsets, you might want to compute\\nthese statistics online. Instead of using the mean or variance from all your data at\\nonce, you compute or approximate these statistics incrementally as you see new data,\\nsuch as the algorithms outlined in “Optimal Quantile Approximation in Streams. ” 22', 'these statistics online. Instead of using the mean or variance from all your data at\\nonce, you compute or approximate these statistics incrementally as you see new data,\\nsuch as the algorithms outlined in “Optimal Quantile Approximation in Streams. ” 22\\nPopular frameworks today offer some capacity for computing running statistics—for\\nexample, sklearn’s StandardScaler has a partial_fit that allows a feature scaler to be\\nused with running statistics—but the built-in methods are slow and don’t support a\\nwide range of running statistics.\\nFour Stages of Continual Learning\\nWe’ve discussed what continual learning is, why continual learning matters, and the\\nchallenges of continual learning. Next, we’ll discuss how to overcome these challenges\\nand make continual learning happen. As of the writing of this book, continual\\nlearning isn’t something that companies start out with. The move toward continual\\n274 | Chapter 9: Continual Learning and Test in Production', '23 We’ll cover ML platforms in the section “ML Platform” on page 319.\\nlearning happens in four stages, as outlined next. We’ll go over what happens in each\\nstage as well as the requirements necessary to move from a previous stage to this\\nstage.\\nStage 1: Manual, stateless retraining\\nIn the beginning, the ML team often focuses on developing ML models to solve as\\nmany business problems as possible. For example, if your company is an ecommerce\\nwebsite, you might develop four models in the following succession:\\n1. A model to detect fraudulent transactions1.\\n2. A model to recommend relevant products to users2.\\n3. A model to predict whether a seller is abusing a system3.\\n4. A model to predict how long it will take to ship an order4.\\nBecause your team is focusing on developing new models, updating existing models\\ntakes a backseat. Y ou update an existing model only when the following two condi‐\\ntions are met: the model’s performance has degraded to the point that it’s doing more\\nharm than good, and your team has time to update it. Some of your models are being\\nupdated once every six months. Some are being updated once a quarter. Some have\\nbeen out in the wild for a year and haven’t been updated at all.\\nThe process of updating a model is manual and ad hoc. Someone, usually a data\\nengineer, has to query the data warehouse for new data. Someone else cleans this new\\ndata, extracts features from it, retrains that model from scratch on both the old and\\nnew data, and then exports the updated model into a binary format. Then someone\\nelse takes that binary format and deploys the updated model. Oftentimes, the code\\nencapsulating data, features, and model logic was changed during the retraining\\nprocess but these changes failed to be replicated to production, causing bugs that are\\nhard to track down.\\nIf this process sounds painfully familiar to you, you’re not alone. A vast majority of\\ncompanies outside the tech industry—e.g., any company that adopted ML less than\\nthree years ago and doesn’t have an ML platform team—are in this stage.23\\nStage 2: Automated retraining\\nAfter a few years, your team has managed to deploy models to solve most of the\\nobvious problems. Y ou have anywhere between 5 and 10 models in production.\\nY our priority is no longer to develop new models, but to maintain and improve\\nexisting models. The ad hoc, manual process of updating models mentioned from the\\nContinual Learning | 275', '24 Y ou might need to train your embedding model more frequently if you have a lot of new items each day.\\nprevious stage has grown into a pain point too big to be ignored. Y our team decides\\nto write a script to automatically execute all the retraining steps. This script is then\\nrun periodically using a batch process such as Spark.\\nMost companies with somewhat mature ML infrastructure are in this stage. Some\\nsophisticated companies run experiments to determine the optimal retraining fre‐\\nquency. However, for most companies in this stage, the retraining frequency is set\\nbased on gut feeling—e.g., “once a day seems about right” or “let’s kick off the\\nretraining process each night when we have idle compute. ”\\nWhen creating scripts to automate the retraining process for your system, you need\\nto take into account that different models in your system might require different\\nretraining schedules. For example, consider a recommender system that consists of\\ntwo models: one model to generate embeddings for all products, and another model\\nto rank the relevance of each product given a query. The embedding model might\\nneed to be retrained a lot less frequently than the ranking model. Because products’\\ncharacteristics don’t change that often, you might be able to get away with retraining\\nyour embeddings once a week, 24 whereas your ranking models might need to be\\nretrained once a day.\\nThe automating script might get even more complicated if there are dependencies\\namong your models. For example, because the ranking model depends on the embed‐\\ndings, when the embeddings change, the ranking model should be updated too.\\nRequirements.    If your company has ML models in production, it’s likely that your\\ncompany already has most of the infrastructure pieces needed for automated retrain‐\\ning. The feasibility of this stage revolves around the feasibility of writing a script to\\nautomate your workflow and configure your infrastructure to automatically:\\n1. Pull data.1.\\n2. Downsample or upsample this data if necessary.2.\\n3. Extract features.3.\\n4. Process and/or annotate labels to create training data.4.\\n5. Kick off the training process.5.\\n6. Evaluate the newly trained model.6.\\n7. Deploy it.7.\\nHow long it will take to write this script depends on many factors, including the\\nscript writer’s competency. However, in general, the three major factors that will affect\\nthe feasibility of this script are: scheduler, data, and model store.', '6. Evaluate the newly trained model.6.\\n7. Deploy it.7.\\nHow long it will take to write this script depends on many factors, including the\\nscript writer’s competency. However, in general, the three major factors that will affect\\nthe feasibility of this script are: scheduler, data, and model store.\\n276 | Chapter 9: Continual Learning and Test in Production', 'A scheduler is basically a tool that handles task scheduling, which we’ll cover in the\\nsection “Cron, Schedulers, and Orchestrators” on page 311. If you don’t already have\\na scheduler, you’ll need time to set up one. However, if you already have a scheduler\\nsuch as Airflow or Argo, wiring the scripts together shouldn’t be that hard.\\nThe second factor is the availability and accessibility of your data. Do you need\\nto gather data yourself into your data warehouse? Will you have to join data from\\nmultiple organizations? Do you need to extract a lot of features from scratch? Will\\nyou also need to label your data? The more questions you answer yes to, the more\\ntime it will take to set up this script. Stefan Krawczyk, ML/data platform manager at\\nStitch Fix, commented that he suspects most people’s time might be spent here.\\nThe third factor you’ll need is a model store to automatically version and store all the\\nartifacts needed to reproduce a model. The simplest model store is probably just an\\nS3 bucket that stores serialized blobs of models in some structured manner. However,\\nblob storage like S3 is neither very good at versioning artifacts nor human-readable.\\nY ou might need a more mature model store like Amazon SageMaker (managed\\nservice) and Databricks’ MLflow (open source). We’ll go into detail on what a model\\nstore is and evaluate different model stores in the section “Model Store” on page 321.\\nFeature Reuse (Log and Wait)\\nWhen creating training data from new data to update your model,\\nremember that the new data has already gone through the predic‐\\ntion service. This prediction service has already extracted features\\nfrom this new data to input into models for predictions. Some\\ncompanies reuse these extracted features for model retraining,\\nwhich both saves computation and allows for consistency between\\nprediction and training. This approach is known as “log and wait. ”\\nIt’s a classic approach to reduce the train-serving skew discussed in\\nChapter 8 (see the section “Production data differing from training\\ndata” on page 229).\\nLog and wait isn’t yet a popular approach, but it’s getting more\\npopular. Faire has a great blog post discussing the pros and cons of\\ntheir “log and wait” approach.\\nStage 3: Automated, stateful training\\nIn stage 2, each time you retrain your model, you train it from scratch (stateless\\nretraining). It makes your retraining costly, especially for retraining with a higher', 'their “log and wait” approach.\\nStage 3: Automated, stateful training\\nIn stage 2, each time you retrain your model, you train it from scratch (stateless\\nretraining). It makes your retraining costly, especially for retraining with a higher\\nfrequency. Y ou read the section “Stateless Retraining Versus Stateful Training” on\\npage 265 and decide that you want to do stateful training—why train on data from\\nthe last three months every day when you can continue training using only data from\\nthe last day?\\nContinual Learning | 277', 'So in this stage, you reconfigure your automatic updating script so that, when the\\nmodel update is kicked off, it first locates the previous checkpoint and loads it into\\nmemory before continuing training on this checkpoint.\\nRequirements.    The main thing you need in this stage is a change in the mindset:\\nretraining from scratch is such a norm—many companies are so used to data scien‐\\ntists handing off a model to engineers to deploy from scratch each time—that many\\ncompanies don’t think about setting up their infrastructure to enable stateful training.\\nOnce you’re committed to stateful training, reconfiguring the updating script is\\nstraightforward. The main thing you need at this stage is a way to track your data and\\nmodel lineage. Imagine you first upload model version 1.0. This model is updated\\nwith new data to create model version 1.1, and so on to create model 1.2. Then\\nanother model is uploaded and called model version 2.0. This model is updated with\\nnew data to create model version 2.1. After a while, you might have model version\\n3.32, model version 2.11, model version 1.64. Y ou might want to know how these\\nmodels evolve over time, which model was used as its base model, and which data\\nwas used to update it so that you can reproduce and debug it. As far as I know, no\\nexisting model store has this model lineage capacity, so you’ll likely have to build the\\nsolution in-house.\\nIf you want to pull fresh data from the real-time transports instead of from data\\nwarehouses, as discussed in the section “Fresh data access challenge” on page 270,\\nand your streaming infrastructure isn’t mature enough, you might need to revamp\\nyour streaming pipeline.\\nStage 4: Continual learning\\nAt stage 3, your models are still updated based on a fixed schedule set out by\\ndevelopers. Finding the optimal schedule isn’t straightforward and can be situation-\\ndependent. For example, last week, nothing much happened in the market, so your\\nmodels didn’t decay that fast. However, this week, a lot of events happen, so your\\nmodels decay much faster and require a much faster retraining schedule.\\nInstead of relying on a fixed schedule, you might want your models to be auto‐\\nmatically updated whenever data distributions shift and the model’s performance\\nplummets.\\nThe holy grail is when you combine continual learning with edge deployment. Imag‐\\nine you can ship a base model with a new device—a phone, a watch, a drone, etc.—', 'matically updated whenever data distributions shift and the model’s performance\\nplummets.\\nThe holy grail is when you combine continual learning with edge deployment. Imag‐\\nine you can ship a base model with a new device—a phone, a watch, a drone, etc.—\\nand the model on that device will continually update and adapt to its environment as\\nneeded without having to sync with a centralized server. There will be no need for a\\ncentralized server, which means no centralized server cost. There will also be no need\\nto transfer data back and forth between device and cloud, which means better data\\nsecurity and privacy!\\n278 | Chapter 9: Continual Learning and Test in Production', 'Requirements.    The move from stage 3 to stage 4 is steep. Y ou’ll first need a mecha‐\\nnism to trigger model updates. This trigger can be:\\nTime-based\\nFor example, every five minutes\\nPerformance-based\\nFor example, whenever model performance plummets\\nVolume-based\\nFor example, whenever the total amount of labeled data increases by 5%\\nDrift-based\\nFor example, whenever a major data distribution shift is detected\\nFor this trigger mechanism to work, you’ll need a solid monitoring solution. We\\ndiscussed in the section “Monitoring and Observability” on page 250 that the hard\\npart is not to detect the changes, but to determine which of these changes matter.\\nIf your monitoring solution gives a lot of false alerts, your model will end up being\\nupdated much more frequently than it needs to be.\\nY ou’ll also need a solid pipeline to continually evaluate your model updates. Writing\\na function to update your models isn’t much different from what you’ d do in stage 3.\\nThe hard part is to ensure that the updated model is working properly. We’ll go over\\nvarious testing techniques you can use in the section “Test in Production” on page\\n281.\\nHow Often to Update Your Models\\nNow that your infrastructure has been set up to update a model quickly, you started\\nasking the question that has been haunting ML engineers at companies of all shapes\\nand sizes: “How often should I update my models?” Before attempting to answer that\\nquestion, we first need to figure out how much gain your model will get from being\\nupdated with fresh data. The more gain your model can get from fresher data, the\\nmore frequently it should be retrained.\\nValue of data freshness\\nThe question of how often to update a model becomes a lot easier if we know how\\nmuch the model performance will improve with updating. For example, if we switch\\nfrom retraining our model every month to every week, how much performance gain\\ncan we get? What if we switch to daily retraining? People keep saying that data\\ndistributions shift, so fresher data is better, but how much better is fresher data?\\nContinual Learning | 279', '25 Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Tanxin Shi, et al., “Practical Lessons from\\nPredicting Clicks on Ads at Facebook, ” in ADKDD ’14: Proceedings of the Eighth International Workshop on\\nData Mining for Online Advertising (August 2014): 1–9, https://oreil.ly/oS16J.\\n26 Qian Yu, “Machine Learning with Flink in Weibo, ” QCon 2019, video, 17:57, https://oreil.ly/Yia6v.\\nOne way to figure out the gain is by training your model on the data from different\\ntime windows in the past and evaluating it on the data from today to see how the\\nperformance changes. For example, consider that you have data from the year 2020.\\nTo measure the value of data freshness, you can experiment with training model\\nversion A on the data from January to June 2020, model version B on the data from\\nApril to September, and model version C on the data from June to November, then\\ntest each of these model versions on the data from December, as shown in Figure 9-5.\\nThe difference in the performance of these versions will give you a sense of the\\nperformance gain your model can get from fresher data. If the model trained on data\\nfrom a quarter ago is much worse than the model trained on data from a month ago,\\nyou know that you shouldn’t wait a quarter to retrain your model.\\nFigure 9-5. To get a sense of the performance gain you can get from fresher data, train\\nyour model on data from different time windows in the past and test on data from today\\nto see how the performance changes\\nThis is a simple example to illustrate how the data freshness experiment works. In\\npractice, you might want your experiments to be much more fine-grained, operating\\nnot in months but in weeks, days, even hours or minutes. In 2014, Facebook did\\na similar experiment for ad click-through-rate prediction and found out that they\\ncould reduce the model’s loss by 1% by going from retraining weekly to retraining\\ndaily, and this performance gain was significant enough for them to switch their\\nretraining pipeline from weekly to daily. 25 Given that online contents today are so\\nmuch more diverse and users’ attention online changes much faster, we can imagine\\nthat the value of data freshness for ad click-through rate is even higher. Some of\\nthe companies with sophisticated ML infrastructure have found enough performance\\ngain to switch their retraining pipeline to every few minutes.26\\n280 | Chapter 9: Continual Learning and Test in Production', 'Model iteration versus data iteration\\nWe discussed earlier in this chapter that not all model updates are the same. We\\ndifferentiated between model iteration (adding a new feature to an existing model\\narchitecture or changing the model architecture) and data iteration (same model\\narchitecture and features but you refresh this model with new data). Y ou might\\nwonder not only how often to update your model, but also what kind of model\\nupdates to perform.\\nIn theory, you can do both types of updates, and in practice, you should do both from\\ntime to time. However, the more resources you spend in one approach, the fewer\\nresources you can spend in another.\\nOn the one hand, if you find that iterating on your data doesn’t give you much\\nperformance gain, then you should spend your resources on finding a better model.\\nOn the other hand, if finding a better model architecture requires 100X compute for\\ntraining and gives you 1% performance whereas updating the same model on data\\nfrom the last three hours requires only 1X compute and also gives 1% performance\\ngain, you’ll be better off iterating on data.\\nMaybe in the near future, we’ll get more theoretical understanding to know in what\\nsituation an approach will work better (cue “call for research”), but as of today, no\\nbook can give you the answer on which approach will work better for your specific\\nmodel on your specific task. Y ou’ll have to do experiments to find out.\\nThe question on how often to update your model is a difficult one to answer, and I\\nhope that this section has sufficiently explained its nuances. In the beginning, when\\nyour infrastructure is nascent and the process of updating a model is manual and\\nslow, the answer is: as often as you can.\\nHowever, as your infrastructure matures and the process of updating a model is\\npartially automated and can be done in a matter of hours, if not minutes, the answer\\nto this question is contingent on the answer to the following question: “How much\\nperformance gain would I get from fresher data?” It’s important to run experiments to\\nquantify the value of data freshness to your models.\\nTest in Production\\nThroughout this book, including this chapter, we’ve talked about the danger of\\ndeploying models that haven’t been sufficiently evaluated. To sufficiently evaluate\\nyour models, you first need a mixture of offline evaluation discussed in Chapter 6\\nand online evaluation discussed in this section. To understand why offline evaluation', 'deploying models that haven’t been sufficiently evaluated. To sufficiently evaluate\\nyour models, you first need a mixture of offline evaluation discussed in Chapter 6\\nand online evaluation discussed in this section. To understand why offline evaluation\\nisn’t enough, let’s go over two major test types for offline evaluation: test splits and\\nbacktests.\\nTest in Production | 281', 'The first type of model evaluation you might think about is the good old test splits\\nthat you can use to evaluate your models offline, as discussed in Chapter 6. These test\\nsplits are usually static and have to be static so that you have a trusted benchmark to\\ncompare multiple models. It’ll be hard to compare the test results of two models if\\nthey are tested on different test sets.\\nHowever, if you update the model to adapt to a new data distribution, it’s not suffi‐\\ncient to evaluate this new model on test splits from the old distribution. Assuming\\nthat the fresher the data, the more likely it is to come from the current distribution,\\none idea is to test your model on the most recent data that you have access to. So,\\nafter you’ve updated your model on the data from the last day, you might want to\\ntest this model on the data from the last hour (assuming that data from the last hour\\nwasn’t included in the data used to update your model). The method of testing a\\npredictive model on data from a specific period of time in the past is known as a\\nbacktest.\\nThe question is whether backtests are sufficient to replace static test splits. Not quite.\\nIf something went wrong with your data pipeline and some data from the last hour is\\ncorrupted, evaluating your model solely on this recent data isn’t sufficient.\\nWith backtests, you should still evaluate your model on a static test set that you have\\nextensively studied and (mostly) trust as a form of sanity check.\\nBecause data distributions shift, the fact that a model does well on the data from\\nthe last hour doesn’t mean that it will continue doing well on the data in the future.\\nThe only way to know whether a model will do well in production is to deploy it.\\nThis insight led to one seemingly terrifying but necessary concept: test in production.\\nHowever, test in production doesn’t have to be scary. There are techniques to help\\nyou evaluate your models in production (mostly) safely. In this section, we’ll cover\\nthe following techniques: shadow deployment, A/B testing, canary analysis, interleav‐\\ning experiments, and bandits.\\nShadow Deployment\\nShadow deployment might be the safest way to deploy your model or any software\\nupdate. Shadow deployment works as follows:\\n1. Deploy the candidate model in parallel with the existing model.1.\\n2. For each incoming request, route it to both models to make predictions, but only2.\\nserve the existing model’s prediction to the user.', 'update. Shadow deployment works as follows:\\n1. Deploy the candidate model in parallel with the existing model.1.\\n2. For each incoming request, route it to both models to make predictions, but only2.\\nserve the existing model’s prediction to the user.\\n3. Log the predictions from the new model for analysis purposes.3.\\nOnly when you’ve found that the new model’s predictions are satisfactory do you\\nreplace the existing model with the new model.\\n282 | Chapter 9: Continual Learning and Test in Production', '27 Ron Kohavi and Stefan Thomke, “The Surprising Power of Online Experiments, ” Harvard Business Review,\\nSeptember–October 2017, https://oreil.ly/OHfj0.\\nBecause you don’t serve the new model’s predictions to users until you’ve made\\nsure that the model’s predictions are satisfactory, the risk of this new model doing\\nsomething funky is low, at least not higher than the existing model. However, this\\ntechnique isn’t always favorable because it’s expensive. It doubles the number of pre‐\\ndictions your system has to generate, which generally means doubling your inference\\ncompute cost.\\nA/B Testing\\nA/B testing is a way to compare two variants of an object, typically by testing respon‐\\nses to these two variants, and determining which of the two variants is more effective.\\nIn our case, we have the existing model as one variant, and the candidate model (the\\nrecently updated model) as another variant. We’ll use A/B testing to determine which\\nmodel is better according to some predefined metrics.\\nA/B testing has become so prevalent that, as of 2017, companies like Microsoft and\\nGoogle each conduct over 10,000 A/B tests annually. 27 It is many ML engineers’ first\\nresponse to how to evaluate ML models in production. A/B testing works as follows:\\n1. Deploy the candidate model alongside the existing model.1.\\n2. A percentage of traffic is routed to the new model for predictions; the rest2.\\nis routed to the existing model for predictions. It’s common for both variants\\nto serve prediction traffic at the same time. However, there are cases where\\none model’s predictions might affect another model’s predictions—e.g., in ride-\\nsharing’s dynamic pricing, a model’s predicted prices might influence the number\\nof available drivers and riders, which, in turn, influence the other model’s predic‐\\ntions. In those cases, you might have to run your variants alternatively, e.g., serve\\nmodel A one day and then serve model B the next day.\\n3. Monitor and analyze the predictions and user feedback, if any, from both models3.\\nto determine whether the difference in the two models’ performance is statisti‐\\ncally significant.\\nTo do A/B testing the right way requires doing many things right. In this book, we’ll\\ndiscuss two important things. First, A/B testing consists of a randomized experiment:\\nthe traffic routed to each model has to be truly random. If not, the test result will\\nbe invalid. For example, if there’s a selection bias in the way traffic is routed to the', 'discuss two important things. First, A/B testing consists of a randomized experiment:\\nthe traffic routed to each model has to be truly random. If not, the test result will\\nbe invalid. For example, if there’s a selection bias in the way traffic is routed to the\\ntwo models, such as users who are exposed to model A are usually on their phones\\nwhereas users exposed to model B are usually on their desktops, then if model A has\\nTest in Production | 283', 'better accuracy than model B, we can’t tell whether it’s because A is better than B or\\nwhether “being on a phone” influences the prediction quality.\\nSecond, your A/B test should be run on a sufficient number of samples to gain\\nenough confidence about the outcome. How to calculate the number of samples\\nneeded for an A/B test is a simple question with a very complicated answer, and I’ d\\nrecommend readers reference a book on A/B testing to learn more.\\nThe gist here is that if your A/B test result shows that a model is better than another\\nwith statistical significance, you can determine which model is indeed better. To\\nmeasure statistical significance, A/B testing uses statistical hypothesis testing such as\\ntwo-sample tests. We saw two-sample tests in Chapter 8 when we used them to detect\\ndistribution shifts. As a reminder, a two-sample test is a test to determine whether\\nthe difference between these two populations is statistically significant. In the distri‐\\nbution shift use case, if a statistical difference suggests that the two populations come\\nfrom different distributions, this means that the original distribution has shifted. In\\nthe A/B testing use case, statistical differences mean that we’ve gathered sufficient\\nevidence to show that one variant is better than the other variant.\\nStatistical significance, while useful, isn’t foolproof. Say we run a two-sample test and\\nget the result that model A is better than model B with the p-value of p = 0.05 or\\n5%, and we define statistical significance as p ≤ 0.5. This means that if we run the\\nsame A/B testing experiment multiple times, (100 – 5 =) 95% of the time, we’ll get the\\nresult that A is better than B, and the other 5% of the time, B is better than A. So even\\nif the result is statistically significant, it’s possible that if we run the experiment again,\\nwe’ll pick another model.\\nEven if your A/B test result isn’t statistically significant, it doesn’t mean that this A/B\\ntest fails. If you’ve run your A/B test with a lot of samples and the difference between\\nthe two tested models is statistically insignificant, maybe there isn’t much difference\\nbetween these two models, and it’s probably OK for you to use either.\\nFor readers interested in learning more about A/B testing and other statistical\\nconcepts important in ML, I recommend Ron Kohav’s book Trustworthy Online\\nControlled Experiments (A Practical Guide to A/B Testing)  (Cambridge University', 'For readers interested in learning more about A/B testing and other statistical\\nconcepts important in ML, I recommend Ron Kohav’s book Trustworthy Online\\nControlled Experiments (A Practical Guide to A/B Testing)  (Cambridge University\\nPress) and Michael Barber’s great introduction to statistics for data science  (much\\nshorter).\\nOften, in production, you don’t have just one candidate but multiple candidate\\nmodels. It’s possible to do A/B testing with more than two variants, which means we\\ncan have A/B/C testing or even A/B/C/D testing.\\n284 | Chapter 9: Continual Learning and Test in Production', '28 Danilo Sato, “CanaryRelease, ” June 25, 2014, MartinFowler.com, https://oreil.ly/YtKJE.\\nCanary Release\\nCanary release is a technique to reduce the risk of introducing a new software version\\nin production by slowly rolling out the change to a small subset of users before\\nrolling it out to the entire infrastructure and making it available to everybody.28 In the\\ncontext of ML deployment, canary release works as follows:\\n1. Deploy the candidate model alongside the existing model. The candidate model1.\\nis called the canary.\\n2. A portion of the traffic is routed to the candidate model.2.\\n3. If its performance is satisfactory, increase the traffic to the candidate model. If3.\\nnot, abort the canary and route all the traffic back to the existing model.\\n4. Stop when either the canary serves all the traffic (the candidate model has4.\\nreplaced the existing model) or when the canary is aborted.\\nThe candidate model’s performance is measured against the existing model’s perfor‐\\nmance according to the metrics you care about. If the candidate model’s key metrics\\ndegrade significantly, the canary is aborted and all the traffic will be routed to the\\nexisting model.\\nCanary releases can be used to implement A/B testing due to the similarities in their\\nsetups. However, you can do canary analysis without A/B testing. For example, you\\ndon’t have to randomize the traffic to route to each model. A plausible scenario is that\\nyou first roll out the candidate model to a less critical market before rolling out to\\neverybody.\\nFor readers interested in how canary release works in the industry, Netflix and\\nGoogle have a great shared blog post  on how automated canary analysis is used at\\ntheir companies.\\nInterleaving Experiments\\nImagine you have two recommender systems, A and B, and you want to evaluate\\nwhich one is better. Each time, a model recommends 10 items users might like. With\\nA/B testing, you’ d divide your users into two groups: one group is exposed to A and\\nthe other group is exposed to B. Each user will be exposed to the recommendations\\nmade by one model.\\nWhat if instead of exposing a user to recommendations from a model, we expose\\nthat user to recommendations from both models and see which model’s recommen‐\\ndations they will click on? That’s the idea behind interleaving experiments, originally\\nTest in Production | 285', '29 Thorsten Joachims, “Optimizing Search Engines using Clickthrough Data, ” KDD 2002, https://oreil.ly/XnH5G.\\n30 Joshua Parks, Juliette Aurisset, and Michael Ramm, “Innovating Faster on Personalization Algorithms at\\nNetflix Using Interleaving, ” Netflix Technology Blog, November 29, 2017, https://oreil.ly/lnvDY.\\nproposed by Thorsten Joachims in 2002 for the problems of search rankings. 29 In\\nexperiments, Netflix found that interleaving “reliably identifies the best algorithms\\nwith considerably smaller sample size compared to traditional A/B testing. ”30\\nFigure 9-6  shows how interleaving differs from A/B testing. In A/B testing, core\\nmetrics like retention and streaming are measured and compared between the two\\ngroups. In interleaving, the two algorithms can be compared by measuring user pref‐\\nerences. Because interleaving can be decided by user preferences, there’s no guarantee\\nthat user preference will lead to better core metrics.\\nFigure 9-6. An illustration of interleaving versus A/B testing. Source: Adapted from an\\nimage by Parks et al.\\nWhen we show recommendations from multiple models to users, it’s important to\\nnote that the position of a recommendation influences how likely a user will click\\non it. For example, users are much more likely to click on the top recommendation\\nthan the bottom recommendation. For interleaving to yield valid results, we must\\nensure that at any given position, a recommendation is equally likely to be gener‐\\nated by A or B. To ensure this, one method we can use is team-draft interleaving,\\nwhich mimics the drafting process in sports. For each recommendation position,\\nwe randomly select A or B with equal probability, and the chosen model picks the\\n286 | Chapter 9: Continual Learning and Test in Production', '31 Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue, “Large-Scale Validation and Analysis\\nof Interleaved Search Evaluation, ” ACM Transactions on Information Systems 30, no. 1 (February 2012): 6,\\nhttps://oreil.ly/lccvK.\\n32 Parks et al., “Innovating Faster on Personalization Algorithms. ”\\ntop recommendation that hasn’t already been picked. 31 A visualization of how this\\nteam-drafting method works is shown in Figure 9-7.\\nFigure 9-7. Interleaving video recommendations from two ranking algorithms using\\nteam draft. Source: Parks et al.32\\nBandits\\nFor those unfamiliar, bandit algorithms originated in gambling. A casino has multiple\\nslot machines with different payouts. A slot machine is also known as a one-armed\\nbandit, hence the name. Y ou don’t know which slot machine gives the highest payout.\\nY ou can experiment over time to find out which slot machine is the best while maxi‐\\nmizing your payout. Multi-armed bandits are algorithms that allow you to balance\\nbetween exploitation (choosing the slot machine that has paid the most in the past)\\nand exploration (choosing other slot machines that may pay off even more).\\nTest in Production | 287', '33 Greg Rafferty, “ A/B Testing—Is There a Better Way? An Exploration of Multi-Armed Bandits, ” Towards Data\\nScience, January 22, 2020, https://oreil.ly/MsaAK.\\nAs of today, the standard method for testing models in production is A/B testing.\\nWith A/B testing, you randomly route traffic to each model for predictions and\\nmeasure at the end of your trial which model works better. A/B testing is stateless:\\nyou can route traffic to each model without having to know about their current\\nperformance. Y ou can do A/B testing even with batch prediction.\\nWhen you have multiple models to evaluate, each model can be considered a slot\\nmachine whose payout (i.e., prediction accuracy) you don’t know. Bandits allow you\\nto determine how to route traffic to each model for prediction to determine the best\\nmodel while maximizing prediction accuracy for your users. Bandit is stateful: before\\nrouting a request to a model, you need to calculate all models’ current performance.\\nThis requires three things:\\n• Y our model must be able to make online predictions.•\\n• Preferably short feedback loops: you need to get feedback on whether a predic‐•\\ntion is good or not. This is usually true for tasks where labels can be determined\\nfrom users’ feedback, like in recommendations—if users click on a recommenda‐\\ntion, it’s inferred to be good. If the feedback loops are short, you can update the\\npayoff of each model quickly.\\n• A mechanism to collect feedback, calculate and keep track of each model’s•\\nperformance, and route prediction requests to different models based on their\\ncurrent performance.\\nBandits are well-studied in academia and have been shown to be a lot more data-\\nefficient than A/B testing (in many cases, bandits are even optimal). Bandits require\\nless data to determine which model is the best and, at the same time, reduce opportu‐\\nnity cost as they route traffic to the better model more quickly. See discussions on\\nbandits at LinkedIn, Netflix, Facebook, and Dropbox , Zillow, and Stitch Fix . For a\\nmore theoretical view, see Chapter 2 of Reinforcement Learning (Sutton and Barto\\n2020).\\nIn an experiment by Google’s Greg Rafferty, A/B testing required over 630,000\\nsamples to get a confidence interval of 95%, whereas a simple bandit algorithm\\n(Thompson Sampling) determined that a model was 5% better than the other with\\nless than 12,000 samples.33\\nHowever, bandits are a lot more difficult to implement than A/B testing because', 'samples to get a confidence interval of 95%, whereas a simple bandit algorithm\\n(Thompson Sampling) determined that a model was 5% better than the other with\\nless than 12,000 samples.33\\nHowever, bandits are a lot more difficult to implement than A/B testing because\\nit requires computing and keeping track of models’ payoffs. Therefore, bandit algo‐\\nrithms are not widely used in the industry other than at a few big tech companies.\\n288 | Chapter 9: Continual Learning and Test in Production', '34 William R. Thompson, “On the Likelihood that One Unknown Probability Exceeds Another in View of the\\nEvidence of Two Samples, ” Biometrika 25, no. 3/4 (December 1933): 285–94, https://oreil.ly/TH1HC.\\n35 Peter Auer, “Using Confidence Bounds for Exploitation–Exploration Trade-offs, ” Journal of Machine Learning\\nResearch 3 (November 2002): 397–422, https://oreil.ly/vp9mI.\\nBandit Algorithms\\nMany of the solutions for the multi-armed bandit problem can be used here. The\\nsimplest algorithm for exploration is ε-greedy. For a percentage of time, say 90% of\\nthe time or ε = 0.9, you route traffic to the model that is currently the best-performing\\none, and for the other 10% of the time, you route traffic to a random model. This\\nmeans that for each of the predictions your system generates, 90% of them come from\\nthe best-at-that-point-in-time model.\\nTwo of the most popular exploration algorithms are Thompson Sampling and Upper\\nConfidence Bound (UCB). Thompson Sampling selects a model with a probability\\nthat this model is optimal given the current knowledge. 34 In our case, it means that\\nthe algorithm selects the model based on its probability of having a higher value\\n(better performance) than all other models. On the other hand, UCB selects the item\\nwith the highest upper confidence bound.35 We say that UCB implements optimism in\\nthe face of uncertainty, it gives an “uncertainty bonus, ” also called “exploration bonus, ”\\nto the items it’s uncertain about.\\nContextual bandits as an exploration strategy\\nIf bandits for model evaluation are to determine the payout (i.e., prediction accuracy)\\nof each model, contextual bandits are to determine the payout of each action. In\\nthe case of recommendations/ads, an action is an item/ad to show to users, and the\\npayout is how likely it is a user will click on it. Contextual bandits, like other bandits,\\nare an amazing technique to improve the data efficiency of your model.\\nSome people also call bandits for model evaluation “contextual\\nbandits. ” This makes conversations confusing, so in this book,\\n“contextual bandits” refer to exploration strategies to determine the\\npayout of predictions.\\nImagine that you’re building a recommender system with 1,000 items to recommend,\\nwhich makes it a 1,000-arm bandit problem. Each time, you can only recommend the\\ntop 10 most relevant items to a user. In bandit terms, you’ll have to choose the best\\n10 arms. The shown items get user feedback, inferred via whether the user clicks on', 'which makes it a 1,000-arm bandit problem. Each time, you can only recommend the\\ntop 10 most relevant items to a user. In bandit terms, you’ll have to choose the best\\n10 arms. The shown items get user feedback, inferred via whether the user clicks on\\nthem. But you won’t get feedback on the other 990 items. This is known as the partial\\nTest in Production | 289', '36 Lihong Li, Wei Chu, John Langford, and Robert E. Schapire, “ A Contextual-Bandit Approach to Personalized\\nNews Article Recommendation, ” arXiv, February 28, 2010, https://oreil.ly/uaWHm.\\n37 According to Wikipedia, multi-armed bandit is a classic reinforcement learning problem that exemplifies\\nthe exploration–exploitation trade-off dilemma (s.v., “Multi-armed bandit, ” https://oreil.ly/ySjwo). The name\\ncomes from imagining a gambler at a row of slot machines (sometimes known as “one-armed bandits”) who\\nhas to decide which machines to play, how many times to play each machine and in which order to play them,\\nand whether to continue with the current machine or try a different machine.\\nfeedback problem, also known as bandit feedback. Y ou can also think of contextual\\nbandits as a classification problem with bandit feedback.\\nLet’s say that each time a user clicks on an item, this item gets 1 value point. When an\\nitem has 0 value points, it could either be because the item has never been shown to a\\nuser, or because it’s been shown but not clicked on. Y ou want to show users the items\\nwith the highest value to them, but if you keep showing users only the items with\\nthe most value points, you’ll keep on recommending the same popular items, and the\\nnever-before-shown items will keep having 0 value points.\\nContextual bandits are algorithms that help you balance between showing users the\\nitems they will like and showing the items that you want feedback on. 36 It’s the same\\nexploration–exploitation trade-off that many readers might have encountered in\\nreinforcement learning. Contextual bandits are also called “one-shot” reinforcement\\nlearning problems. 37 In reinforcement learning, you might need to take a series of\\nactions before seeing the rewards. In contextual bandits, you can get bandit feedback\\nright away after an action—e.g., after recommending an ad, you get feedback on\\nwhether a user has clicked on that recommendation.\\nContextual bandits are well researched and have been shown to improve models’\\nperformance significantly (see reports by Twitter and Google). However, contextual\\nbandits are even harder to implement than model bandits, since the exploration\\nstrategy depends on the ML model’s architecture (e.g., whether it’s a decision tree\\nor a neural network), which makes it less generalizable across use cases. Readers\\ninterested in combining contextual bandits with deep learning should check out a', 'strategy depends on the ML model’s architecture (e.g., whether it’s a decision tree\\nor a neural network), which makes it less generalizable across use cases. Readers\\ninterested in combining contextual bandits with deep learning should check out a\\ngreat paper written by a team at Twitter: “Deep Bayesian Bandits: Exploring in Online\\nPersonalized Recommendations” (Guo et al. 2020).\\nBefore we wrap up this section, there’s one point I want to emphasize. We’ve gone\\nthrough multiple types of tests for ML models. However, it’s important to note\\nthat a good evaluation pipeline is not only about what tests to run, but also about\\nwho should run those tests. In ML, the evaluation process is often owned by data\\nscientists—the same people who developed the model are responsible for evaluating\\nit. Data scientists tend to evaluate their new model ad hoc using the sets of tests that\\nthey like. First, this process is imbued with biases—data scientists have contexts about\\ntheir models that most users don’t, which means they probably won’t use this model\\nin a way most of their users will. Second, the ad hoc nature of the process means\\n290 | Chapter 9: Continual Learning and Test in Production', 'that the results might be variable. One data scientist might perform a set of tests and\\nfind that model A is better than model B, while another data scientist might report\\ndifferently.\\nThe lack of a way to ensure models’ quality in production has led to many mod‐\\nels failing after being deployed, which, in turn, fuels data scientists’ anxiety when\\ndeploying models. To mitigate this issue, it’s important for each team to outline clear\\npipelines on how models should be evaluated: e.g., the tests to run, the order in which\\nthey should run, the thresholds they must pass in order to be promoted to the next\\nstage. Better, these pipelines should be automated and kicked off whenever there’s a\\nnew model update. The results should be reported and reviewed, similar to the con‐\\ntinuous integration/continuous deployment (CI/CD) process for traditional software\\nengineering. It’s crucial to understand that a good evaluation process involves not\\nonly what tests to run but also who should run those tests.\\nSummary\\nThis chapter touches on a topic that I believe is among the most exciting yet under‐\\nexplored topics: how to continually update your models in production to adapt\\nthem to changing data distributions. We discussed the four stages a company might\\ngo through in the process of modernizing their infrastructure for continual learn‐\\ning: from the manual, training from scratch stage to automated, stateless continual\\nlearning.\\nWe then examined the question that haunts ML engineers at companies of all shapes\\nand sizes, “How often should I update my models?” by urging them to consider the\\nvalue of data freshness to their models and the trade-offs between model iteration\\nand data iteration.\\nSimilar to online prediction discussed in Chapter 7 , continual learning requires\\na mature streaming infrastructure. The training part of continual learning can be\\ndone in batch, but the online evaluation part requires streaming. Many engineers\\nworry that streaming is hard and costly. It was true three years ago, but streaming\\ntechnologies have matured significantly since then. More and more companies are\\nproviding solutions to make it easier for companies to move to streaming, including\\nSpark Streaming, Snowflake Streaming, Materialize, Decodable, Vectorize, etc.\\nContinual learning is a problem specific to ML, but it largely requires an infrastruc‐\\ntural solution. To be able to speed up the iteration cycle and detect failures in new', 'Spark Streaming, Snowflake Streaming, Materialize, Decodable, Vectorize, etc.\\nContinual learning is a problem specific to ML, but it largely requires an infrastruc‐\\ntural solution. To be able to speed up the iteration cycle and detect failures in new\\nmodel updates quickly, we need to set up our infrastructure in the right way. This\\nrequires the data science/ML team and the platform team to work together. We’ll\\ndiscuss infrastructure for ML in the next chapter.\\nSummary | 291', 'CHAPTER 10\\nInfrastructure and Tooling for MLOps\\nIn Chapters 4 to 6, we discussed the logic for developing ML systems. In Chapters\\n7 to 9, we discussed the considerations for deploying, monitoring, and continually\\nupdating an ML system. Up until now, we’ve assumed that ML practitioners have\\naccess to all the tools and infrastructure they need to implement that logic and carry\\nout these considerations. However, that assumption is far from being true. Many data\\nscientists have told me that they know the right things to do for their ML systems, but\\nthey can’t do them because their infrastructure isn’t set up in a way that enables them\\nto do so.\\nML systems are complex. The more complex a system, the more it can benefit from\\ngood infrastructure. Infrastructure, when set up right, can help automate processes,\\nreducing the need for specialized knowledge and engineering time. This, in turn, can\\nspeed up the development and delivery of ML applications, reduce the surface area\\nfor bugs, and enable new use cases. When set up wrong, however, infrastructure is\\npainful to use and expensive to replace. In this chapter, we’ll discuss how to set up\\ninfrastructure right for ML systems.\\nBefore we dive in, it’s important to note that every company’s infrastructure needs are\\ndifferent. The infrastructure required for you depends on the number of applications\\nyou develop and how specialized the applications are. At one end of the spectrum,\\nyou have companies that use ML for ad hoc business analytics such as to project\\nthe number of new users they’ll have next year to present at their quarterly planning\\nmeeting. These companies probably won’t need to invest in any infrastructure—\\nJupyter Notebooks, Python, and Pandas would be their best friends. If you have\\nonly one simple ML use case, such as an Android app for object detection to show\\nyour friends, you probably won’t need any infrastructure either—you just need an\\nAndroid-compatible ML framework like TensorFlow Lite.\\n293', '1 Kunal Shah, “This Is What Makes SEO Important for Every Business, ” Entrepreneur India, May 11, 2020,\\nhttps://oreil.ly/teQlX.\\n2 For a sneak peek into Tesla’s compute infrastructure for ML, I highly recommend watching the recording of\\nTesla AI Day 2021 on Y ouTube.\\n3 The definition for “reasonable scale” was inspired by Jacopo Tagliabue in his paper “Y ou Do Not Need a\\nBigger Boat: Recommendations at Reasonable Scale in a (Mostly) Serverless and Open Stack, ” arXiv, July 15,\\n2021, https://oreil.ly/YNRZQ. For more discussion on reasonable scale, see “ML and MLOps at a Reasonable\\nScale” by Ciro Greco (October 2021).\\n4 FAAAM is short for Facebook, Apple, Amazon, Alphabet, Microsoft.\\n5 Reza Shiftehfar, “Uber’s Big Data Platform: 100+ Petabytes with Minute Latency, ” Uber Engineering, October\\n17, 2018, https://oreil.ly/6Ykd3; Kaushik Krishnamurthi, “Building a Big Data Pipeline to Process Clickstream\\nData, ” Zillow, April 6, 2018, https://oreil.ly/SGmNe.\\n6 Nathan Bronson and Janet Wiener, “Facebook’s Top Open Data Problems, ” Meta, October 21, 2014,\\nhttps://oreil.ly/p6QjX.\\nAt the other end of the spectrum, there are companies that work on applications\\nwith unique requirements. For example, self-driving cars have unique accuracy and\\nlatency requirements—the algorithm must be able to respond within milliseconds\\nand its accuracy must be near-perfect since a wrong prediction can lead to serious\\naccidents. Similarly, Google Search has a unique scale requirement since most com‐\\npanies don’t process 63,000 search queries a second, which translates to 234 million\\nsearch queries an hour, like Google does. 1 These companies will likely need to\\ndevelop their own highly specialized infrastructure. Google developed a large part\\nof their internal infrastructure for search; so did self-driving car companies like Tesla\\nand Waymo.2 It’s common that part of specialized infrastructure is later made public\\nand adopted by other companies. For example, Google extended their internal cloud\\ninfrastructure to the public, resulting in Google Cloud Platform.\\nIn the middle of the spectrum are the majority of companies, those who use ML\\nfor multiple common applications—a fraud detection model, a price optimization\\nmodel, a churn prediction model, a recommender system, etc.—at reasonable scale.\\n“Reasonable scale” refers to companies that work with data in the order of gigabytes\\nand terabytes, instead of petabytes, a day. Their data science team might range from', 'model, a churn prediction model, a recommender system, etc.—at reasonable scale.\\n“Reasonable scale” refers to companies that work with data in the order of gigabytes\\nand terabytes, instead of petabytes, a day. Their data science team might range from\\n10 to hundreds of engineers. 3 This category might include any company from a\\n20-person startup to a company at Zillow’s scale, but not at FAAAM scale. 4 For\\nexample, back in 2018, Uber was adding tens of terabytes of data a day to their data\\nlake, and Zillow’s biggest dataset was bringing in 2 terabytes of uncompressed data\\na day.5 In contrast, even back in 2014, Facebook was generating 4 petabytes of data a\\nday.6\\n294 | Chapter 10: Infrastructure and Tooling for MLOps', '7 Wikipedia, s.v. “Infrastructure, ” https://oreil.ly/YaIk8.\\nCompanies in the middle of the spectrum will likely benefit from generalized ML\\ninfrastructure that is being increasingly standardized (see Figure 10-1). In this book,\\nwe’ll focus on the infrastructure for the vast majority of ML applications at a reason‐\\nable scale.\\nFigure 10-1. Infrastructure requirements for companies at different production scales\\nIn order to set up the right infrastructure for your needs, it’s important to understand\\nexactly what infrastructure means and what it consists of. According to Wikipedia,\\nin the physical world, “infrastructure is the set of fundamental facilities and systems\\nthat support the sustainable functionality of households and firms. ”7 In the ML world,\\ninfrastructure is the set of fundamental facilities that support the development and\\nmaintenance of ML systems. What should be considered the “fundamental facilities”\\nvaries greatly from company to company, as discussed earlier in this chapter. In this\\nsection, we will examine the following four layers:\\nStorage and compute\\nThe storage layer is where data is collected and stored. The compute layer pro‐\\nvides the compute needed to run your ML workloads such as training a model,\\ncomputing features, generating features, etc.\\nResource management\\nResource management comprises tools to schedule and orchestrate your work‐\\nloads to make the most out of your available compute resources. Examples of\\ntools in this category include Airflow, Kubeflow, and Metaflow.\\nInfrastructure and Tooling for MLOps | 295', 'ML platform\\nThis provides tools to aid the development of ML applications such as model\\nstores, feature stores, and monitoring tools. Examples of tools in this category\\ninclude SageMaker and MLflow.\\nDevelopment environment\\nThis is usually referred to as the dev environment; it is where code is written and\\nexperiments are run. Code needs to be versioned and tested. Experiments need to\\nbe tracked.\\nThese four different layers are shown in Figure 10-2 . Data and compute are the\\nessential resources needed for any ML project, and thus the storage and compute layer\\nforms the infrastructural foundation for any company that wants to apply ML. This\\nlayer is also the most abstract to a data scientist. We’ll discuss this layer first because\\nthese resources are the easiest to explain.\\nFigure 10-2. Different layers of infrastructure for ML\\nThe dev environment is what data scientists have to interact with daily, and therefore,\\nit is the least abstract to them. We’ll discuss this category next, then we’ll discuss\\nresource management, a contentious topic among data scientists—people are still\\ndebating whether a data scientist needs to know about this layer or not. Because “ML\\nplatform” is a relatively new concept with its different components still maturing,\\nwe’ll discuss this category last, after we’ve familiarized ourselves with all other cate‐\\ngories. An ML platform requires up-front investment from a company, but if it’s done\\nright, it can make the life of data scientists across business use cases at that company\\nso much easier.\\n296 | Chapter 10: Infrastructure and Tooling for MLOps', '8 I’ve seen a company whose data is spread over Amazon Redshift and GCP BigQuery, and their engineers are\\nnot very happy about it.\\n9 We only discuss data storage here since we’ve discussed data systems in Chapter 2.\\nEven if two companies have the exact same infrastructure needs, their resulting infra‐\\nstructure might look different depending on their approaches to build versus buy\\ndecisions—i.e., what they want to build in-house versus what they want to outsource\\nto other companies. We’ll discuss the build versus buy decisions in the last part of this\\nchapter, where we’ll also discuss the hope for standardized and unified abstractions\\nfor ML infrastructure.\\nLet’s dive in!\\nStorage and Compute\\nML systems work with a lot of data, and this data needs to be stored somewhere. The\\nstorage layer is where data is collected and stored. At its simplest form, the storage\\nlayer can be a hard drive disk (HDD) or a solid state disk (SSD). The storage layer\\ncan be in one place, e.g., you might have all your data in Amazon S3 or in Snowflake,\\nor spread out over multiple locations.8 Y our storage layer can be on-prem in a private\\ndata center or on the cloud. In the past, companies might have tried to manage their\\nown storage layer. However, in the last decade, the storage layer has been mostly\\ncommoditized and moved to the cloud. Data storage has become so cheap that most\\ncompanies just store all the data they have without the cost. 9 We’ve covered the data\\nlayer intensively in Chapter 3, so in this chapter, we’ll focus on the compute layer.\\nThe compute layer refers to all the compute resources a company has access to and the\\nmechanism to determine how these resources can be used. The amount of compute\\nresources available determines the scalability of your workloads. Y ou can think of the\\ncompute layer as the engine to execute your jobs. At its simplest form, the compute\\nlayer can just be a single CPU core or a GPU core that does all your computation.\\nIts most common form is cloud compute managed by a cloud provider such as AWS\\nElastic Compute Cloud (EC2) or GCP .\\nThe compute layer can usually be sliced into smaller compute units to be used\\nconcurrently. For example, a CPU core might support two concurrent threads; each\\nthread is used as a compute unit to execute its own job. Or multiple CPU cores might\\nbe joined together to form a larger compute unit to execute a larger job. A compute\\nunit can be created for a specific short-lived job such as an AWS Step Function or a', 'thread is used as a compute unit to execute its own job. Or multiple CPU cores might\\nbe joined together to form a larger compute unit to execute a larger job. A compute\\nunit can be created for a specific short-lived job such as an AWS Step Function or a\\nGCP Cloud Run—the unit will be eliminated after the job finishes. A compute unit\\ncan also be created to be more “permanent, ” aka without being tied to a job, like a\\nvirtual machine. A more permanent compute unit is sometimes called an “instance. ”\\nStorage and Compute | 297', '10 As of writing this book, an ML workload typically requires between 4 GB and 8 GB of memory; 16 GB of\\nmemory is enough to handle most ML workloads.\\n11 See operation fusion in the section “Model optimization” on page 216.\\n12 “What Is FLOP/s and Is It a Good Measure of Performance?, ” Stack Overflow, last updated October 7, 2020,\\nhttps://oreil.ly/M8jPP.\\nHowever, the compute layer doesn’t always use threads or cores as compute units.\\nThere are compute layers that abstract away the notions of cores and use other\\nunits of computation. For example, computation engines like Spark and Ray use\\n“job” as their unit, and Kubernetes uses “pod, ” a wrapper around containers, as its\\nsmallest deployable unit. While you can have multiple containers in a pod, you can’t\\nindependently start or stop different containers in the same pod.\\nTo execute a job, you first need to load the required data into your compute unit’s\\nmemory, then execute the required operations—addition, multiplication, division,\\nconvolution, etc.—on that data. For example, to add two arrays, you will first need\\nto load these two arrays into memory, and then perform addition on the two arrays.\\nIf the compute unit doesn’t have enough memory to load these two arrays, the\\noperation will be impossible without an algorithm to handle out-of-memory compu‐\\ntation. Therefore, a compute unit is mainly characterized by two metrics: how much\\nmemory it has and how fast it runs an operation.\\nThe memory metric can be specified using units like GB, and it’s generally straight‐\\nforward to evaluate: a compute unit with 8 GB of memory can handle more data in\\nmemory than a compute unit with only 2 GB, and it is generally more expensive. 10\\nSome companies care not only how much memory a compute unit has but also\\nhow fast it is to load data in and out of memory, so some cloud providers advertise\\ntheir instances as having “high bandwidth memory” or specify their instances’ I/O\\nbandwidth.\\nThe operation speed is more contentious. The most common metric is FLOPS—\\nfloating point operations per second. As the name suggests, this metric denotes the\\nnumber of float point operations a compute unit can run per second. Y ou might\\nsee a hardware vendor advertising that their GPUs or TPUs or IPUs (intelligence\\nprocessing units) have teraFLOPS (one trillion FLOPS) or another massive number\\nof FLOPS.\\nHowever, this metric is contentious because, first, companies that measure this metric', 'see a hardware vendor advertising that their GPUs or TPUs or IPUs (intelligence\\nprocessing units) have teraFLOPS (one trillion FLOPS) or another massive number\\nof FLOPS.\\nHowever, this metric is contentious because, first, companies that measure this metric\\nmight have different ideas on what is counted as an operation. For example, if a\\nmachine fuses two operations into one and executes this fused operation, 11 does this\\ncount as one operation or two? Second, just because a compute unit is capable of\\ndoing a trillion FLOPS doesn’t mean you’ll be able to execute your job at the speed\\nof a trillion FLOPS. The ratio of the number of FLOPS a job can run to the number\\nof FLOPs a compute unit is capable of handling is called utilization. 12 If an instance\\n298 | Chapter 10: Infrastructure and Tooling for MLOps', '13 For readers interested in FLOPS and bandwidth and how to optimize them for deep learning models, I\\nrecommend the post “Making Deep Learning Go Brrrr From First Principles” (He 2022).\\n14 According to Amazon, “EC2 instances support multithreading, which enables multiple threads to run concur‐\\nrently on a single CPU core. Each thread is represented as a virtual CPU (vCPU) on the instance. An instance\\nhas a default number of CPU cores, which varies according to instance type. For example, an m5.xlarge\\ninstance type has two CPU cores and two threads per core by default—four vCPUs in total” (“Optimize CPU\\nOptions, ” Amazon Web Services, last accessed April 2020, https://oreil.ly/eeOtd).\\nis capable of doing a million FLOPs and your job runs with 0.3 million FLOPS, that’s\\na 30% utilization rate. Of course, you’ d want to have your utilization rate as high as\\npossible. However, it’s near impossible to achieve 100% utilization rate. Depending\\non the hardware backend and the application, the utilization rate of 50% might be\\nconsidered good or bad. Utilization also depends on how fast you can load data into\\nmemory to perform the next operations—hence the importance of I/O bandwidth.13\\nWhen evaluating a new compute unit, it’s important to evaluate how long it will\\ntake this compute unit to do common workloads. For example, MLPerf is a popular\\nbenchmark for hardware vendors to measure their hardware performance by show‐\\ning how long it will take their hardware to train a ResNet-50 model on the ImageNet\\ndataset or use a BERT-large model to generate predictions for the SQuAD dataset.\\nBecause thinking about FLOPS is not very useful, to make things easier, when\\nevaluating compute performance, many people just look into the number of cores\\na compute unit has. So you might use an instance with 4 CPU cores and 8 GB of\\nmemory. Keep in mind that AWS uses the concept of vCPU, which stands for virtual\\nCPU and which, for practical purposes, can be thought of as half a physical core. 14\\nY ou can see the number of cores and memory offered by some AWS EC2 and GCP\\ninstances in Figure 10-3.\\nFigure 10-3. Examples of GPU and TPU instances available on AWS and GCP as of\\nFebruary 2022. Source: Screenshots of AWS and GCP websites\\nStorage and Compute | 299', '15 Which costs $26.688/hour.\\n16 On-demand instances are instances that are available when you request them. Spot instances are instances\\nthat are available when nobody else is using them. Cloud providers tend to offer spot instances at a discount\\ncompared to on-demand instances.\\n17 Synergy Research Group, “2020—The Y ear That Cloud Service Revenues Finally Dwarfed Enterprise Spend‐\\ning on Data Centers, ” March 18, 2021, https://oreil.ly/uPx94.\\nPublic Cloud Versus Private Data Centers\\nLike data storage, the compute layer is largely commoditized. This means that instead\\nof setting up their own data centers for storage and compute, companies can pay\\ncloud providers like AWS and Azure for the exact amount of compute they use.\\nCloud compute makes it extremely easy for companies to start building without\\nhaving to worry about the compute layer. It’s especially appealing to companies that\\nhave variable-sized workloads. Imagine if your workloads need 1,000 CPU cores one\\nday of the year and only 10 CPU cores the rest of the year. If you build your own\\ndata centers, you’ll need to pay for 1,000 CPU cores up front. With cloud compute,\\nyou only need to pay for 1,000 CPU cores one day of the year and 10 CPU cores\\nthe rest of the year. It’s convenient to be able to just add more compute or shut\\ndown instances as needed—most cloud providers even do that automatically for\\nyou—reducing engineering operational overhead. This is especially useful in ML as\\ndata science workloads are bursty. Data scientists tend to run experiments a lot for a\\nfew weeks during development, which requires a surge of compute power. Later on,\\nduring production, the workload is more consistent.\\nKeep in mind that cloud compute is elastic but not magical. It doesn’t actually have\\ninfinite compute. Most cloud providers offer limits on the compute resources you can\\nuse at a time. Some, but not all, of these limits can be raised through petitions. For\\nexample, as of writing this book, AWS EC2’s largest instance is X1e with 128 vCPUs\\nand almost 4 TB of memory. 15 Having a lot of compute resources doesn’t mean that\\nit’s always easy to use them, especially if you have to work with spot instances to save\\ncost.16\\nDue to the cloud’s elasticity and ease of use, more and more companies are choosing\\nto pay for the cloud over building and maintaining their own storage and compute\\nlayer. Synergy Research Group’s research shows that in 2020, “enterprise spending', 'cost.16\\nDue to the cloud’s elasticity and ease of use, more and more companies are choosing\\nto pay for the cloud over building and maintaining their own storage and compute\\nlayer. Synergy Research Group’s research shows that in 2020, “enterprise spending\\non cloud infrastructure services [grew] by 35% to reach almost $130 billion” while\\n“enterprise spending on data [centers] dropped by 6% to under $90 billion, ” 17 as\\nshown in Figure 10-4.\\n300 | Chapter 10: Infrastructure and Tooling for MLOps', '18 Sarah Wang and Martin Casado, “The Cost of Cloud, a Trillion Dollar Paradox, ” a16z, https://oreil.ly/3nWU3.\\n19 Wang and Casado, “The Cost of Cloud. ”\\nFigure 10-4. In 2020, enterprise spending on cloud infrastructure services grew by 35%\\nwhile spending on data centers dropped by 6%. Source: Adapted from an image by\\nSynergy Research Group\\nWhile leveraging the cloud tends to give companies higher returns than building\\ntheir own storage and compute layers early on, this becomes less defensible as a\\ncompany grows. Based on disclosed cloud infrastructure spending by public software\\ncompanies, the venture capital firm a16z shows that cloud spending accounts for\\napproximately 50% cost of revenue of these companies.18\\nThe high cost of the cloud has prompted companies to start moving their workloads\\nback to their own data centers, a process called “cloud repatriation. ” Dropbox’s S-1\\nfiling in 2018  shows that the company was able to save $75M over the two years\\nprior to IPO due to their infrastructure optimization overhaul—a large chunk of it\\nconsisted of moving their workloads from public cloud to their own data centers. Is\\nthe high cost of cloud unique to Dropbox because Dropbox is in the data storage\\nbusiness? Not quite. In the aforementioned analysis, a16z estimated that “across 50\\nof the top public software companies currently utilizing cloud infrastructure, an\\nestimated $100B of market value is being lost among them due to cloud impact on\\nmargins—relative to running the infrastructure themselves. ”19\\nWhile getting started with the cloud is easy, moving away from the cloud is hard.\\nCloud repatriation requires nontrivial up-front investment in both commodities and\\nStorage and Compute | 301', '20 Laurence Goasduff, “Why Organizations Choose a Multicloud Strategy, ” Gartner, May 7, 2019,\\nhttps://oreil.ly/ZiqzQ.\\n21 Goasduff, “Why Organizations Choose a Multicloud Strategy. ”\\nengineering effort. More and more companies are following a hybrid approach:\\nkeeping most of their workloads on the cloud but slowly increasing their investment\\nin data centers.\\nOn Multicloud Strategy\\nAnother way for companies to reduce their dependence on any single cloud provider\\nis to follow a multicloud strategy: spreading their workloads on multiple cloud pro‐\\nviders.20 This allows companies to architect their systems so that they can be compati‐\\nble with multiple clouds, enabling them to leverage the best and most cost-effective\\ntechnologies available instead of being stuck with the services provided by a single\\ncloud provider, a situation known as vendor lock-in. A 2019 study by Gartner shows\\nthat 81% of organizations are working with two or more public cloud providers. 21 A\\ncommon pattern that I’ve seen for ML workloads is to do training on GCP or Azure,\\nand deployment on AWS.\\nThe multicloud strategy doesn’t usually happen by choice. As Josh Wills, one of our\\nearly reviewers, put it: “Nobody in their right mind intends to use multicloud. ” It’s\\nincredibly hard to move data and orchestrate workloads across clouds.\\nOften, multicloud just happens because different parts of the organization operate\\nindependently, and each part makes their own cloud decision. It can also happen\\nfollowing an acquisition—the acquired team is already on a cloud different from the\\nhost organization, and migrating hasn’t happened yet.\\nIn my work, I’ve seen multicloud happen due to strategic investments. Microsoft and\\nGoogle are big investors in the startup ecosystem, and several companies that I work\\nwith that were previously on AWS have moved to Azure/GCP after Microsoft/Google\\ninvested in them.\\nDevelopment Environment\\nThe dev environment is where ML engineers write code, run experiments, and\\ninteract with the production environment where champion models are deployed\\nand challenger models evaluated. The dev environment consists of the following\\ncomponents: IDE (integrated development environment), versioning, and CI/CD.\\nIf you’re a data scientist or ML engineer who writes code daily, you’re probably very\\nfamiliar with all these tools and might wonder what there is to say about them. In my\\nexperience, outside of a handful of tech companies, the dev environment is severely', 'If you’re a data scientist or ML engineer who writes code daily, you’re probably very\\nfamiliar with all these tools and might wonder what there is to say about them. In my\\nexperience, outside of a handful of tech companies, the dev environment is severely\\n302 | Chapter 10: Infrastructure and Tooling for MLOps', '22 Ville Tuulos, Effective Data Science Infrastructure (Manning, 2022).\\nunderrated and underinvested in at most companies. According to Ville Tuulos in\\nhis book Effective Data Science Infrastructure, “you would be surprised to know how\\nmany companies have well-tuned, scalable production infrastructure but the question\\nof how the code is developed, debugged, and tested in the first place is solved in an\\nad-hoc manner. ”22\\nHe suggested that “if you have time to set up only one piece of infrastructure\\nwell, make it the development environment for data scientists. ” Because the dev envi‐\\nronment is where engineers work, improvements in the dev environment translate\\ndirectly into improvements in engineering productivity.\\nIn this section, we’ll first cover different components of the dev environment, then\\nwe’ll discuss the standardization of the dev environment before we discuss how to\\nbring your changes from the dev environment to the production environment with\\ncontainers.\\nDev Environment Setup\\nThe dev environment should be set up to contain all the tools that can make it easier\\nfor engineers to do their job. It should also consist of tools for versioning. As of this\\nwriting, companies use an ad hoc set of tools to version their ML workflows, such as\\nGit to version control code, DVC to version data, Weights & Biases or Comet.ml to\\ntrack experiments during development, and MLflow to track artifacts of models when\\ndeploying them. Claypot AI is working on a platform that can help you version and\\ntrack all your ML workflows in one place. Versioning is important for any software\\nengineering projects, but even more so for ML projects because of both the sheer\\nnumber of things you can change (code, parameters, the data itself, etc.) and the need\\nto keep track of prior runs to reproduce later on. We’ve covered this in the section\\n“Experiment Tracking and Versioning” on page 162.\\nThe dev environment should also be set up with a CI/CD test suite to test your code\\nbefore pushing it to the staging or production environment. Examples of tools to\\norchestrate your CI/CD test suite are GitHub Actions and CircleCI. Because CI/CD is\\na software engineering concern, it’s beyond the scope of this book.\\nIn this section, we’ll focus on the place where engineers write code: the IDE.\\nIDE\\nThe IDE is the editor where you write your code. IDEs tend to support multiple\\nprogramming languages. IDEs can be native apps like VS Code or Vim. IDEs can be', 'In this section, we’ll focus on the place where engineers write code: the IDE.\\nIDE\\nThe IDE is the editor where you write your code. IDEs tend to support multiple\\nprogramming languages. IDEs can be native apps like VS Code or Vim. IDEs can be\\nbrowser-based, which means they run in browsers, such as AWS Cloud9.\\nDevelopment Environment | 303', '23 As of writing this book, Google Colab even offers free GPUs for their users.\\nMany data scientists write code not just in IDEs but also in notebooks like Jupyter\\nNotebooks and Google Colab. 23 Notebooks are more than just places to write code.\\nY ou can include arbitrary artifacts such as images, plots, data in nice tabular formats,\\netc., which makes notebooks very useful for exploratory data analysis and analyzing\\nmodel training results.\\nNotebooks have a nice property: they are stateful—they can retain states after runs.\\nIf your program fails halfway through, you can rerun from the failed step instead of\\nhaving to run the program from the beginning. This is especially helpful when you\\nhave to deal with large datasets that might take a long time to load. With notebooks,\\nyou only need to load your data once—notebooks can retain this data in memory—\\ninstead of having to load it each time you want to run your code. As shown in\\nFigure 10-5, if your code fails at step 4 in a notebook, you’ll only need to rerun step 4\\ninstead of from the beginning of your program.\\nFigure 10-5. In Jupyter Notebooks, if step 4 fails, you only need to run step 4 again,\\ninstead of having to run steps 1 to 4 again\\nNote that this statefulness can be a double-edged sword, as it allows you to execute\\nyour cells out of order. For example, in a normal script, cell 4 must run after cell 3\\nand cell 3 must run after cell 2. However, in notebooks, you can run cell 2, 3, then 4\\nor cell 4, 3, then 2. This makes notebook reproducibility harder unless your notebook\\ncomes with an instruction on the order in which to run your cells. This difficulty is\\ncaptured in a joke by Chris Albon (see Figure 10-6).\\n304 | Chapter 10: Infrastructure and Tooling for MLOps', '24 Michelle Ufford, M. Pacer, Matthew Seal, and Kyle Kelley, “Beyond Interactive: Notebook Innovation at\\nNetflix, ” Netflix Technology Blog, August 16, 2018, https://oreil.ly/EHvAe.\\nFigure 10-6. Notebooks’ statefulness allows you to execute cells out of order, making it\\nhard to reproduce a notebook\\nBecause notebooks are so useful for data exploration and experiments, notebooks\\nhave become an indispensable tool for data scientists and ML. Some companies\\nhave made notebooks the center of their data science infrastructure. In their seminal\\narticle, “Beyond Interactive: Notebook Innovation at Netflix, ” Netflix included a list of\\ninfrastructure tools that can be used to make notebooks even more powerful. 24 The\\nlist includes:\\nPapermill\\nFor spawning multiple notebooks with different parameter sets—such as when\\nyou want to run different experiments with different sets of parameters and\\nexecute them concurrently. It can also help summarize metrics from a collection\\nof notebooks.\\nCommuter\\nA notebook hub for viewing, finding, and sharing notebooks within an\\norganization.\\nDevelopment Environment | 305', '25 For the uninitiated, a new pull request can be understood as a new piece of code being added to the codebase.\\nAnother interesting project aimed at improving the notebook experience is nbdev, a\\nlibrary on top of Jupyter Notebooks that encourages you to write documentation and\\ntests in the same place.\\nStandardizing Dev Environments\\nThe first thing about the dev environment is that it should be standardized, if not\\ncompany-wide, then at least team-wide. We’ll go over a story to understand what it\\nmeans to have the dev environment standardized and why that is needed.\\nIn the early days of our startup, we each worked from our own computer. We had a\\nbash file that a new team member could run to create a new virtual environment—in\\nour case, we use conda for virtual environments—and install the required packages\\nneeded to run our code. The list of the required packages was the good old require‐\\nments.txt that we kept adding to as we started using a new package. Sometimes, one\\nof us got lazy and we just added a package name (e.g., torch) without specifying\\nwhich version of the package it was (e.g., torch==1.10.0+cpu). Occasionally, a new\\npull request would run well on my computer but not another coworker’s computer, 25\\nand we usually quickly figured out that it was because we used different versions\\nof the same package. We resolved to always specify the package name together with\\nthe package version when adding a new package to the requirements.txt, and that\\nremoved a lot of unnecessary headaches.\\nOne day, we ran into this weird bug that only happened during some runs and not\\nothers. I asked my coworker to look into it, but he wasn’t able to reproduce the bug.\\nI told him that the bug only happened some of the time, so he might have to run\\nthe code around 20 times just to be sure. He ran the code 20 times and still found\\nnothing. We compared our packages and everything matched. After a few hours of\\nhair-pulling frustration, we discovered that it was a concurrency issue that is only an\\nissue for Python version 3.8 or earlier. I had Python 3.8 and my coworker had Python\\n3.9, so he didn’t see the bug. We resolved to have everyone on the same Python\\nversion, and that removed some more headaches.\\nThen one day, my coworker got a new laptop. It was a MacBook with the then new\\nM1 chip. He tried to follow our setup steps on this new laptop but ran into difficulty.\\nIt was because the M1 chip was new, and some of the tools we used, including', 'version, and that removed some more headaches.\\nThen one day, my coworker got a new laptop. It was a MacBook with the then new\\nM1 chip. He tried to follow our setup steps on this new laptop but ran into difficulty.\\nIt was because the M1 chip was new, and some of the tools we used, including\\nDocker, weren’t working well with M1 chips yet. After seeing him struggling with set‐\\nting the environment up for a day, we decided to move to a cloud dev environment.\\nThis means that we still standardize the virtual environment and tools and packages,\\nbut now everyone uses the virtual environment and tools and packages on the same\\ntype of machine too, provided by a cloud provider.\\n306 | Chapter 10: Infrastructure and Tooling for MLOps', '26 See editor war, the decade-long, heated debate on Vim versus Emacs.\\nWhen using a cloud dev environment, you can use a cloud dev environment that also\\ncomes with a cloud IDE like AWS Cloud9 (which has no built-in notebooks) and\\nAmazon SageMaker Studio (which comes with hosted JupyterLab). As of writing this\\nbook, Amazon SageMaker Studio seems more widely used than Cloud9. However,\\nmost engineers I know who use cloud IDEs do so by installing IDEs of their choice,\\nlike Vim, on their cloud instances.\\nA much more popular option is to use a cloud dev environment with a local IDE. For\\nexample, you can use VS Code installed on your computer and connect the local IDE\\nto the cloud environment using a secure protocol like Secure Shell (SSH).\\nWhile it’s generally agreed upon that tools and packages should be standardized,\\nsome companies are hesitant to standardize IDEs. Engineers can get emotionally\\nattached to IDEs, and some have gone to great length to defend their IDE of choice, 26\\nso it’ll be hard forcing everyone to use the same IDE. However, over the years, some\\nIDEs have emerged to be the most popular. Among them, VS Code is a good choice\\nsince it allows easy integration with cloud dev instances.\\nAt our startup, we chose GitHub Codespaces  as our cloud dev environment, but\\nan AWS EC2 or a GCP instance that you can SSH into is also a good option.\\nBefore moving to cloud environments, like many other companies, we were worried\\nabout the cost—what if we forgot to shut down our instances when not in use and\\nthey kept charging us money? However, this worry has gone away for two reasons.\\nFirst, tools like GitHub Codespaces automatically shut down your instance after 30\\nminutes of inactivity. Second, some instances are pretty cheap. For example, an AWS\\ninstance with 4 vCPUs and 8 GB of memory costs around $0.1/hour, which comes\\nto approximately $73/month if you never shut it down. Because engineering time is\\nexpensive, if a cloud dev environment can help you save a few hours of engineering\\ntime a month, it’s worth it for many companies.\\nMoving from local dev environments to cloud dev environments has many other\\nbenefits. First, it makes IT support so much easier—imagine having to support\\n1,000 different local machines instead of having to support only one type of cloud\\ninstance. Second, it’s convenient for remote work—you can just SSH into your dev\\nenvironment wherever you go from any computer. Third, cloud dev environments', '1,000 different local machines instead of having to support only one type of cloud\\ninstance. Second, it’s convenient for remote work—you can just SSH into your dev\\nenvironment wherever you go from any computer. Third, cloud dev environments\\ncan help with security. For example, if an employee’s laptop is stolen, you can just\\nrevoke access to cloud instances from that laptop to prevent the thief from accessing\\nyour codebase and proprietary information. Of course, some companies might not\\nbe able to move to cloud dev environments also because of security concerns. For\\nexample, they aren’t allowed to have their code or data on the cloud.\\nDevelopment Environment | 307', 'The fourth benefit, which I would argue is the biggest benefit for companies that\\ndo production on the cloud, is that having your dev environment on the cloud\\nreduces the gap between the dev environment and the production environment. If\\nyour production environment is in the cloud, bringing your dev environment to the\\ncloud is only natural.\\nOccasionally, a company has to move their dev environments to the cloud not only\\nbecause of the benefits, but also out of necessity. For the use cases where data can’t be\\ndownloaded or stored on a local machine, the only way to access it is via a notebook\\nin the cloud (SageMaker Studio) that can read the data from S3, provided it has the\\nright permissions.\\nOf course, cloud dev environments might not work for every company due to cost,\\nsecurity, or other concerns. Setting up cloud dev environments also requires some\\ninitial investments, and you might need to educate your data scientists on cloud\\nhygiene, including establishing secure connections to the cloud, security compliance,\\nor avoiding wasteful cloud usage. However, standardization of dev environments\\nmight make your data scientists’ lives easier and save you money in the long run.\\nFrom Dev to Prod: Containers\\nDuring development, you might usually work with a fixed number of machines or\\ninstances (usually one) because your workloads don’t fluctuate a lot—your model\\ndoesn’t suddenly change from serving only 1,000 requests an hour to 1 million\\nrequests an hour.\\nA production service, on the other hand, might be spread out on multiple instances.\\nThe number of instances changes from time to time depending on the incoming\\nworkloads, which can be unpredictable at times. For example, a celebrity tweets about\\nyour fledgling app and suddenly your traffic spikes 10x. Y ou will have to turn on new\\ninstances as needed, and these instances will need to be set up with required tools and\\npackages to execute your workloads.\\nPreviously, you’ d have to spin up and shut down instances yourself, but most public\\ncloud providers have taken care of the autoscaling part. However, you still have to\\nworry about setting up new instances.\\nWhen you consistently work with the same instance, you can install dependencies\\nonce and use them whenever you use this instance. In production, if you dynamically\\nallocate instances as needed, your environment is inherently stateless. When a new\\ninstance is allocated for your workload, you’ll need to install dependencies using a list', 'once and use them whenever you use this instance. In production, if you dynamically\\nallocate instances as needed, your environment is inherently stateless. When a new\\ninstance is allocated for your workload, you’ll need to install dependencies using a list\\nof predefined instructions.\\nA question arises: how do you re-create an environment on any new instance?\\nContainer technology—of which Docker is the most popular—is designed to answer\\nthis question. With Docker, you create a Dockerfile with step-by-step instructions on\\n308 | Chapter 10: Infrastructure and Tooling for MLOps', 'how to re-create an environment in which your model can run: install this package,\\ndownload this pretrained model, set environment variables, navigate into a folder, etc.\\nThese instructions allow hardware anywhere to run your code.\\nTwo key concepts in Docker are image and container. Running all the instructions in\\na Dockerfile gives you a Docker image. If you run this Docker image, you get back\\na Docker container. Y ou can think of a Dockerfile as the recipe to construct a mold,\\nwhich is a Docker image. From this mold, you can create multiple running instances;\\neach is a Docker container.\\nY ou can build a Docker image either from scratch or from another Docker image.\\nFor example, NVIDIA might provide a Docker image that contains TensorFlow and\\nall necessary libraries to optimize TensorFlow for GPUs. If you want to build an\\napplication that runs TensorFlow on GPUs, it’s not a bad idea to use this Docker\\nimage as your base and install dependencies specific to your application on top of this\\nbase image.\\nA container registry is where you can share a Docker image or find an image created\\nby other people to be shared publicly or only with people inside their organizations.\\nCommon container registries include Docker Hub and AWS ECR (Elastic Container\\nRegistry).\\nHere’s an example of a simple Dockerfile that runs the following instructions. The\\nexample is to show how Dockerfiles work in general, and might not be executable.\\n1. Download the latest PyTorch base image.1.\\n2. Clone NVIDIA ’s apex repository on GitHub, navigate to the newly created apex2.\\nfolder, and install apex.\\n3. Set fancy-nlp-project to be the working directory.3.\\n4. Clone Hugging Face’s transformers repository on GitHub, navigate to the newly4.\\ncreated transformers folder, and install transformers.\\nFROM pytorch/pytorch:latest\\nRUN git clone https://github.com/NVIDIA/apex\\nRUN cd apex && \\\\\\n    python3 setup.py install && \\\\\\n    pip install -v --no-cache-dir --global-option=\"--cpp_ext\" \\\\\\n    --global-option=\"--cuda_ext\" ./\\nWORKDIR /fancy-nlp-project\\nRUN git clone https://github.com/huggingface/transformers.git && \\\\\\n    cd transformers && \\\\\\n    python3 -m pip install --no-cache-dir.\\nDevelopment Environment | 309', '27 Chip Huyen, “Why Data Scientists Shouldn’t Need to Know Kubernetes, ” September 13, 2021, https://huyen\\nchip.com/2021/09/13/data-science-infrastructure.html; Neil Conway and David Hershey, “Data Scientists Don’t\\nCare About Kubernetes, ” Determined AI, November 30, 2020, https://oreil.ly/FFDQW; I Am Developer on\\nTwitter (@iamdevloper): “I barely understand my own feelings how am I supposed to understand kubernetes, ”\\nJune 26, 2021, https://oreil.ly/T2eQE.\\nIf your application does anything interesting, you will probably need more than one\\ncontainer. Consider the case where your project consists of the featurizing code that\\nis fast to run but requires a lot of memory, and the model training code that is slow\\nto run but requires less memory. If you run both parts of the code on the same\\nGPU instances, you’ll need GPU instances with high memory, which can be very\\nexpensive. Instead, you can run your featurizing code on CPU instances and the\\nmodel training code on GPU instances. This means you’ll need one container for\\nfeaturizing and another container for training.\\nDifferent containers might also be necessary when different steps in your pipeline\\nhave conflicting dependencies, such as your featurizer code requires NumPy 0.8 but\\nyour model requires NumPy 1.0.\\nIf you have 100 microservices and each microservice requires its own container, you\\nmight have 100 containers running at the same time. Manually building, running,\\nallocating resources for, and stopping 100 containers might be a painful chore. A\\ntool to help you manage multiple containers is called container orchestration. Docker\\nCompose is a lightweight container orchestrator that can manage containers on a\\nsingle host.\\nHowever, each of your containers might run on its own host, and this is where\\nDocker Compose is at its limits. Kubernetes (K8s) is a tool for exactly that. K8s\\ncreates a network for containers to communicate and share resources. It can help you\\nspin up containers on more instances when you need more compute/memory as well\\nas shutting down containers when you no longer need them, and it helps maintain\\nhigh availability for your system.\\nK8s was one of the fastest-growing technologies in the 2010s. Since its inception in\\n2014, it’s become ubiquitous in production systems today. Jeremy Jordan has a great\\nintroduction to K8s  for readers interested in learning more. However, K8s is not\\nthe most data-scientist-friendly tool, and there have been many discussions on how', '2014, it’s become ubiquitous in production systems today. Jeremy Jordan has a great\\nintroduction to K8s  for readers interested in learning more. However, K8s is not\\nthe most data-scientist-friendly tool, and there have been many discussions on how\\nto move data science workloads away from it. 27 We’ll go more into K8s in the next\\nsection.\\n310 | Chapter 10: Infrastructure and Tooling for MLOps', 'Resource Management\\nIn the pre-cloud world (and even today in companies that maintain their own data\\ncenters), storage and compute were finite. Resource management back then centered\\naround how to make the most out of limited resources. Increasing resources for\\none application could mean decreasing resources for other applications, and complex\\nlogic was required to maximize resource utilization, even if that meant requiring\\nmore engineering time.\\nHowever, in the cloud world where storage and compute resources are much more\\nelastic, the concern has shifted from how to maximize resource utilization to how\\nto use resources cost-effectively. Adding more resources to an application doesn’t\\nmean decreasing resources for other applications, which significantly simplifies the\\nallocation challenge. Many companies are OK with adding more resources to an\\napplication as long as the added cost is justified by the return, e.g., extra revenue or\\nsaved engineering time.\\nIn the vast majority of the world, where engineers’ time is more valuable than\\ncompute time, companies are OK using more resources if this means it can help their\\nengineers become more productive. This means that it might make sense for compa‐\\nnies to invest in automating their workloads, which might make using resources less\\nefficient than manually planning their workloads, but free their engineers to focus\\non work with higher returns. Often, if a problem can be solved by either using more\\nnon-human resources (e.g., throwing more compute at it) or using more human\\nresources (e.g., requiring more engineering time to redesign), the first solution might\\nbe preferred.\\nIn this section, we’ll discuss how to manage resources for ML workflows. We’ll focus\\non cloud-based resources; however, the discussed ideas can also be applicable for\\nprivate data centers.\\nCron, Schedulers, and Orchestrators\\nThere are two key characteristics of ML workflows that influence their resource\\nmanagement: repetitiveness and dependencies.\\nIn this book, we’ve discussed at length how developing ML systems is an iterative\\nprocess. Similarly, ML workloads are rarely one-time operations but something repet‐\\nitive. For example, you might train a model every week or generate a new batch\\nof predictions every four hours. These repetitive processes can be scheduled and\\norchestrated to run smoothly and cost-effectively using available resources.\\nScheduling repetitive jobs to run at fixed times is exactly what cron does. This is also', 'of predictions every four hours. These repetitive processes can be scheduled and\\norchestrated to run smoothly and cost-effectively using available resources.\\nScheduling repetitive jobs to run at fixed times is exactly what cron does. This is also\\nall that cron does: run a script at a predetermined time and tell you whether the job\\nsucceeds or fails. It doesn’t care about the dependencies between the jobs it runs—you\\nResource Management | 311', 'can run job A after job B with cron but you can’t schedule anything complicated like\\nrun B if A succeeds and run C if A fails.\\nThis leads us to the second characteristic: dependencies. Steps in an ML workflow\\nmight have complex dependency relationships with each other. For example, an ML\\nworkflow might consist of the following steps:\\n1. Pull last week’s data from data warehouses.1.\\n2. Extract features from this pulled data.2.\\n3. Train two models, A and B, on the extracted features.3.\\n4. Compare A and B on the test set.4.\\n5. Deploy A if A is better; otherwise deploy B.5.\\nEach step depends on the success of the previous step. Step 5 is what we call condi‐\\ntional dependency: the action for this step depends on the outcome of the previous\\nstep. The order of execution and dependencies among these steps can be represented\\nusing a graph, as shown in Figure 10-7.\\nFigure 10-7. A graph that shows the order of execution of a simple ML workflow, which\\nis essentially a DAG (directed acyclic graph)\\nMany readers might recognize that Figure 10-7 is a DAG: directed acyclic graph. It\\nhas to be directed to express the dependencies among steps. It can’t contain cycles\\nbecause, if it does, the job will just keep on running forever. DAG is a common way\\nto represent computing workflows in general, not just ML workflows. Most workflow\\nmanagement tools require you to specify your workflows in a form of DAGs.\\n312 | Chapter 10: Infrastructure and Tooling for MLOps', '28 Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, David Oppenheimer, Eric Tune, and John Wilkes,\\n“Large-Scale Cluster Management at Google with Borg, ” EuroSys ’15: Proceedings of the Tenth European\\nConference on Computer Systems (April 2015): 18, https://oreil.ly/9TeTM.\\nSchedulers are cron programs that can handle dependencies. It takes in the DAG of\\na workflow and schedules each step accordingly. Y ou can even schedule to start a\\njob based on an event-based trigger, e.g., start a job whenever an event X happens.\\nSchedulers also allow you to specify what to do if a job fails or succeeds, e.g., if it fails,\\nhow many times to retry before giving up.\\nSchedulers tend to leverage queues to keep track of jobs. Jobs can be queued, priori‐\\ntized, and allocated resources needed to execute. This means that schedulers need to\\nbe aware of the resources available and the resources needed to run each job—the\\nresources needed are either specified as options when you schedule a job or estimated\\nby the scheduler. For instance, if a job requires 8 GB of memory and two CPUs, the\\nscheduler needs to find among the resources it manages an instance with 8 GB of\\nmemory and two CPUs and wait until the instance is not executing other jobs to run\\nthis job on the instance.\\nHere’s an example of how to schedule a job with the popular scheduler Slurm, where\\nyou specify the job name, the time when the job needs to be executed, and the\\namount of memory and CPUs to be allocated for the job:\\n#!/bin/bash\\n#SBATCH -J JobName\\n#SBATCH --time=11:00:00       # When to start the job\\n#SBATCH --mem-per-cpu=4096   # Memory, in MB, to be allocated per CPU\\n#SBATCH --cpus-per-task=4          # Number of cores per task\\nSchedulers should also optimize for resource utilization since they have information\\non resources available, jobs to run, and resources needed for each job to run. How‐\\never, the number of resources specified by users is not always correct. For example,\\nI might estimate, and therefore specify, that a job needs 4 GB of memory, but this\\njob only needs 3 GB of memory or needs 4 GB of memory at peak and only 1–2\\nGB of memory otherwise. Sophisticated schedulers like Google’s Borg estimate how\\nmany resources a job will actually need and reclaim unused resources for other jobs,28\\nfurther optimizing resource utilization.\\nDesigning a general-purpose scheduler is hard, since this scheduler will need to\\nbe able to manage almost any number of concurrent machines and workflows. If', 'many resources a job will actually need and reclaim unused resources for other jobs,28\\nfurther optimizing resource utilization.\\nDesigning a general-purpose scheduler is hard, since this scheduler will need to\\nbe able to manage almost any number of concurrent machines and workflows. If\\nyour scheduler is down, every single workflow that this scheduler touches will be\\ninterrupted.\\nIf schedulers are concerned with when to run jobs and what resources are needed\\nto run those jobs, orchestrators are concerned with where to get those resources.\\nSchedulers deal with job-type abstractions such as DAGs, priority queues, user-level\\nResource Management | 313', 'quotas (i.e., the maximum number of instances a user can use at a given time), etc.\\nOrchestrators deal with lower-level abstractions like machines, instances, clusters,\\nservice-level grouping, replication, etc. If the orchestrator notices that there are more\\njobs than the pool of available instances, it can increase the number of instances in\\nthe available instance pool. We say that it “provisions” more computers to handle\\nthe workload. Schedulers are often used for periodical jobs, whereas orchestrators\\nare often used for services where you have a long-running server that responds to\\nrequests.\\nThe most well-known orchestrator today is undoubtedly Kubernetes, the container\\norchestrator we discussed in the section “From Dev to Prod: Containers” on page\\n308. K8s can be used on-prem (even on your laptop via minikube). However, I’ve\\nnever met anyone who enjoys setting up their own K8s clusters, so most companies\\nuse K8s as a hosted service managed by their cloud providers, such as AWS’s Elastic\\nKubernetes Service (EKS) or Google Kubernetes Engine (GKE).\\nMany people use schedulers and orchestrators interchangeably because schedulers\\nusually run on top of orchestrators. Schedulers like Slurm and Google’s Borg have\\nsome orchestrating capacity, and orchestrators like HashiCorp Nomad and K8s\\ncome with some scheduling capacity. But you can have separate schedulers and\\norchestrators, such as running Spark’s job scheduler on top of Kubernetes or AWS\\nBatch scheduler on top of EKS. Orchestrators such as HashiCorp Nomad and data\\nscience–specific orchestrators including Airflow, Argo, Prefect, and Dagster have\\ntheir own schedulers.\\nData Science Workflow Management\\nWe’ve discussed the differences between schedulers and orchestrators and how they\\ncan be used to execute workflows in general. Readers familiar with workflow man‐\\nagement tools aimed especially at data science like Airflow, Argo, Prefect, Kubeflow,\\nMetaflow, etc. might wonder where they fit in this scheduler versus orchestrator\\ndiscussion. We’ll go into this topic here.\\nIn its simplest form, workflow management tools manage workflows. They generally\\nallow you to specify your workflows as DAGs, similar to the one in Figure 10-7. A\\nworkflow might consist of a featurizing step, a model training step, and an evaluation\\nstep. Workflows can be defined using either code (Python) or configuration files\\n(YAML). Each step in a workflow is called a task.', 'allow you to specify your workflows as DAGs, similar to the one in Figure 10-7. A\\nworkflow might consist of a featurizing step, a model training step, and an evaluation\\nstep. Workflows can be defined using either code (Python) or configuration files\\n(YAML). Each step in a workflow is called a task.\\nAlmost all workflow management tools come with some schedulers, and therefore,\\nyou can think of them as schedulers that, instead of focusing on individual jobs, focus\\non the workflow as a whole. Once a workflow is defined, the underlying scheduler\\nusually works with an orchestrator to allocate resources to run the workflow, as\\nshown in Figure 10-8.\\n314 | Chapter 10: Infrastructure and Tooling for MLOps', \"Figure 10-8. After a workflow is defined, the tasks in this workflow are scheduled and\\norchestrated\\nThere are many articles online comparing different data science workflow manage‐\\nment tools. In this section, we’ll go over five of the most common tools: Airflow,\\nArgo, Prefect, Kubeflow, and Metaflow. This section isn’t meant to be a comprehen‐\\nsive comparison of those tools, but to give you an idea of different features a work‐\\nflow management tool might need.\\nOriginally developed at Airbnb and released in 2014, Airflow is one of the earliest\\nworkflow orchestrators. It’s an amazing task scheduler that comes with a huge library\\nof operators that makes it easy to use Airflow with different cloud providers, data‐\\nbases, storage options, and so on. Airflow is a champion of the “configuration as\\ncode” principle. Its creators believed that data workflows are complex and should be\\ndefined using code (Python) instead of YAML or other declarative language. Here’s an\\nexample of an Airflow workflow, drawn from the platform’s GitHub repository:\\nfrom datetime import datetime, timedelta\\nfrom airflow import DAG\\nfrom airflow.operators.bash import BashOperator\\nfrom airflow.providers.docker.operators.docker import DockerOperator\\n  \\ndag = DAG(\\n    'docker_sample',\\n    default_args={'retries': 1},\\n    schedule_interval=timedelta(minutes=10),\\n    start_date=datetime(2021, 1, 1),\\n    catchup=False,\\n)\\n  \\nt1 = BashOperator(task_id='print_date', bash_command='date', dag=dag)\\nt2 = BashOperator(task_id='sleep', bash_command='sleep 5', retries=3, dag=dag)\\nt3 = DockerOperator(\\n    docker_url='tcp://localhost:2375',  # Set your docker URL\\n    command='/bin/sleep 30',\\n    image='centos:latest',\\nResource Management | 315\", 'network_mode=\\'bridge\\',\\n    task_id=\\'docker_op_tester\\',\\n    dag=dag,\\n)\\n  \\nt4 = BashOperator(\\n    task_id=\\'print_hello\\', \\n    bash_command=\\'echo \"hello world!!!\"\\', \\n    dag=dag\\n)\\n  \\nt1 >> t2\\nt1 >> t3\\nt3 >> t4   \\nHowever, because Airflow was created earlier than most other tools, it had no tool to\\nlearn lessons from and suffers from many drawbacks, as discussed in detail in a blog\\npost by Uber Engineering. Here, we’ll go over only three to give you an idea.\\nFirst, Airflow is monolithic, which means it packages the entire workflow into one\\ncontainer. If two different steps in your workflow have different requirements, you\\ncan, in theory, create different containers for them using Airflow’s DockerOperator,\\nbut it’s not that easy to do so.\\nSecond, Airflow’s DAGs are not parameterized, which means you can’t pass param‐\\neters into your workflows. So if you want to run the same model with different\\nlearning rates, you’ll have to create different workflows.\\nThird, Airflow’s DAGs are static, which means it can’t automatically create new steps\\nat runtime as needed. Imagine you’re reading from a database and you want to create\\na step to process each record in the database (e.g., to make a prediction), but you\\ndon’t know in advance how many records there are in the database. Airflow won’t be\\nable to handle that.\\nThe next generation of workflow orchestrators (Argo, Prefect) were created to\\naddress different drawbacks of Airflow.\\nPrefect’s CEO, Jeremiah Lowin, was a core contributor of Airflow. Their early mar‐\\nketing campaign drew intense comparison  between Prefect and Airflow. Prefect’s\\nworkflows are parameterized and dynamic, a vast improvement compared to Airflow.\\nIt also follows the “configuration as code” principle so workflows are defined in\\nPython.\\nHowever, like Airflow, containerized steps aren’t the first priority of Prefect. Y ou can\\nrun each step in a container, but you’ll still have to deal with Dockerfiles and register\\nyour docker with your workflows in Prefect.\\nArgo addresses the container problem. Every step in an Argo workflow is run in its\\nown container. However, Argo’s workflows are defined in YAML, which allows you\\n316 | Chapter 10: Infrastructure and Tooling for MLOps', 'to define each step and its requirements in the same file. The following code sample,\\ndrawn from the Argo GitHub repository , demonstrates how to create a workflow to\\nshow a coin flip:\\napiVersion: argoproj.io/v1alpha1\\nkind: Workflow\\nmetadata:\\n  generateName: coinflip-\\n  annotations:\\n    workflows.argoproj.io/description: |\\n      This is an example of coin flip defined as a sequence of conditional steps.\\n      You can also run it in Python: \\n      https://couler-proj.github.io/couler/examples/#coin-flip\\nspec:\\n  entrypoint: coinflip\\n  templates:\\n  - name: coinflip\\n    steps:\\n    - - name: flip-coin\\n        template: flip-coin\\n    - - name: heads\\n        template: heads\\n        when: \"{{steps.flip-coin.outputs.result}} == heads\"\\n      - name: tails\\n        template: tails\\n        when: \"{{steps.flip-coin.outputs.result}} == tails\"\\n    \\n    - name: flip-coin\\n      script:\\n        image: python:alpine3.6\\n        command: [python]\\n        source: |\\n          import random\\n          result = \"heads\" if random.randint(0,1) == 0 else \"tails\"\\n          print(result)\\n    - name: heads\\n      container:\\n        image: alpine:3.6\\n        command: [sh, -c]\\n        args: [\"echo \\\\\"it was heads\\\\\"\"]\\n    \\n    - name: tails\\n      container:\\n        image: alpine:3.6\\n        command: [sh, -c]\\n        args: [\"echo \\\\\"it was tails\\\\\"\"]\\nResource Management | 317', 'The main drawback of Argo, other than its messy YAML files, is that it can only run\\non K8s clusters, which are only available in production. If you want to test the same\\nworkflow locally, you’ll have to use minikube to simulate a K8s on your laptop, which\\ncan get messy.\\nEnter Kubeflow and Metaflow, the two tools that aim to help you run the workflow\\nin both dev and prod environments by abstracting away infrastructure boilerplate\\ncode usually needed to run Airflow or Argo. They promise to give data scientists\\naccess to the full compute power of the prod environment from local notebooks,\\nwhich effectively allows data scientists to use the same code in both dev and prod\\nenvironments.\\nEven though both tools have some scheduling capacity, they are meant to be used\\nwith a bona fide scheduler and orchestrator. One component of Kubeflow is Kube‐\\nflow Pipelines, which is built on top of Argo, and it’s meant to be used on top of K8s.\\nMetaflow can be used with AWS Batch or K8s.\\nBoth tools are fully parameterized and dynamic. Currently, Kubeflow is the more\\npopular one. However, from a user experience perspective, Metaflow is superior, in\\nmy opinion. In Kubeflow, while you can define your workflow in Python, you still\\nhave to write a Dockerfile and a YAML file to specify the specs of each component\\n(e.g., process data, train, deploy) before you can stitch them together in a Python\\nworkflow. Basically, Kubeflow helps you abstract away other tools’ boilerplate by\\nmaking you write Kubeflow boilerplate.\\nIn Metaflow, you can use a Python decorator @conda to specify the requirements for\\neach step—required libraries, memory and compute requirements—and Metaflow\\nwill automatically create a container with all these requirements to execute the step.\\nY ou save on Dockerfiles or YAML files.\\nMetaflow allows you to work seamlessly with both dev and prod environments from\\nthe same notebook/script. Y ou can run experiments with small datasets on local\\nmachines, and when you’re ready to run with the large dataset on the cloud, simply\\nadd @batch decorator to execute it on AWS Batch. Y ou can even run different steps in\\nthe same workflow in different environments. For example, if a step requires a small\\nmemory footprint, it can run on your local machine. But if the next step requires a\\nlarge memory footprint, you can just add @batch to execute it on the cloud.\\n# Example: sketch of a recommender system that uses an ensemble of two models.', '# Example: sketch of a recommender system that uses an ensemble of two models. \\n# Model A will be run on your local machine and model B will be run on AWS.\\nclass RecSysFlow(FlowSpec):\\n    @step\\n    def start(self):\\n        self.data = load_data()\\n        self.next(self.fitA, self.fitB)\\n318 | Chapter 10: Infrastructure and Tooling for MLOps', '# fitA requires a different version of NumPy compared to fitB\\n    @conda(libraries={\"scikit-learn\":\"0.21.1\", \"numpy\":\"1.13.0\"})\\n    @step\\n    def fitA(self):\\n        self.model = fit(self.data, model=\"A\")\\n        self.next(self.ensemble)\\n    \\n    @conda(libraries={\"numpy\":\"0.9.8\"})\\n    # Requires 2 GPU of 16GB memory\\n    @batch(gpu=2, memory=16000)\\n    @step\\n    def fitB(self):\\n        self.model = fit(self.data, model=\"B\")\\n        self.next(self.ensemble)\\n    \\n    @step\\n    def ensemble(self, inputs):\\n        self.outputs = (\\n                   (inputs.fitA.model.predict(self.data) +   \\n                    inputs.fitB.model.predict(self.data)) / 2\\n                   for input in inputs\\n        )\\n        self.next(self.end)\\n    def end(self):\\n        print(self.outputs)\\nML Platform\\nThe manager of the ML platform team at a major streaming company told me the\\nstory of how his team got started. He originally joined the company to work on\\ntheir recommender systems. To deploy their recommender systems, they needed to\\nbuild out tools such as feature management, model management, monitoring, etc.\\nLast year, his company realized that these same tools could be used by other ML\\napplications, not just recommender systems. They created a new team, the ML plat‐\\nform team, with the goal of providing shared infrastructure across ML applications.\\nBecause the recommender system team had the most mature tool, their tools were\\nadopted by other teams, and some members from the recommender system team\\nwere asked to join the new ML platform team.\\nThis story represents a growing trend since early 2020. As each company finds uses\\nfor ML in more and more applications, there’s more to be gained by leveraging the\\nsame set of tools for multiple applications instead of supporting a separate set of tools\\nfor each application. This shared set of tools for ML deployment makes up the ML\\nplatform.\\nBecause ML platforms are relatively new, what exactly constitutes an ML platform\\nvaries from company to company. Even within the same company, it’s an ongoing\\nML Platform | 319', 'discussion. Here, I’ll focus on the components that I most often see in ML platforms,\\nwhich include model development, model store, and feature store.\\nEvaluating a tool for each of these categories depends on your use case. However,\\nhere are two general aspects you might want to keep in mind:\\nWhether the tool works with your cloud provider or allows you to use it on your own\\ndata center\\nY ou’ll need to run and serve your models from a compute layer, and usually tools\\nonly support integration with a handful of cloud providers. Nobody likes having\\nto adopt a new cloud provider for another tool.\\nWhether it’s open source or a managed service\\nIf it’s open source, you can host it yourself and have to worry less about\\ndata security and privacy. However, self-hosting means extra engineering time\\nrequired to maintain it. If it’s managed service, your models and likely some of\\nyour data will be on its service, which might not work for certain regulations.\\nSome managed services work with virtual private clouds, which allows you to\\ndeploy your machines in your own cloud clusters, helping with compliance. We’ll\\ndiscuss this more in the section “Build Versus Buy” on page 327.\\nLet’s start with the first component: model deployment.\\nModel Deployment\\nOnce a model is trained (and hopefully tested), you want to make its predictive\\ncapability accessible to users. In Chapter 7, we talked at length on how a model can\\nserve its predictions: online or batch prediction. We also discussed how the simplest\\nway to deploy a model is to push your model and its dependencies to a location\\naccessible in production then expose your model as an endpoint to your users. If you\\ndo online prediction, this endpoint will provoke your model to generate a prediction.\\nIf you do batch prediction, this endpoint will fetch a precomputed prediction.\\nA deployment service can help with both pushing your models and their depen‐\\ndencies to production and exposing your models as endpoints. Since deploying is\\nthe name of the game, deployment is the most mature among all ML platform\\ncomponents, and many tools exist for this. All major cloud providers offer tools\\nfor deployment: AWS with SageMaker, GCP with Vertex AI, Azure with Azure ML,\\nAlibaba with Machine Learning Studio, and so on. There are also a myriad of startups\\nthat offer model deployment tools such as MLflow Models , Seldon, Cortex, Ray\\nServe, and so on.\\nWhen looking into a deployment tool, it’s important to consider how easy it is to', 'Alibaba with Machine Learning Studio, and so on. There are also a myriad of startups\\nthat offer model deployment tools such as MLflow Models , Seldon, Cortex, Ray\\nServe, and so on.\\nWhen looking into a deployment tool, it’s important to consider how easy it is to\\ndo both online prediction and batch prediction with the tool. While it’s usually\\nstraightforward to do online prediction at a smaller scale with most deployment\\n320 | Chapter 10: Infrastructure and Tooling for MLOps', '29 When doing online prediction at a smaller scale, you can just hit an endpoint with payloads and get back\\npredictions. Batch prediction requires setting up batch jobs and storing predictions.\\nservices, doing batch prediction is usually trickier. 29 Some tools allow you to batch\\nrequests together for online prediction, which is different from batch prediction.\\nMany companies have separate deployment pipelines for online prediction and batch\\nprediction. For example, they might use Seldon for online prediction but leverage\\nDatabricks for batch prediction.\\nAn open problem with model deployment is how to ensure the quality of a model\\nbefore it’s deployed. In Chapter 9 , we talked about different techniques for test in\\nproduction such as shadow deployment, canary release, A/B testing, and so on. When\\nchoosing a deployment service, you might want to check whether this service makes\\nit easy for you to perform the tests that you want.\\nModel Store\\nMany companies dismiss model stores because they sound simple. In the section\\n“Model Deployment” on page 320, we talked about how, to deploy a model, you have\\nto package your model and upload it to a location accessible in production. Model\\nstore suggests that it stores models—you can do so by uploading your models to\\nstorage like S3. However, it’s not quite that simple. Imagine now that your model’s\\nperformance dropped for a group of inputs. The person who was alerted to the\\nproblem is a DevOps engineer, who, after looking into the problem, decided that she\\nneeded to inform the data scientist who created this model. But there might be 20\\ndata scientists in the company; who should she ping?\\nImagine now that the right data scientist is looped in. The data scientist first wants to\\nreproduce the problems locally. She still has the notebook she used to generate this\\nmodel and the final model, so she starts the notebook and uses the model with the\\nproblematic sets of inputs. To her surprise, the outputs the model produces locally are\\ndifferent from the outputs produced in production. Many things could have caused\\nthis discrepancy; here are just a few examples:\\n• The model being used in production right now is not the same model that she•\\nhas locally. Perhaps she uploaded the wrong model binary to production?\\n• The model being used in production is correct, but the list of features used•\\nis wrong. Perhaps she forgot to rebuild the code locally before pushing it to\\nproduction?\\nML Platform | 321', '• The model is correct, the feature list is correct, but the featurization code is•\\noutdated.\\n• The model is correct, the feature list is correct, the featurization code is correct,•\\nbut something is wrong with the data processing pipeline.\\nWithout knowing the cause of the problem, it’ll be very difficult to fix it. In this\\nsimple example, we assume that the data scientist responsible still has access to the\\ncode used to generate the model. What if that data scientist no longer has access to\\nthat notebook, or she has already quit or is on vacation?\\nMany companies have realized that storing the model alone in blob storage isn’t\\nenough. To help with debugging and maintenance, it’s important to track as much\\ninformation associated with a model as possible. Here are eight types of artifacts that\\nyou might want to store. Note that many artifacts mentioned here are information\\nthat should be included in the model card, as discussed in the section “Create model\\ncards” on page 351.\\nModel definition\\nThis is the information needed to create the shape of the model, e.g., what loss\\nfunction it uses. If it’s a neural network, this includes how many hidden layers it\\nhas and how many parameters are in each layer.\\nModel parameters\\nThese are the actual values of the parameters of your model. These values are\\nthen combined with the model’s shape to re-create a model that can be used to\\nmake predictions. Some frameworks allow you to export both the parameters\\nand the model definition together.\\nFeaturize and predict functions\\nGiven a prediction request, how do you extract features and input these features\\ninto the model to get back a prediction? The featurize and predict functions pro‐\\nvide the instruction to do so. These functions are usually wrapped in endpoints.\\nDependencies\\nThe dependencies—e.g., Python version, Python packages—needed to run your\\nmodel are usually packaged together into a container.\\nData\\nThe data used to train this model might be pointers to the location where the\\ndata is stored or the name/version of your data. If you use tools like DVC to\\nversion your data, this can be the DVC commit that generated the data.\\n322 | Chapter 10: Infrastructure and Tooling for MLOps', 'Model generation code\\nThis is the code that specifies how your model was created, such as:\\n• What frameworks it used•\\n• How it was trained•\\n• The details on how the train/valid/test splits were created•\\n• The number of experiments run•\\n• The range of hyperparameters considered•\\n• The actual set of hyperparameters that final model used•\\nVery often, data scientists generate models by writing code in notebooks. Com‐\\npanies with more mature pipelines make their data scientists commit the model\\ngeneration code into their Git repos on GitHub or GitLab. However, in many\\ncompanies, this process is ad hoc, and data scientists don’t even check in their\\nnotebooks. If the data scientist responsible for the model loses the notebook or\\nquits or goes on vacation, there’s no way to map a model in production to the\\ncode that generated it for debugging or maintenance.\\nExperiment artifacts\\nThese are the artifacts generated during the model development process, as dis‐\\ncussed in the section “Experiment Tracking and Versioning” on page 162. These\\nartifacts can be graphs like the loss curve. These artifacts can be raw numbers like\\nthe model’s performance on the test set.\\nTags\\nThis includes tags to help with model discovery and filtering, such as owner (the\\nperson or the team who is the owner of this model) or task (the business problem\\nthis model solves, like fraud detection).\\nMost companies store a subset, but not all, of these artifacts. The artifacts a company\\nstores might not be in the same place but scattered. For example, model definitions\\nand model parameters might be in S3. Containers that contain dependencies might\\nbe in ECS (Elastic Container Service). Data might be in Snowflake. Experiment\\nartifacts might be in Weights & Biases. Featurize and prediction functions might be in\\nAWS Lambda. Some data scientists might manually keep track of these locations in,\\nsay, a README, but this file can be easily lost.\\nML Platform | 323', 'A model store that can store sufficient general use cases is far from being a solved\\nproblem. As of writing this book, MLflow is undoubtedly the most popular model\\nstore that isn’t associated with a major cloud provider. Y et three out of the six top\\nMLflow questions on Stack Overflow are about storing and accessing artifacts in\\nMLflow, as shown in Figure 10-9. Model stores are due for a makeover, and I hope\\nthat in the near future a startup will step up and solve this.\\nFigure 10-9. MLflow is the most popular model store, yet it’s far from solving the artifact\\nproblem. Three out of the six top MLflow questions on Stack Overflow are about storing\\nand accessing artifacts in MLflow. Source: Screenshot of Stack Overflow page\\n324 | Chapter 10: Infrastructure and Tooling for MLOps', '30 Neal Lathia, “Building a Feature Store, ” December 5, 2020, https://oreil.ly/DgsvA; Jordan Volz, “Why Y ou Need\\na Feature Store, ” Continual, September 28, 2021, https://oreil.ly/kQPMb; Mike Del Balso, “What Is a Feature\\nStore?” Tecton, October 20, 2020, https://oreil.ly/pzy0I.\\nBecause of the lack of a good model store solution, companies like Stitch Fix resolve\\nto build their own model store. Figure 10-10  shows the artifacts that Stitch Fix’s\\nmodel store tracks. When a model is uploaded to their model store, this model comes\\nwith the link to the serialized model, the dependencies needed to run the model\\n(Python environment), the Git commit where the model code generation is created\\n(Git information), tags (to at least specify the team that owns the model), etc.\\nFigure 10-10. Artifacts that Stitch Fix’s model store tracks. Source: Adapted from a slide\\nby Stefan Krawczyk for CS 329S (Stanford).\\nFeature Store\\n“Feature store” is an increasingly loaded term that can be used by different people\\nto refer to very different things. There have been many attempts by ML practitioners\\nto define what features a feature store should have. 30 At its core, there are three\\nmain problems that a feature store can help address: feature management, feature\\nML Platform | 325', '31 Jeremy Hermann and Mike Del Balso, “Meet Michelangelo: Uber’s Machine Learning Platform, ” Uber Engi‐\\nneering, September 5, 2017, https://oreil.ly/XteNy.\\n32 Some people use the term “feature transformation. ”\\ntransformation, and feature consistency. A feature store solution might address one\\nor a combination of these problems:\\nFeature management\\nA company might have multiple ML models, each model using a lot of features.\\nBack in 2017, Uber had about 10,000 features across teams! 31 It’s often the case\\nthat features used for one model can be useful for another model. For example,\\nteam A might have a model to predict how likely a user will churn, and team\\nB has a model to predict how likely a free user will convert into a paid user.\\nThere are many features that these two models can share. If team A discovers that\\nfeature X is super useful, team B might be able to leverage that too.\\nA feature store can help teams share and discover features, as well as manage\\nroles and sharing settings for each feature. For example, you might not want\\neveryone in the company to have access to sensitive financial information of\\neither the company or its users. In this capacity, a feature store can be thought\\nof as a feature catalog. Examples of tools for feature management are Amundsen\\n(developed at Lyft) and DataHub (developed at LinkedIn).\\nFeature computation32\\nFeature engineering logic, after being defined, needs to be computed. For exam‐\\nple, the feature logic might be: use the average meal preparation time from\\nyesterday. The computation part involves actually looking into your data and\\ncomputing this average.\\nIn the previous point, we discussed how multiple models might share a feature.\\nIf the computation of this feature isn’t too expensive, it might be acceptable\\ncomputing this feature each time it is required by a model. However, if the\\ncomputation is expensive, you might want to execute it only once the first time it\\nis required, then store it for feature uses.\\nA feature store can help with both performing feature computation and storing\\nthe results of this computation. In this capacity, a feature store acts like a data\\nwarehouse.\\nFeature consistency\\nIn Chapter 7 , we talked about the problem of having two separate pipelines\\nfor the same model: the training pipeline extracts batch features from historical\\ndata and the inference pipeline extracts streaming features. During development,\\ndata scientists might define features and create models using Python. Production', 'for the same model: the training pipeline extracts batch features from historical\\ndata and the inference pipeline extracts streaming features. During development,\\ndata scientists might define features and create models using Python. Production\\n326 | Chapter 10: Infrastructure and Tooling for MLOps', 'code, however, might be written in another language, such as Java or C, for\\nperformance.\\nThis means that feature definitions written in Python during development might\\nneed to be converted into the languages used in production. So you have to\\nwrite the same features twice, once for training and once for inference. First, it’s\\nannoying and time-consuming. Second, it creates extra surface for bugs since one\\nor more features in production might differ from their counterparts in training,\\ncausing weird model behaviors.\\nA key selling point of modern feature stores is that they unify the logic for both\\nbatch features and streaming features, ensuring the consistency between features\\nduring training and features during inference.\\nFeature store is a newer category that only started taking off around 2020. While\\nit’s generally agreed that feature stores should manage feature definitions and ensure\\nfeature consistency, their exact capacities vary from vendor to vendor. Some feature\\nstores only manage feature definitions without computing features from data; some\\nfeature stores do both. Some feature stores also do feature validation, i.e., detecting\\nwhen a feature doesn’t conform to a predefined schema, and some feature stores leave\\nthat aspect to a monitoring tool.\\nAs of writing this book, the most popular open source feature store is Feast. However,\\nFeast’s strength is in batch features, not streaming features. Tecton is a fully managed\\nfeature store that promises to be able to handle both batch features and online\\nfeatures, but their actual traction is slow because they require deep integration. Plat‐\\nforms like SageMaker and Databricks also offer their own interpretations of feature\\nstores. Out of 95 companies I surveyed in January 2022, only around 40% of them\\nuse a feature store. Out of those who use a feature store, half of them build their own\\nfeature store.\\nBuild Versus Buy\\nAt the beginning of this chapter, we discussed how difficult it is to set up the right\\ninfrastructure for your ML needs. What infrastructure you need depends on the\\napplications you have and the scale at which you run these applications.\\nHow much you need to invest into infrastructure also depends on what you want\\nto build in-house and what you want to buy. For example, if you want to use fully\\nmanaged Databricks clusters, you probably need only one engineer. However, if you\\nwant to host your own Spark Elastic MapReduce clusters, you might need five more\\npeople.', 'to build in-house and what you want to buy. For example, if you want to use fully\\nmanaged Databricks clusters, you probably need only one engineer. However, if you\\nwant to host your own Spark Elastic MapReduce clusters, you might need five more\\npeople.\\nAt one extreme, you can outsource all your ML use cases to a company that provides\\nML applications end-to-end, and then perhaps the only piece of infrastructure you\\nBuild Versus Buy | 327', 'need is for data movement: moving your data from your applications to your vendor,\\nand moving predictions from that vendor back to your users. The rest of your\\ninfrastructure is managed by your vendor.\\nAt the other extreme, if you’re a company that handles sensitive data that prevents\\nyou from using services managed by another company, you might need to build and\\nmaintain all your infrastructure in-house, even having your own data centers.\\nMost companies, however, are in neither of these extremes. If you work for one of\\nthese companies, you’ll likely have some components managed by other companies\\nand some components developed in-house. For example, your compute might be\\nmanaged by AWS EC2 and your data warehouse managed by Snowflake, but you\\nhave your own feature store and your own monitoring dashboards.\\nY our build versus buy decisions depend on many factors. Here, we’ll discuss three\\ncommon ones that I often encounter when talking with heads of infrastructures on\\nhow they evaluate these decisions:\\nThe stage your company is at\\nIn the beginning, you might want to leverage vendor solutions to get started\\nas quickly as possible so that you can focus your limited resources on the\\ncore offerings of your product. As your use cases grow, however, vendor costs\\nmight become exorbitant and it might be cheaper for you to invest in your own\\nsolution.\\nWhat you believe to be the focus or the competitive advantages of your company\\nStefan Krawczyk, manager of the ML platform team at Stitch Fix, explained\\nto me his build versus buy decision: “If it’s something we want to be really\\ngood at, we’ll manage that in-house. If not, we’ll use a vendor. ” For the vast\\nmajority of companies outside the technology sector—e.g., companies in retail,\\nbanking, manufacturing—ML infrastructure isn’t their focus, so they tend to bias\\ntoward buying. When I talk to these companies, they prefer managed services,\\neven point solutions (e.g., solutions that solve a business problem for them, like\\na demand forecasting service). For many tech companies where technology is\\ntheir competitive advantage, and whose strong engineering teams prefer to have\\ncontrol over their stacks, they tend to bias toward building. If they use a managed\\nservice, they might prefer that service to be modular and customizable, so that\\nthey can plug and play with any component.\\nThe maturity of the available tools\\nFor example, your team might decide that you need a model store, and you’ d', 'service, they might prefer that service to be modular and customizable, so that\\nthey can plug and play with any component.\\nThe maturity of the available tools\\nFor example, your team might decide that you need a model store, and you’ d\\nhave preferred to use a vendor, but there’s no vendor mature enough for your\\nneeds, so you have to build your own feature store, perhaps on top of an open\\nsource solution.\\n328 | Chapter 10: Infrastructure and Tooling for MLOps', '33 Erik Bernhardsson on Twitter (@bernhardsson), September 29, 2021, https://oreil.ly/GnxOH.\\nThis is what happens in the early days of ML adoption in the industry. Com‐\\npanies that are early adopters, i.e., big tech companies, build out their own\\ninfrastructure because there are no solutions mature enough for their needs. This\\nleads to the situation where every company’s infrastructure is different. A few\\nyears later, solution offerings mature. However, these offerings find it difficult to\\nsell to big tech companies because it’s impossible to create a solution that works\\nwith the majority of custom infrastructure.\\nAs we’re building out Claypot AI, other founders have actually advised us to\\navoid selling to big tech companies because, if we do, we’ll get sucked into what\\nthey call “integration hell”—spending more time integrating our solution with\\ncustom infrastructure instead of building out our core features. They advised us\\nto focus on startups with much cleaner slates to build on.\\nSome people think that building is cheaper than buying, which is not necessarily\\nthe case. Building means that you’ll have to bring on more engineers to build and\\nmaintain your own infrastructure. It can also come with future cost: the cost of\\ninnovation. In-house, custom infrastructure makes it hard to adopt new technologies\\navailable because of the integration issues.\\nThe build versus buy decisions are complex, highly context-dependent, and likely\\nwhat heads of infrastructure spend much time mulling over. Erik Bernhardsson,\\nex-CTO of Better.com, said in a tweet that “one of the most important jobs of a\\nCTO is vendor/product selection and the importance of this keeps going up rapidly\\nevery year since the infrastructure space grows so fast. ” 33 There’s no way that a small\\nsection can address all its nuances. But I hope that this section provides you with\\nsome pointers to start the discussion.\\nSummary\\nIf you’ve stayed with me until now, I hope you agree that bringing ML models to\\nproduction is an infrastructural problem. To enable data scientists to develop and\\ndeploy ML models, it’s crucial to have the right tools and infrastructure set up.\\nIn this chapter, we covered different layers of infrastructure needed for ML systems.\\nWe started from the storage and compute layer, which provides vital resources for\\nany engineering project that requires intensive data and compute resources like ML\\nprojects. The storage and compute layer is heavily commoditized, which means that', 'We started from the storage and compute layer, which provides vital resources for\\nany engineering project that requires intensive data and compute resources like ML\\nprojects. The storage and compute layer is heavily commoditized, which means that\\nmost companies pay cloud services for the exact amount of storage and compute\\nthey use instead of setting up their own data centers. However, while cloud providers\\nmake it easy for a company to get started, their cost becomes prohibitive as this\\nSummary | 329', 'company grows, and more and more large companies are looking into repatriating\\nfrom the cloud to private data centers.\\nWe then continued on to discuss the development environment where data scien‐\\ntists write code and interact with the production environment. Because the dev\\nenvironment is where engineers spend most of their time, improvements in the dev\\nenvironment translate directly into improvements in productivity. One of the first\\nthings a company can do to improve the dev environment is to standardize the dev\\nenvironment for data scientists and ML engineers working on the same team. We\\ndiscussed in this chapter why standardization is recommended and how to do so.\\nWe then discussed an infrastructural topic whose relevance to data scientists has been\\ndebated heavily in the last few years: resource management. Resource management\\nis important to data science workflows, but the question is whether data scientists\\nshould be expected to handle it. In this section, we traced the evolution of resource\\nmanagement tools from cron to schedulers to orchestrators. We also discussed why\\nML workflows are different from other software engineering workflows and why\\nthey need their own workflow management tools. We compared various workflow\\nmanagement tools such as Airflow, Argo, and Metaflow.\\nML platform is a team that has emerged recently as ML adoption matures. Since it’s\\nan emerging concept, there are still disagreements on what an ML platform should\\nconsist of. We chose to focus on the three sets of tools that are essential for most ML\\nplatforms: deployment, model store, and feature store. We skipped monitoring of the\\nML platform since it’s already covered in Chapter 8.\\nWhen working on infrastructure, a question constantly haunts engineering managers\\nand CTOs alike: build or buy? We ended this chapter with a few discussion points\\nthat I hope can provide you or your team with sufficient context to make those\\ndifficult decisions.\\n330 | Chapter 10: Infrastructure and Tooling for MLOps', '1 Sometimes, you can get different results if you run the same model on the same input twice\\nat the exact same time.\\nCHAPTER 11\\nThe Human Side of Machine Learning\\nThroughout this book, we’ve covered many technical aspects of designing an ML\\nsystem. However, ML systems aren’t just technical. They involve business decision\\nmakers, users, and, of course, developers of the systems. We’ve discussed stakeholders\\nand their objectives in Chapters 1 and 2. In this chapter, we’ll discuss how users and\\ndevelopers of ML systems might interact with these systems.\\nWe’ll first consider how user experience might be altered and affected due to the\\nprobabilistic nature of ML models. We’ll continue to discuss organizational structure\\nto allow different developers of the same ML system to work together effectively.\\nWe’ll end the chapter with how ML systems can affect the society as a whole in the\\nsection “Responsible AI” on page 339.\\nUser Experience\\nWe’ve discussed at length how ML systems behave differently from traditional soft‐\\nware systems. First, ML systems are probabilistic instead of deterministic. Usually, if\\nyou run the same software on the same input twice at different times, you can expect\\nthe same result. However, if you run the same ML system twice at different times on\\nthe exact same input, you might get different results.1 Second, due to this probabilistic\\nnature, ML systems’ predictions are mostly correct, and the hard part is we usually\\ndon’t know for what inputs the system will be correct! Third, ML systems can also be\\nlarge and might take an unexpectedly long time to produce a prediction.\\nThese differences mean that ML systems can affect user experience differently, espe‐\\ncially for users that have so far been used to traditional software. Due to the relatively\\n331', 'new usage of ML in the real world, how ML systems affect user experience is still not\\nwell studied. In this section, we’ll discuss three challenges that ML systems pose to\\ngood user experience and how to address them.\\nEnsuring User Experience Consistency\\nWhen using an app or a website, users expect a certain level of consistency. For\\nexample, I’m used to Chrome having their “minimize” button on the top left corner\\non my MacBook. If Chrome moved this button to the right, I’ d be confused, even\\nfrustrated.\\nML predictions are probabilistic and inconsistent, which means that predictions gen‐\\nerated for one user today might be different from what will be generated for the same\\nuser the next day, depending on the context of the predictions. For tasks that want to\\nleverage ML to improve users’ experience, the inconsistency in ML predictions can be\\na hindrance.\\nTo make this concrete, consider a case study  published by Booking.com in 2020.\\nWhen you book accommodations on Booking.com, there are about 200 filters you\\ncan use to specify your preferences, such as “breakfast included, ” “pet friendly, ” and\\n“non-smoking rooms. ” There are so many filters that it takes time for users to find\\nthe filters that they want. The applied ML team at Booking.com wanted to use ML to\\nautomatically suggest filters that a user might want, based on the filters they’ve used\\nin a given browsing session.\\nThe challenge they encountered is that if their ML model kept suggesting different\\nfilters each time, users could get confused, especially if they couldn’t find a filter that\\nthey had already applied before. The team resolved this challenge by creating a rule to\\nspecify the conditions in which the system must return the same filter recommenda‐\\ntions (e.g., when the user has applied a filter) and the conditions in which the system\\ncan return new recommendations (e.g., when the user changes their destination).\\nThis is known as the consistency–accuracy trade-off, since the recommendations\\ndeemed most accurate by the system might not be the recommendations that can\\nprovide user consistency.\\nCombatting “Mostly Correct” Predictions\\nIn the previous section, we talked about the importance of ensuring the consistency\\nof a model’s predictions. In this section, we’ll talk about how, in some cases, we want\\nless consistency and more diversity in a model’s predictions.\\nSince 2018, the large language model GPT and its successors, GPT-2 and GPT-3,', 'of a model’s predictions. In this section, we’ll talk about how, in some cases, we want\\nless consistency and more diversity in a model’s predictions.\\nSince 2018, the large language model GPT and its successors, GPT-2 and GPT-3,\\nhave been taking the world by storm. An advantage of these large language models\\nis that they’re able to generate predictions for a wide range of tasks with little to no\\ntask-specific training data required. For example, you can use the requirements for a\\n332 | Chapter 11: The Human Side of Machine Learning', 'web page as an input to the model, and it’ll output the React code needed to create\\nthat web page, as shown in Figure 11-1.\\nFigure 11-1. GPT-3 can help you write code for your website. Source: Adapted from\\nscreenshots of a video by Sharif Shameem\\nHowever, a drawback of these models is that these predictions are not always correct,\\nand it’s very expensive to fine-tune them on task-specific data to improve their\\npredictions. These mostly correct predictions can be useful for users who can easily\\ncorrect them. For example, in the case of customer support, for each customer\\nrequest, ML systems can produce mostly correct responses and the human operators\\ncan quickly edit those responses. This can speed up the response compared to having\\nto write the response from scratch.\\nHowever, these mostly correct predictions won’t be very useful if users don’t know\\nhow to or can’t correct the responses. Consider the same task of using a language\\nmodel to generate React code for a web page. The generated code might not work,\\nor if it does, it might not render to a web page that meets the specified requirements.\\nA React engineer might be able to fix this code quickly, but many users of this\\napplication might not know React. And this application might attract a lot of users\\nwho don’t know React—that’s why they needed this app in the first place!\\nTo overcome this, an approach is to show users multiple resulting predictions for\\nthe same input to increase the chance of at least one of them being correct. These\\npredictions should be rendered in a way that even nonexpert users can evaluate them.\\nIn this case, given a set of requirements input by users, you can have the model\\nUser Experience | 333', 'produce multiple snippets of React code. The code snippets are rendered into visual\\nweb pages so that nonengineering users can evaluate which one is the best for them.\\nThis approach is very common and is sometimes called “human-in-the-loop” AI, as it\\ninvolves humans to pick the best predictions or to improve on the machine-generated\\npredictions. For readers interested in human-in-the-loop AI, I’ d highly recommend\\nJessy Lin’s “Rethinking Human-AI Interaction”.\\nSmooth Failing\\nWe’ve talked at length about the effect of an ML model’s inference latency on user\\nexperience in the section “Computational priorities” on page 15. We’ve also discussed\\nhow to compress models and optimize them for faster inference speed in the section\\n“Model Compression” on page 206. However, normally fast models might still take\\ntime with certain queries. This can happen especially with models that deal with\\nsequential data like language models or time-series models—e.g., the model takes\\nlonger to process long series than shorter series. What should we do with the queries\\nwhere models take too long to respond?\\nSome companies that I’ve worked with use a backup system that is less optimal than\\nthe main system but is guaranteed to generate predictions quickly. These systems can\\nbe heuristics or simple models. They can even be cached precomputed predictions.\\nThis means that you might have a rule that specifies: if the main model takes longer\\nthan X milliseconds to generate predictions, use the backup model instead. Some\\ncompanies, instead of having this simple rule, have another model to predict how\\nlong it’ll take the main model to generate predictions for a given query, and route that\\nprediction to either the main model or the backup model accordingly. Of course, this\\nadded model might also add extra inference latency to your system.\\nThis is related to the speed–accuracy trade-off: a model might have worse perfor‐\\nmance than another model but can do inference much faster. This less-optimal\\nbut fast model might give users worse predictions but might still be preferred in\\nsituations where latency is crucial. Many companies have to choose one model over\\nanother, but with a backup system, you can do both.\\nTeam Structure\\nAn ML project involves not only data scientists and ML engineers, but also other\\ntypes of engineers such as DevOps engineers and platform engineers as well as\\nnondeveloper stakeholders like subject matter experts (SMEs). Given a diverse set', 'Team Structure\\nAn ML project involves not only data scientists and ML engineers, but also other\\ntypes of engineers such as DevOps engineers and platform engineers as well as\\nnondeveloper stakeholders like subject matter experts (SMEs). Given a diverse set\\nof stakeholders, the question is what is the optimal structure when organizing ML\\nteams. We’ll focus on two aspects: cross-functional teams collaboration and the much\\ndebated role of an end-to-end data scientist.\\n334 | Chapter 11: The Human Side of Machine Learning', 'Cross-functional Teams Collaboration\\nSMEs (doctors, lawyers, bankers, farmers, stylists, etc.) are often overlooked in the\\ndesign of ML systems, but many ML systems wouldn’t work without subject matter\\nexpertise. They’re not only users but also developers of ML systems.\\nMost people only think of subject matter expertise during the data labeling phase—\\ne.g., you’ d need trained professionals to label whether a CT scan of a lung shows\\nsigns of cancer. However, as training ML models becomes an ongoing process in\\nproduction, labeling and relabeling might also become an ongoing process spanning\\nthe entire project lifecycle. An ML system would benefit a lot to have SMEs involved\\nin the rest of the lifecycle, such as problem formulation, feature engineering, error\\nanalysis, model evaluation, reranking predictions, and user interface: how to best\\npresent results to users and/or to other parts of the system.\\nThere are many challenges that arise from having multiple different profiles working\\non a project. For example, how do you explain ML algorithms’ limitations and\\ncapacities to SMEs who might not have engineering or statistical backgrounds? To\\nbuild an ML system, we want everything to be versioned, but how do you translate\\ndomain expertise (e.g., if there’s a small dot in this region between X and Y then it\\nmight be a sign of cancer) into code and version that?\\nGood luck trying to get your doctor to use Git.\\nIt’s important to involve SMEs early on in the project planning phase and empower\\nthem to make contributions without having to burden engineers to give them access.\\nFor example, to help SMEs get more involved in the development of ML systems,\\nmany companies are building no-code/low-code platforms that allow people to make\\nchanges without writing code. Most of the no-code ML solutions for SMEs are\\ncurrently at the labeling, quality assurance, and feedback stages, but more platforms\\nare being developed to aid in other critical junctions such as dataset creation and\\nviews for investigating issues that require SME input.\\nEnd-to-End Data Scientists\\nThrough this book, I hope I’ve convinced you that ML production is not just an\\nML problem but also an infrastructure problem. To do MLOps, we need not only\\nML expertise but also Ops (operational) expertise, especially around deployment,\\ncontainerization, job orchestration, and workflow management.\\nTo be able to bring all these areas of expertise into an ML project, companies tend', 'ML expertise but also Ops (operational) expertise, especially around deployment,\\ncontainerization, job orchestration, and workflow management.\\nTo be able to bring all these areas of expertise into an ML project, companies tend\\nto follow one of the two following approaches: have a separate team to manage all\\nthe Ops aspects or include data scientists on the team and have them own the entire\\nprocess.\\nLet’s take a closer look at how each of these approaches works in practice.\\nTeam Structure | 335', 'Approach 1: Have a separate team to manage production\\nIn this approach, the data science/ML team develops models in the dev environment.\\nThen a separate team, usually the Ops/platform/ML engineering team, production‐\\nizes the models in prod. This approach makes hiring easier as it’s easier to hire people\\nwith one set of skills instead of people with multiple sets of skills. It might also\\nmake life easier for each person involved, as they only have to focus on one concern\\n(e.g., developing models or deploying models). However, this approach has many\\ndrawbacks:\\nCommunication and coordination overhead\\nA team can become blockers for other teams. According to Frederick P . Brooks,\\n“What one programmer can do in one month, two programmers can do in two\\nmonths. ”\\nDebugging challenges\\nWhen something fails, you don’t know whether your team’s code or some other\\nteam’s code might have caused it. It might not have been because of your compa‐\\nny’s code at all. Y ou need cooperation from multiple teams to figure out what’s\\nwrong.\\nFinger-pointing\\nEven when you’ve figured out what went wrong, each team might think it’s\\nanother team’s responsibility to fix it.\\nNarrow context\\nNo one has visibility into the entire process to optimize/improve it. For example,\\nthe platform team has ideas on how to improve the infrastructure but they can\\nonly act on requests from data scientists, but data scientists don’t have to deal\\nwith infrastructure so they have less incentives to proactively make changes to it.\\nApproach 2: Data scientists own the entire process\\nIn this approach, the data science team also has to worry about productionizing\\nmodels. Data scientists become grumpy unicorns, expected to know everything about\\nthe process, and they might end up writing more boilerplate code than data science.\\nAbout a year ago, I tweeted about a set of skills I thought was important to become an\\nML engineer or data scientist, as shown in Figure 11-2. The list covers almost every\\npart of the workflow: querying data, modeling, distributed training, and setting up\\nendpoints. It even includes tools like Kubernetes and Airflow.\\n336 | Chapter 11: The Human Side of Machine Learning', '2 Eugene Y an, “Unpopular Opinion—Data Scientists Should be More End-to-End, ” EugeneY an.com, August 9,\\n2020, https://oreil.ly/A6oPi.\\n3 Eric Colson, “Beware the Data Science Pin Factory: The Power of the Full-Stack Data Science Generalist and\\nthe Perils of Division of Labor Through Function, ” MultiThreaded, March 11, 2019, https://oreil.ly/m6WWu.\\nFigure 11-2. I used to think that a data scientist would need to know all these things\\nThe tweet seems to resonate with my audience. Eugene Y an also wrote about how\\n“data scientists should be more end-to-end. ”2 Eric Colson, Stitch Fix’s chief algorithms\\nofficer (who previously was also VP data science and engineering at Netflix), wrote a\\npost on “the power of the full-stack data science generalist and the perils of division\\nof labor through function. ”3\\nTeam Structure | 337', '4 Erik Bernhardsson on Twitter (@bernhardsson), July 20, 2021, https://oreil.ly/7X4J9.\\n5 Colson, “Beware the Data Science Pin Factory. ”\\nWhen I wrote that tweet, I believed that Kubernetes was essential to the ML work‐\\nflow. This sentiment came from the frustration at my own job—my life as an ML\\nengineer would’ve been much easier if I was more fluent with K8s.\\nHowever, as I learned more about low-level infrastructure, I realized how unreason‐\\nable it is to expect data scientists to know about it. Infrastructure requires a very\\ndifferent set of skills from data science. In theory, you can learn both sets of skills.\\nIn practice, the more time you spend on one means the less time you spend on the\\nother. I love Erik Bernhardsson’s analogy that expecting data scientists to know about\\ninfrastructure is like expecting app developers to know about how Linux kernels\\nwork.4 I joined an ML company because I wanted to spend more time with data, not\\nwith spinning up AWS instances, writing Dockerfiles, scheduling/scaling clusters, or\\ndebugging YAML configuration files.\\nFor data scientists to own the entire process, we need good tools. In other words, we\\nneed good infrastructure.\\nWhat if we have an abstraction to allow data scientists to own the process end-to-end\\nwithout having to worry about infrastructure?\\nWhat if I can just tell this tool, “Here’s where I store my data (S3), here are the\\nsteps to run my code (featurizing, modeling), here’s where my code should run (EC2\\ninstances, serverless stuff like AWS Batch, Function, etc.), here’s what my code needs\\nto run at each step (dependencies), ” and then this tool manages all the infrastructure\\nstuff for me?\\nAccording to both Stitch Fix and Netflix, the success of a full-stack data scientist\\nrelies on the tools they have. They need tools that “abstract the data scientists from\\nthe complexities of containerization, distributed processing, automatic failover, and\\nother advanced computer science concepts. ”5\\nIn Netflix’s model, the specialists—people who originally owned a part of the\\nproject—first create tools that automate their parts, as shown in Figure 11-3 . Data\\nscientists can leverage these tools to own their projects end-to-end.\\n338 | Chapter 11: The Human Side of Machine Learning', '6 “Full Cycle Developers at Netflix—Operate What Y ou Build, ” Netflix Technology Blog, May 17, 2018,\\nhttps://oreil.ly/iYgQs.\\nFigure 11-3. Full-cycle developers at Netflix. Source: Adapted from an image by Netflix6\\nWe’ve talked about how ML systems might affect user experience and how organiza‐\\ntional structure might influence productivity of ML projects. In the second half of\\nthis chapter, we’ll focus on an even more crucial consideration: how ML systems\\nmight affect society and what ML system developers should do to ensure that the\\nsystems they develop do more good than harm.\\nResponsible AI\\nThis section was written with generous contributions from Abhishek Gupta, founder\\nand principal researcher at the Montreal AI Ethics Institute . His work focuses on\\napplied technical and policy measures to build ethical, safe, and inclusive AI systems.\\nThe question of how to make intelligent systems responsible is rele‐\\nvant not only to ML systems but also general artificial intelligence\\n(AI) systems. AI is a broader term that includes ML. Therefore, in\\nthis section, we use AI instead of ML.\\nResponsible AI | 339', 'Responsible AI is the practice of designing, developing, and deploying AI systems\\nwith good intention and sufficient awareness to empower users, to engender trust,\\nand to ensure fair and positive impact to society. It consists of areas like fairness,\\nprivacy, transparency, and accountability.\\nThese terms are no longer just philosophical musings, but serious considerations for\\nboth policy makers and everyday practitioners. Given ML is being deployed into\\nalmost every aspect of our lives, failing to make our ML systems fair and ethical\\ncan lead to catastrophic consequences, as outlined in the book Weapons of Math\\nDestruction (Cathy O’Neil, Crown Books, 2016), and through other case studies\\nmentioned throughout this book.\\nAs developers of ML systems, you have the responsibility not only to think about how\\nyour systems will impact users and society at large, but also to help all stakeholders\\nbetter realize their responsibilities toward the users by concretely implementing\\nethics, safety, and inclusivity into your ML systems. This section is a brief introduc‐\\ntion to what can happen when insufficient efforts are spent to make ML systems\\nresponsible. We’ll start with two case studies of quite unfortunate and public failures\\nof ML. We will then propose a preliminary framework for data scientists and ML\\nengineers to select the tools and guidelines that best help with making your ML\\nsystems responsible.\\nDisclaimer: Responsible AI is a complex topic with growing literature that deserves\\nits own coverage and can easily span multiple books. This section is far from an\\nexhaustive guide. We only aim to give ML developers an overview to effectively\\nnavigate the developments in this field. Those interested in further reading are highly\\nrecommended to check out the following resources:\\n• NIST Special Publication 1270: Towards a Standard for Identifying and Manag‐•\\ning Bias in Artificial Intelligence\\n• ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT)•\\npublications\\n• Trustworthy ML ’s list of recommended resources and fundamental papers  for•\\nresearchers and practitioners who want to learn more about trustworthy ML\\n• Sara Hooker’s awesome slide deck  on fairness, security, and governance in•\\nmachine learning (2022)\\n• Timnit Gebru and Emily Denton’s tutorials on fairness, accountability, transpar‐•\\nency, and ethics (2020)\\n340 | Chapter 11: The Human Side of Machine Learning', '7 Elliot Jones and Cansu Safak, “Can Algorithms Ever Make the Grade?” Ada Lovelace Institute Blog, 2020,\\nhttps://oreil.ly/ztTxR.\\n8 Tom Simonite, “Skewed Grading Algorithms Fuel Backlash Beyond the Classroom, ” Wired, August 19, 2020,\\nhttps://oreil.ly/GFRet.\\n9 Ofqual, “ Awarding GCSE, AS & A Levels in Summer 2020: Interim Report, ” Gov.uk, August 13, 2020,\\nhttps://oreil.ly/r22iz.\\nIrresponsible AI: Case Studies\\nWe’ll start this section off by looking at two failures of AI systems that led to severe\\nharm for not only the users of these systems but also to the organizations who devel‐\\noped the systems. We’ll trace some of the places where the organizations went wrong\\nand what the practitioners could have done to potentially anticipate these points of\\nfailure. These highlights will serve as background as we dive into the engineering\\nframework for responsible AI.\\nThere are other interesting examples of “ AI incidents” logged at the AI Incident\\nDatabase. Keep in mind that while the following two examples and the ones logged\\nat AI Incident Database are the ones that caught attention, there are many more\\ninstances of irresponsible AI that happen silently.\\nCase study I: Automated grader’s biases\\nIn the summer of 2020, the United Kingdom canceled A levels, the high-stakes\\nexams that determine college placement, due to the COVID-19 pandemic. Ofqual,\\nthe regulatory body for education and examinations in the UK, sanctioned the use\\nof an automated system to assign final A-level grades to students—without them\\ntaking the test. According to Jones and Safak from Ada Lovelace Institute, “ Awarding\\nstudents’ grades based on teacher assessment was originally rejected by Ofqual on\\nthe grounds of unfairness between schools, incomparability across generations and\\ndevaluing of results because of grade inflation. The fairer option, Ofqual surmised,\\nwas to combine previous attainment data and teacher assessment to assign grades,\\nusing a particular statistical model—an ‘algorithm. ’”7\\nThe results published by this algorithm, however, turned out to be unjust and\\nuntrustworthy. They quickly led to public outcries to get rid of it, with hundreds\\nof students chanting in protest.8\\nWhat caused the public outcries? The first glance seems to point at the algorithm’s\\npoor performance. Ofqual stated that their model, tested on 2019 data, had about\\n60% average accuracy across A-level subjects. 9 This means that they expected 40% of', 'of students chanting in protest.8\\nWhat caused the public outcries? The first glance seems to point at the algorithm’s\\npoor performance. Ofqual stated that their model, tested on 2019 data, had about\\n60% average accuracy across A-level subjects. 9 This means that they expected 40% of\\nthe grades assigned by this model to be different from the students’ actual grades.\\nResponsible AI | 341', '10 Ofqual, “ Awarding GCSE, AS & A levels. ”\\n11 Jones and Safak, “Can Algorithms Ever Make the Grade?”\\n12 Jones and Safak, “Can Algorithms Ever Make the Grade?”\\nWhile the model’s accuracy seems low, Ofqual defended their algorithm as being\\nbroadly comparable to the accuracy of human graders. When comparing an examin‐\\ner’s grades with those made by a senior examiner, the agreement is also around 60%.10\\nThe accuracy by both human examiners and the algorithm exposes the underlying\\nuncertainty in assessing students at a single point in time, 11 further fueling the frus‐\\ntration of the public.\\nIf you’ve read this book thus far, you know that coarse-grained accuracy alone is\\nnowhere close to being sufficient to evaluate a model’s performance, especially for\\na model whose performance can influence the future of so many students. A closer\\nlook into this algorithm reveals at least three major failures along the process of\\ndesigning and developing this automated grading system:\\n• Failure to set the right objective•\\n• Failure to perform fine-grained evaluation to discover potential biases•\\n• Failure to make the model transparent•\\nWe’ll go into detail about each of these failures. Keep in mind that even if these\\nfailures are addressed, the public might still be upset with the auto-grading system.\\nFailure 1: Setting the wrong objective.    We discussed in Chapter 2 how the objective of\\nan ML project will affect the resulting ML system’s performance. When developing an\\nautomated system to grade students, you would’ve thought that the objective of this\\nsystem would be “grading accuracy for students. ”\\nHowever, the objective that Ofqual seemingly chose to optimize was “maintaining\\nstandards” across schools—fitting the model’s predicted grades to historical grade\\ndistributions from each school. For example, if school A had historically outper‐\\nformed school B in the past, Ofqual wanted an algorithm that, on average, also gives\\nstudents from school A higher grades than students from school B. Ofqual prioritized\\nfairness between schools over fairness between students—they preferred a model that\\ngets school-level results right over another model that gets each individual’s grades\\nright.\\nDue to this objective, the model disproportionately downgraded high-performing\\ncohorts from historically low-performing schools. A students from classes where\\nstudents had historically received straight Ds were downgraded to Bs and Cs.12\\n342 | Chapter 11: The Human Side of Machine Learning', '13 Ofqual, “ Awarding GCSE, AS & A Levels. ”\\n14 Jones and Safak, “Can Algorithms Ever Make the Grade?”\\nOfqual failed to take into account the fact that schools with more resources tend\\nto outperform schools with fewer resources. By prioritizing schools’ historical per‐\\nformance over students’ current performance, this auto-grader punished students\\nfrom low resource schools, which tend to have more students from underprivileged\\nbackgrounds.\\nFailure 2: Insufficient fine-grained model evaluation to discover biases.    Bias against stu‐\\ndents from historically low-performing schools is only one of the many biases dis‐\\ncovered about this model after the results were brought to the public. The automated\\ngrading system took into account teachers’ assessments as inputs but failed to address\\nteachers’ inconsistency in evaluation across demographic groups. It also “does not\\ntake into consideration the impact of multiple disadvantages for some protected\\ngroups [under the] 2010 Equalities Act, who will be double/triple disadvantaged\\nby low teacher expectations, [and] racial discrimination that is endemic in some\\nschools. ”13\\nBecause the model took into account each school’s historical performance, Ofqual\\nacknowledged that their model didn’t have enough data for small schools. For these\\nschools, instead of using this algorithm to assign final grades, they only used teacher-\\nassessed grades. In practice, this led to “better grades for private school students who\\ntend to have smaller classes. ”14\\nIt might have been possible to discover these biases through the public release of\\nthe model’s predicted grades with fine-grained evaluation to understand their model’s\\nperformance for different slices of data—e.g., evaluating the model’s accuracy for\\nschools of different sizes and for students from different backgrounds.\\nFailure 3: Lack of transparency.    Transparency is the first step in building trust in sys‐\\ntems, yet Ofqual failed to make important aspects of their auto-grader public before\\nit was too late. For example, they didn’t let the public know that the objective of\\ntheir system was to maintain fairness between schools until the day the grades were\\npublished. The public, therefore, couldn’t express their concern over this objective as\\nthe model was being developed.\\nFurther, Ofqual didn’t let teachers know how their assessments would be used by\\nthe auto-grader until after the assessments and student ranking had been submitted.', 'published. The public, therefore, couldn’t express their concern over this objective as\\nthe model was being developed.\\nFurther, Ofqual didn’t let teachers know how their assessments would be used by\\nthe auto-grader until after the assessments and student ranking had been submitted.\\nOfqual’s rationale was to avoid teachers attempting to alter their assessments to\\ninfluence the model’s predictions. Ofqual chose not to release the exact model being\\nused until results day to ensure that everyone would find out their results at the same\\ntime.\\nResponsible AI | 343', '15 “Royal Statistical Society Response to the House of Commons Education Select Committee Call for Evidence:\\nThe Impact of COVID-19 on Education and Children’s Services Inquiry, ” Royal Statistical Society, June 8,\\n2020, https://oreil.ly/ernho.\\nThese considerations came from good intention; however, Ofqual’s decision to keep\\ntheir model development in the dark meant that their system didn’t get sufficient\\nindependent, external scrutiny. Any system that operates on the trust of the public\\nshould be reviewable by independent experts trusted by the public. The Royal Statisti‐\\ncal Society (RSS), in their inquiry into the development of this auto-grader, expressed\\nconcerns over the composition of the “technical advisory group” that Ofqual put\\ntogether to evaluate the model. RSS indicated that “without a stronger procedural\\nbasis to ensure statistical rigor, and greater transparency about the issues that Ofqual\\nis examining, ”15 the legitimacy of Ofqual’s statistical model is questionable.\\nThis case study shows the importance of transparency when building a model that\\ncan make a direct impact on the lives of so many people, and what the consequences\\ncan be for failing to disclose important aspects of your model at the right time. It\\nalso shows the importance of choosing the right objective to optimize, as the wrong\\nobjective (e.g., prioritizing fairness among schools) can not only lead you to choose a\\nmodel that underperforms for the right objective, but also perpetuate biases.\\nIt also exemplifies the currently mucky boundary between what should be automated\\nby algorithms and what should not. There must be people in the UK government\\nwho think it’s OK for A-level grading to be automated by algorithms, but it’s also\\npossible to argue that due to the potential for catastrophic consequences of the\\nA-level grading, it should never have been automated in the first place. Until there\\nis a clearer boundary, there will be more cases of misusing AI algorithms. A clearer\\nboundary can only be achieved with more investments in time and resources as well\\nas serious considerations from AI developers, the public, and the authorities.\\nCase study II: The danger of “anonymized” data\\nThis case study is interesting to me because here, the algorithm is not an explicit\\nculprit. Rather it’s how the interface and collection of data is designed that allows\\nthe leakage of sensitive data. Since the development of ML systems relies heavily on', 'This case study is interesting to me because here, the algorithm is not an explicit\\nculprit. Rather it’s how the interface and collection of data is designed that allows\\nthe leakage of sensitive data. Since the development of ML systems relies heavily on\\nthe quality of data, it’s important for user data to be collected. The research commu‐\\nnity needs access to high-quality datasets to develop new techniques. Practitioners\\nand companies require access to data to discover new use cases and develop new\\nAI-powered products.\\n344 | Chapter 11: The Human Side of Machine Learning', '16 “Guidance on the Protection of Personal Identifiable Information, ” US Department of Labor,\\nhttps://oreil.ly/FokAV.\\n17 Sasha Lekach, “Strava’s Fitness Heatmap Has a Major Security Problem for the Military, ” Mashable, January 28,\\n2018, https://oreil.ly/9ogYx.\\n18 Jeremy Hsu, “The Strava Heat Map and the End of Secrets, ” Wired, January 29, 2018, https://oreil.ly/mB0GD.\\n19 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See the Names of People Exercising on Military Bases, ”\\nWired, January 30, 2018, https://oreil.ly/eJPdj.\\n20 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See”; Rosie Spinks, “Using a Fitness App Taught\\nMe the Scary Truth About Why Privacy Settings Are a Feminist Issue, ” Quartz, August 1, 2017,\\nhttps://oreil.ly/DO3WR.\\nHowever, collecting and sharing datasets might violate the privacy and security of\\nthe users whose data is part of these datasets. To protect users, there have been\\ncalls for anonymization of personally identifiable information (PII). According to\\nthe US Department of Labor, PII is defined as “any representation of information\\nthat permits the identity of an individual to whom the information applies to be\\nreasonably inferred by either direct or indirect means” such as name, address, or\\ntelephone number.16\\nHowever, anonymization may not be a sufficient guarantee for preventing data\\nmisuse and erosion of privacy expectations. In 2018, online fitness tracker Strava\\npublished a heatmap showing the paths it records of its users around the world\\nas they exercise, e.g., running, jogging, or swimming. The heatmap was aggregated\\nfrom one billion activities recorded between 2015 and September 2017, covering 27\\nbillion kilometers of distance. Strava stated that the data used had been anonymized,\\nand “excludes activities that have been marked as private and user-defined privacy\\nzones. ”17\\nSince Strava was used by military personnel, their public data, despite anonymization,\\nallowed people to discover patterns that expose activities of US military bases over‐\\nseas, including the “forward operating bases in Afghanistan, Turkish military patrols\\nin Syria, and a possible guard patrol in the Russian operating area of Syria. ” 18 An\\nexample of these discriminating patterns is shown in Figure 11-4. Some analysts even\\nsuggested that the data could reveal the names and heart rates of individual Strava\\nusers.19\\nSo where did the anonymization go wrong? First, Strava’s default privacy setting was', 'example of these discriminating patterns is shown in Figure 11-4. Some analysts even\\nsuggested that the data could reveal the names and heart rates of individual Strava\\nusers.19\\nSo where did the anonymization go wrong? First, Strava’s default privacy setting was\\n“opt-out, ” meaning that it requires users to manually opt out if they don’t want their\\ndata to be collected. However, users have pointed out that these privacy settings aren’t\\nalways clear and can cause surprises to users. 20 Some of the privacy settings can only\\nbe changed through the Strava website rather than in its mobile app. This shows the\\nimportance of educating users about your privacy settings. Better, data opt-in (data\\ncollecting isn’t by default), not opt-out, should be the default.\\nResponsible AI | 345', '21 “Fitness App Strava Lights Up Staff at Military Bases, ” BBC News, January 29, 2018, https://oreil.ly/hXwpN.\\n22 Matt Burgess, “Strava’s Heatmap Data Lets Anyone See. ”\\nFigure 11-4. Image created based on analysis done by BBC News21\\nWhen this issue with the Strava heatmap became public, some of the responsibilities\\nwere shifted toward users: e.g., how military personnel shouldn’t use non-military-\\nissue devices with GPS tracking and how location services should be turned off.22\\n346 | Chapter 11: The Human Side of Machine Learning', 'However, privacy settings and users’ choices only address the problem at a surface\\nlevel. The underlying problem is that the devices we use today are constantly collect‐\\ning and reporting data on us. This data has to be moved and stored somewhere,\\ncreating opportunities for it to be intercepted and misused. The data that Strava has\\nis small compared to much more widely used applications like Amazon, Facebook,\\nGoogle, etc. Strava’s blunder might have exposed military bases’ activities, but other\\nprivacy failures might cause even more dangers not only to individuals but also to\\nsociety at large.\\nCollecting and sharing data is essential for the development of data-driven technol‐\\nogies like AI. However, this case study shows the hidden danger of collecting and\\nsharing data, even when data is supposedly anonymized and was released with good\\nintention. Developers of applications that gather user data must understand that their\\nusers might not have the technical know-how and privacy awareness to choose the\\nright privacy settings for themselves, and so developers must proactively work to\\nmake the right settings the default, even at the cost of gathering less data.\\nA Framework for Responsible AI\\nIn this section, we will lay down the foundations for you, as an ML practitioner, to\\naudit model behavior and set out guidelines that best help you meet the needs of\\nyour projects. This framework is not sufficient for every use case. There are certain\\napplications where the use of AI might altogether be inappropriate or unethical (e.g.,\\ncriminal sentencing decisions, predictive policing), regardless of which framework\\nyou follow.\\nDiscover sources for model biases\\nAs someone who has been following the discussions around ML systems design, you\\nknow that biases can creep in your system through the entire workflow. Y our first\\nstep is to discover how these biases can creep in. The following are some examples of\\nthe sources of data, but keep in mind that this list is far from being exhaustive. One\\nof the reasons why biases are so hard to combat is that biases can come from any step\\nduring a project lifecycle.\\nResponsible AI | 347', '23 Michael Feldman, Sorelle Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian, “Certi‐\\nfying and Removing Disparate Impact, ” arXiv, July 16, 2015, https://oreil.ly/FjSve.\\nTraining data\\nIs the data used for developing your model representative of the data your model\\nwill handle in the real world? If not, your model might be biased against the\\ngroups of users with less data represented in the training data.\\nLabeling\\nIf you use human annotators to label your data, how do you measure the quality\\nof these labels? How do you ensure that annotators follow standard guidelines\\ninstead of relying on subjective experience to label your data? The more annota‐\\ntors have to rely on their subjective experience, the more room for human biases.\\nFeature engineering\\nDoes your model use any feature that contains sensitive information? Does your\\nmodel cause a disparate impact on a subgroup of people? Disparate impact\\noccurs “when a selection process has widely different outcomes for different\\ngroups, even as it appears to be neutral. ”23 This can happen when a model’s deci‐\\nsion relies on information correlated with legally protected classes (e.g., ethnicity,\\ngender, religious practice) even when this information isn’t used in training the\\nmodel directly. For example, a hiring process can cause disparate impact by race\\nif it leverages variables correlated with race such as zip code and high school\\ndiplomas. To mitigate this potential disparate impact, you might want to use dis‐\\nparate impact remover techniques proposed by Feldman et al. in “Certifying and\\nRemoving Disparate Impact”  or to use the function DisparateImpactRemover\\nimplemented by AI Fairness 360  (AIF360). Y ou can also identify hidden bias in\\nvariables (which can then be removed from the training set) using the Infogram\\nmethod, implemented in H2O.\\nModel’s objective\\nAre you optimizing your model using an objective that enables fairness to all\\nusers? For example, are you prioritizing your model’s performance on all users,\\nwhich skews your model toward the majority group of users?\\nEvaluation\\nAre you performing adequate, fine-grained evaluation to understand your mod‐\\nel’s performance on different groups of users? This is covered in the section\\n“Slice-based evaluation” on page 185. Fair, adequate evaluation depends on the\\nexistence of fair, adequate evaluation data.\\n348 | Chapter 11: The Human Side of Machine Learning', '24 Wikipedia, s.v. “Differential privacy, ” https://oreil.ly/UcxzZ.\\nUnderstand the limitations of the data-driven approach\\nML is a data-driven approach to solving problems. However, it’s important to under‐\\nstand that data isn’t enough. Data concerns people in the real world, with socioeco‐\\nnomic and cultural aspects to consider. We need to gain a better understanding of\\nthe blind spots caused by too much reliance on data. This often means crossing over\\ndisciplinary and functional boundaries, both within and outside the organization, so\\nthat we can account for the lived experiences of those who will be impacted by the\\nsystems that we build.\\nAs an example, to build an equitable automated grading system, it’s essential to\\nwork with domain experts to understand the demographic distribution of the student\\npopulation and how socioeconomic factors get reflected in the historical performance\\ndata.\\nUnderstand the trade-offs between different desiderata\\nWhen building an ML system, there are different properties you might want this\\nsystem to have. For example, you might want your system to have low inference\\nlatency, which could be obtained by model compression techniques like pruning.\\nY ou might also want your model to have high predictive accuracy, which could be\\nachieved by adding more data. Y ou might also want your model to be fair and\\ntransparent, which could require the model and the data used to develop this model\\nto be made accessible for public scrutiny.\\nOften, ML literature makes the unrealistic assumption that optimizing for one prop‐\\nerty, like model accuracy, holds all others static. People might discuss techniques to\\nimprove a model’s fairness with the assumption that this model’s accuracy or latency\\nwill remain the same. However, in reality, improving one property can cause other\\nproperties to degrade. Here are two examples of these trade-offs:\\nPrivacy versus accuracy trade-off\\nAccording to Wikipedia, differential privacy is “a system for publicly sharing\\ninformation about a dataset by describing the patterns of groups within the\\ndataset while withholding information about individuals in the dataset. The idea\\nbehind differential privacy is that if the effect of making an arbitrary single\\nsubstitution in the database is small enough, the query result cannot be used to\\ninfer much about any single individual, and therefore provides privacy. ”24\\nDifferential privacy is a popular technique used on training data for ML models.', 'substitution in the database is small enough, the query result cannot be used to\\ninfer much about any single individual, and therefore provides privacy. ”24\\nDifferential privacy is a popular technique used on training data for ML models.\\nThe trade-off here is that the higher the level of privacy that differential privacy\\ncan provide, the lower the model’s accuracy. However, this accuracy reduction\\nisn’t equal for all samples. As pointed out by Bagdasaryan and Shmatikov (2019),\\nResponsible AI | 349', '25 Eugene Bagdasaryan and Vitaly Shmatikov, “Differential Privacy Has Disparate Impact on Model Accuracy, ”\\narXiv, May 28, 2019, https://oreil.ly/nrJGK.\\n26 Sarah Hooker, Aaron Courville, Gregory Clark, Y ann Dauphin, and Andrea Frome, “What Do Compressed\\nDeep Neural Networks Forget?” arXiv, November 13, 2019, https://oreil.ly/bgfFX.\\n27 Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, and Emily Denton, “Characterising Bias in\\nCompressed Models, ” arXiv, October 6, 2020, https://oreil.ly/ZTI72.\\n28 Hooker et al., “Characterising Bias in Compressed Models. ”\\n“the accuracy of differential privacy models drops much more for the underre‐\\npresented classes and subgroups. ”25\\nCompactness versus fairness trade-off\\nIn Chapter 7, we talked at length about various techniques for model compres‐\\nsion such as pruning and quantization. We learned that it’s possible to reduce a\\nmodel’s size significantly with minimal cost of accuracy, e.g., reducing a model’s\\nparameter count by 90% with minimal accuracy cost.\\nThe minimal accuracy cost is indeed minimal if it’s spread uniformly across all\\nclasses, but what if the cost is concentrated in only a few classes? In their 2019\\npaper, “What Do Compressed Deep Neural Networks Forget?, ” Hooker et al.\\nfound that “models with radically different numbers of weights have comparable\\ntop-line performance metrics but diverge considerably in behavior on a narrow\\nsubset of the dataset. ” 26 For example, they found that compression techniques\\namplify algorithmic harm when the protected feature (e.g., sex, race, disability)\\nis in the long tail of the distribution. This means that compression disproportion‐\\nately impacts underrepresented features.27\\nAnother important finding from their work is that while all compression techni‐\\nques they evaluated have a nonuniform impact, not all techniques have the same\\nlevel of disparate impact. Pruning incurs a far higher disparate impact than is\\nobserved for the quantization techniques that they evaluated.28\\nSimilar trade-offs continue to be discovered. It’s important to be aware of these\\ntrade-offs so that we can make informed design decisions for our ML systems. If you\\nare working with a system that is compressed or differentially private, allocating more\\nresources to auditing model behavior is recommended to avoid unintended harm.\\nAct early\\nConsider a new building being constructed downtown. A contractor has been called', 'are working with a system that is compressed or differentially private, allocating more\\nresources to auditing model behavior is recommended to avoid unintended harm.\\nAct early\\nConsider a new building being constructed downtown. A contractor has been called\\nupon to build something that will stand for the next 75 years. To save costs, the\\ncontractor uses poor-quality cement. The owner doesn’t invest in supervision since\\nthey want to avoid overhead to be able to move fast. The contractor continues\\nbuilding on top of that poor foundation and finishes the building on time.\\n350 | Chapter 11: The Human Side of Machine Learning', '29 Jonette M. Stecklein, Jim Dabney, Brandon Dick, Bill Haskins, Randy Lovell, and Gregory Moroney,\\n“Error Cost Escalation Through the Project Life Cycle, ” NASA Technical Reports Server (NTRS),\\nhttps://oreil.ly/edzaB.\\n30 Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena\\nSpitzer, Inioluwa Deborah Raji, and Timnit Gebru, “Model Cards for Model Reporting, ” arXiv, October 5,\\n2018, https://oreil.ly/COpah.\\n31 Mitchell et al., “Model Cards for Model Reporting. ”\\nWithin a year, cracks start showing up and it appears that the building might topple.\\nThe city decides that this building poses a safety risk and requests for it to be\\ndemolished. The contractor’s decision to save cost and the owner’s decision to save\\ntime in the beginning now end up costing the owner much more money and time.\\nY ou might encounter this narrative often in ML systems. Companies might decide to\\nbypass ethical issues in ML models to save cost and time, only to discover risks in\\nthe future when they end up costing a lot more, such as the preceding case studies of\\nOfqual and Strava.\\nThe earlier in the development cycle of an ML system that you can start thinking\\nabout how this system will affect the life of users and what biases your system might\\nhave, the cheaper it will be to address these biases. A study by NASA shows that for\\nsoftware development, the cost of errors goes up by an order of magnitude at every\\nstage of your project lifecycle.29\\nCreate model cards\\nModel cards are short documents accompanying trained ML models that provide\\ninformation on how these models were trained and evaluated. Model cards also\\ndisclose the context in which models are intended to be used, as well as their limita‐\\ntions.30 According to the authors of the model card paper, “The goal of model cards\\nis to standardize ethical practice and reporting by allowing stakeholders to compare\\ncandidate models for deployment across not only traditional evaluation metrics but\\nalso along the axes of ethical, inclusive, and fair considerations. ”\\nThe following list has been adapted from content in the paper “Model Cards for\\nModel Reporting” to show the information you might want to report for your\\nmodels:31\\n• Model details: Basic information about the model.•\\n— Person or organization developing model—\\n— Model date—\\n— Model version—\\n— Model type—\\nResponsible AI | 351', '— Information about training algorithms, parameters, fairness constraints or—\\nother applied approaches, and features\\n— Paper or other resource for more information—\\n— Citation details—\\n— License—\\n— Where to send questions or comments about the model—\\n• Intended use: Use cases that were envisioned during development.•\\n— Primary intended uses—\\n— Primary intended users—\\n— Out-of-scope use cases—\\n• Factors: Factors could include demographic or phenotypic groups, environmental•\\nconditions, technical attributes, or others.\\n— Relevant factors—\\n— Evaluation factors—\\n• Metrics: Metrics should be chosen to reflect potential real-world impacts of the•\\nmodel.\\n— Model performance measures—\\n— Decision thresholds—\\n— Variation approaches—\\n• Evaluation data: Details on the dataset(s) used for the quantitative analyses in the•\\ncard.\\n— Datasets—\\n— Motivation—\\n— Preprocessing—\\n• Training data: May not be possible to provide in practice. When possible, this•\\nsection should mirror Evaluation Data. If such detail is not possible, minimal\\nallowable information should be provided here, such as details of the distribution\\nover various factors in the training datasets.\\n• Quantitative analyses•\\n— Unitary results—\\n— Intersectional results—\\n• Ethical considerations•\\n• Caveats and recommendations•\\n352 | Chapter 11: The Human Side of Machine Learning', 'Model cards are a step toward increasing transparency into the development of ML\\nmodels. They are especially important in cases where people who use a model aren’t\\nthe same people who developed this model.\\nNote that model cards will need to be updated whenever a model is updated. For\\nmodels that update frequently, this can create quite an overhead for data scientists if\\nmodel cards are created manually. Therefore, it’s important to have tools to automati‐\\ncally generate model cards, either by leveraging the model card generation feature of\\ntools like TensorFlow, Metaflow, and scikit-learn or by building this feature in-house.\\nBecause the information that should be tracked in a model’s card overlaps with the\\ninformation that should be tracked by a model store, I wouldn’t be surprised if in the\\nnear future, model stores evolve to automatically generate model cards.\\nEstablish processes for mitigating biases\\nBuilding responsible AI is a complex process, and the more ad hoc the process is,\\nthe more room there is for errors. It’s important for businesses to establish systematic\\nprocesses for making their ML systems responsible.\\nY ou might want to create a portfolio of internal tools easily accessible by different\\nstakeholders. Big corporations have tool sets that you can reference. For example,\\nGoogle has published recommended best practices for responsible AI  and IBM has\\nopen-sourced AI Fairness 360 , which contains a set of metrics, explanations, and\\nalgorithms to mitigate bias in datasets and models. Y ou might also consider using\\nthird-party audits.\\nStay up-to-date on responsible AI\\nAI is a fast-moving field. New sources of biases in AI are constantly being discovered,\\nand new challenges for responsible AI constantly emerge. Novel techniques to com‐\\nbat these biases and challenges are actively being developed. It’s important to stay\\nup-to-date with the latest research in responsible AI. Y ou might want to follow the\\nACM FAccT Conference, the Partnership on AI, the Alan Turing Institute’s Fairness,\\nTransparency, Privacy group, and the AI Now Institute.\\nSummary\\nDespite the technical nature of ML solutions, designing ML systems can’t be confined\\nin the technical domain. They are developed by humans, used by humans, and leave\\ntheir marks in society. In this chapter, we deviated from the technical theme of the\\nlast eight chapters to focus on the human side of ML.\\nWe first focused on how the probabilistic, mostly correct, and high-latency nature', 'their marks in society. In this chapter, we deviated from the technical theme of the\\nlast eight chapters to focus on the human side of ML.\\nWe first focused on how the probabilistic, mostly correct, and high-latency nature\\nof ML systems can affect user experience in various ways. The probabilistic nature\\ncan lead to inconsistency in user experience, which can cause frustration—“Hey,\\nSummary | 353', 'I just saw this option right here, and now I can’t find it anywhere. ” The mostly\\ncorrect nature of an ML system might render it useless if users can’t easily fix these\\npredictions to be correct. To counter this, you might want to show users multiple\\n“most correct” predictions for the same input, in the hope that at least one of them\\nwill be correct.\\nBuilding an ML system often requires multiple skill sets, and an organization might\\nwonder how to distribute these required skill sets: to involve different teams with\\ndifferent skill sets or to expect the same team (e.g., data scientists) to have all the\\nskills. We explored the pros and cons of both approaches. The main cons of the first\\napproach is overhead in communication. The main cons of the second approach is\\nthat it’s difficult to hire data scientists who can own the process of developing an ML\\nsystem end-to-end. Even if they can, they might not be happy doing it. However, the\\nsecond approach might be possible if these end-to-end data scientists are provided\\nwith sufficient tools and infrastructure, which was the focus of Chapter 10.\\nWe ended the chapter with what I believe to be the most important topic of this\\nbook: responsible AI. Responsible AI is no longer just an abstraction, but an essential\\npractice in today’s ML industry that merits urgent actions. Incorporating ethics\\nprinciples into your modeling and organizational practices will not only help you\\ndistinguish yourself as a professional and cutting-edge data scientist and ML engineer\\nbut also help your organization gain trust from your customers and users. It will also\\nhelp your organization obtain a competitive edge in the market as more and more\\ncustomers and users emphasize their need for responsible AI products and services.\\nIt is important to not treat this responsible AI as merely a checkbox ticking activity\\nthat we undertake to meet compliance requirements for our organization. It’s true\\nthat the framework proposed in this chapter will help you meet the compliance\\nrequirements for your organization, but it won’t be a replacement for critical thinking\\non whether a product or service should be built in the first place.\\n354 | Chapter 11: The Human Side of Machine Learning', 'Epilogue\\nWow, you made it! Y ou’ve just finished a pretty technical book of 100,000 words and\\nover 100 illustrations written by a writer who speaks English as her second language.\\nWith the help of many colleagues and mentors, I worked really hard on this book,\\nand I’m grateful that you chose to read it out of so many books out there. I hope that\\nthe takeaways you can get from this book will make your work a little bit easier.\\nWith the best practices and tooling that we have now, there are already many incredi‐\\nble ML use cases influencing our everyday life. I have no doubt that the number of\\nimpactful use cases will grow over time as tooling matures, and you might be among\\nthe people who will make this happen. I’m looking forward to seeing what you build!\\nML systems have a lot of challenges. Not all of them are fun, but all of them are\\nopportunities for growth and impact. If you want to talk about these challenges and\\nopportunities, don’t hesitate to reach out. I can be found on Twitter at @chipro or via\\nemail at chip@claypot.ai.\\n355', 'Index\\nSymbols\\n1NF (first normal form), 59\\n2NF (second normal form), 59\\nA\\nA/B testing, 283-284, 288\\naccuracy-related metrics, 252\\nACID (atomicity, consistency, isolation, dura‐\\nbility), 68\\nactive learning, 101-102\\nad hoc analytics, 162\\nadaptability, 31\\nadversarial attacks, 272\\nadversarial augmentation, 116\\nAI (artificial intelligence), ethics, 339, 347-348\\ndata-driven approach limitations, 349\\nirresponsible, case studies, 341-347\\nmitigating biases, 353\\nmodel cards, 351-353\\ntrade-offs, 349\\nAirflow, 315-316\\nalert fatigue, 255, 259\\nalert policies, 259\\nalgorithms\\nbandit algorithms, 287-291\\ncontinual learning and, 273-274\\nfeature importance, 142\\nanalytical processing, 67\\nApache Iceberg, 69\\narchitectural search, 174\\nArgo, 316-318\\nartifacts, 162\\nartificial intelligence (see AI)\\nasynchronous prediction, 198\\nautomated retraining, 275-277\\nAutoML\\narchitecture search, 174-178\\nhard AutoML, 174-178\\nhyperparameter tuning, 173-174\\nlearned optimizer, 174-178\\nsoft AutoML, 173-174\\nautoscaling, 30\\nB\\nbagging, ensembles, 158-159\\nbandit algorithms, 287-291\\nBASE (basically available, soft state, and even‐\\ntual consistency), 68\\nbase learners, 156\\nbase model, fine tuning, 100\\nbaselines, offline model evaluation, 179\\nexisting solutions, 181\\nhuman, 180\\nrandom, 180\\nsimple heuristic, 180\\nzero rule, 180\\nbatch pipeline, 203-205\\nbatch prediction, 197-201\\nmoving to online prediction, 201-203\\nbatch processing, 78-79\\nbatches, overfitting, 167\\nbinary classification, 37\\nbinary data, 57\\nbinary file size, 57\\nboosting, ensembles, 159-161\\nBorg, 314\\nbrand monitoring, 12\\nbrowsers, ML (machine learning) and, 222\\n357', 'building versus buying, 327-329\\nbusiness analysis, 35\\nbusiness objectives, 26-28\\nC\\ncalibration, 183-184\\ncanary release, 285\\ncardinality, classification tasks and, 37\\ncatastrophic forgetting, 264\\ncategorical features, 129-132\\nchampion model, 264\\nchurn prediction, 104\\nclass imbalance, 102\\nalgorithm-level methods, 110\\nclass-balanced loss, 112\\ncost-sensitive learning, 111\\nfocal loss, 112\\nchallenges, 103-105\\nevaluation metrics, 106-108\\nresampling, 109-110\\nclass-balanced loss, 112\\nclassification\\nas regression problem, 107\\nbinary, 37\\nhierarchical, 38\\nhigh cardinality, 37\\nmulticlass, 37, 38\\nmultilabel, 38\\nsentiment analysis, 120\\nclassification models, 36\\ncloud computing, 212, 300-302\\nelasticity, 300\\nmulticloud strategy, 302\\ncode versioning, 164\\ncolumn deletion, 125\\ncolumn-major formats, 54-56\\npandas, 56\\nParquet, 54\\nCommuter, 305\\ncompact convolutional filters, 206\\ncomputational priorities, 15\\ncompute-intensive problems, 6\\nconcept drift, 238, 241\\nconfidence measurement, 185\\ncontainers, 308-310\\ncontextual bandits, 289\\ncontinual learning, 35, 264, 268-270\\nalgorithms and, 273-274\\nevaluation and, 272-273\\nfeature reuse, 277\\nfresh data access, 270-272\\nstateful training, 265-268\\nautomated, 277-278\\nstateless retraining, 265-268\\nmanual, 275\\ntraining, automated retraining, 275-277\\nversus online learning, 268\\nconvenience sampling, 83\\ncost-sensitive learning, 111\\ncovariate data distribution shift, 238-240\\ncron, schedulers, 313-314\\ncross-functional collaboration, teams, 335\\nCSV (comma-separated values), row-major for‐\\nmat, 54\\nD\\nDAG (directed acyclic graph), 312\\ndashboards, monitoring and, 258\\ndata, 5, 18\\nmind versus data, 43-46\\ntraining (see training data)\\nunseen data, 6\\ndata augmentation, 113\\nadversarial augmentation, 116\\ndata synthesis, 116-117\\nperturbation, 114-116\\nsimple label-preserving transformations,\\n114\\ndata distribution shifts\\naddressing, 248-250\\ndetection\\nstatistical methods, 243-244\\ntime scale windows, 245-247\\nML system failure, 237\\nconcept drift, 238, 241\\ncovariate shift, 238-240\\nfeature change, 241\\nlabel schema change, 241\\nlabel shift, 238, 240\\ndata duplication, data leakage and, 139\\ndata engineering, 34\\ndata formats, 53\\nbinary, 57\\ncolumn-major, 54-56\\nJSON, 54\\nmultimodal data, 53\\nrelational model, NoSQL, 63-66\\nrow-major, 54-56\\n358 | Index', 'text, 57\\ndata freshness, model updates and, 279-280\\ndata generation, data leakage and, 140\\ndata iteration, 267\\nmodel updates and, 281\\ndata leakage, 135\\ndata duplication prior to splitting, 139\\ndata generation process and, 140\\ndetecting, 140\\ngroup leakage, 139\\nKaggle competition, 136\\nscaling before splitting, 138\\nstatistics from test split, missing data and,\\n138\\ntime-correlated data, 137\\ndata models\\nrelational, 59-62\\nstructured data, 66-67\\nunstructured data, 66-67\\ndata normalization, 59\\ndata parallelism, distributed training and,\\n168-170\\ndata scientists, teams, 336-339\\ndata sources, 50\\ndatabases, internal, 52\\nlogs, 51\\nsmartphones and, 52\\nsystem-generated data, 50\\nthird-party data, 52\\nuser input, 50\\ndata synthesis, 116-117\\ndata-driven approach, AI ethics and, 349\\ndatabases and dataflow, 72\\ndataflow, 72\\nmessage queue model, 77\\npassing through databases, 72\\npassing through real-time transport, 74-77\\npassing through services, 73-74\\nrequest driven, 75\\nDataFrame, pandas and, 56\\ndebugging, 165\\ndecision trees, pruning, 208-209\\ndeclarative ML systems, 62\\ndeep learning\\nML (machine learning) and, 1\\nML algorithms and, 150\\ndegenerate feedback loops, ML system failure,\\n233\\ncorrecting, 235-236\\ndependencies, 312\\nML models, model store, 322\\ndependency failure, 227\\ndeployment, 34, 192\\nendpoints, exposing, 192\\nfailure, 227\\nML models, 320\\nmyths\\nlimited models at once, 194-195\\nmodel updating, 196\\nperformance, 195\\nscale, 196\\nseparation of responsibilities, 193\\nshadow deployment, 282\\ndevelopment environment, infrastructure, 296,\\n302\\ncontainers, 308-310\\nsetup, 303\\nIDE, 303-306\\nstandardization, 306-308\\ndirected acyclic graph (DAG), 312\\ndirectional expectation tests, 183\\ndiscretization, feature engineering and, 128-129\\ndistributed training, 168\\ndata parallelism and, 168\\nmodel parallelism and, 170-172\\nDocker Compose, 310\\nDocker images, 308-310\\nDockerfiles, 308-310\\ndocument model, 63\\nschemas, 64\\ndowntime, 228\\ndriver management service, 73\\ndynamic sampling, 110\\nE\\nedge cases\\nfailure and, 231-231\\noutliers and, 232\\nedge computing, 213\\nmodel optimization, 214-221\\nEKS (Elastic Kubernetes Service), 314\\nembedding\\npositional embedding, 133-135\\nword embeddings, 133\\nendpoint, exposing, 192\\nensembles\\nbagging, 158-159\\nbase learners, 156\\nIndex | 359', 'boosting, 159-161\\nspam classifiers, 157\\nstacking, 161\\nethics in AI, 339-347\\nETL (extract, transform, load), 70-72\\nevaluation, offline\\nconfidence measurement, 185\\ndirectional expectation tests, 183\\ninvariation tests, 182\\nmodel calibration, 183-184\\nperturbation tests, 181-182\\nslice-based, 185-188\\nexisting data, 5\\nexperiment artifacts, development and, 323\\nexperiment tracking, 162-163\\nthird-party tools, 163\\nexporting models, 193\\nF\\nF1 metrics, 107\\nfactorization, low-rank, 206-208\\nfairness, 19\\nfeature change, 241\\nfeature engineering, 120-122\\ncategorical features, 129-132\\ndiscretization, 128-129\\nfeature crossing, 132\\nfeature generalization, 144-146\\nfeature importance, 142\\nmissing values and, 123\\ndeletion, 125\\nimputation, 125-126\\nMAR (missing at random), 124\\nMCAR (missing completely at random),\\n124\\nMNAR (missing not at random), 124\\nNLP (natural language processing) and, 122\\npositional embeddings, 133-135\\npredictive power of features, 140\\nscaling, 126-128\\nuseless features, 141\\nfeature scaling, 126-128\\nfeature store, 325-327\\nfeatures\\ncomputation, 326\\nconsistency, 326\\nextracting, 255\\nfailures and, 166\\nlearned, 120-122\\nmanagement, 326\\nmonitoring, 253-255\\nonline, 199\\nreuse, 277\\nstreaming, 199\\nfeedback loops, 288\\nML system failure, 234\\nfeedback, users, 93\\nfixed positional embeddings, 135\\nfixed-point inference, 210\\nFLOPS (floating-point operations per second),\\n298\\nforecasting customer demand, 11\\nFourier features, 135\\nfraud detection, 11, 104\\nG\\nGDPR (General Data Protection Regulation),\\n164\\ngeneralization, features, 144-146\\nGKE (Google Kubernetes Engine), 314\\nGoogle Translate, 1\\ngraph model, 65\\nH\\nH20 AutoML, 62\\nhand labels, 88\\nlineage, 90\\nmultiplicity, 89-90\\nhard AutoML, 174-178\\nhardware failure, 228\\nhashed functions, 130\\nheuristics, LFs (labeling functions), 95\\nheuristics-based slicing, 188\\nhierarchical classification, 38\\nhuman baselines, 180\\nhyperparameters\\nfailures and, 166\\ntuning, 173-174\\nvalues over time, 163\\nI\\nIDE (integrated development environment),\\n303\\ncloud dev environment, 307\\nnotebooks and, 304\\nimportance sampling, 87\\ninfrastructure, 293, 295\\n360 | Index', 'building versus buying, 327-329\\ncloud computing and, 300-302\\ndevelopment environment layer, 296, 302\\nsetup, 303-306\\nfundamental facilities, 295\\nML platform layer, 296\\nrequirements, 295\\nresource management layer, 295\\nstorage and compute layer, 295, 296, 297\\ncompute resources, 297\\nFLOPS, 298\\nprivate data centers, 300-302\\npublic cloud, 300-302\\nunits, 297\\ninput, monitoring, 255\\ninstances on-demand, 300\\nintegrated development environment (see IDE)\\ninterleaving experiments, 285-287\\ninternal databases, 52\\ninterpretability, 20\\ninvariation tests, 182\\nIR (intermediate representation), 215\\niterative processes\\nmodel development and, 34\\nperformance check, 149\\nmodel updates and, 281\\ntraining the model and, 32-33\\ndata engineering, 34\\nproject scoping, 34\\nJ\\nJSON (JavaScript Object Notation), 54\\njudgment sampling, 83\\nK\\nk-means clustering models, 150\\nKaggle, data leakage, 136\\nknowledge distillation, 208\\nKubeflow, 318\\nKubernetes (K8s), 310, 314\\nEKS (Elastic Kubernetes Service), 314\\nGKE (Google Kubernetes Engine), 314\\nL\\nlabel computation, 271\\nlabel schema change, 241\\nlabel shift, 238, 240\\nlabeling, 88\\nclass imbalance and, 102\\nerrors, class imbalance and, 105\\nhand labels, 88\\nlineage, 90\\nmultiplicity, 89-90\\nlack of labels, 94\\nactive learning, 101-102\\nsemi-supervision, 98-99\\ntransfer learning, 99-101\\nweak supervision, 95-98\\nML algorithms, 151\\nnatural labels, 91\\nfeedback loop length, 92\\nrecommender systems, 91\\nperturbation, 114-116\\nsimple label-preserving transformations,\\n114\\nlanguage modeling, sampling and, 83\\nlatency, 16\\nlatency versus throughput, 16-18\\nlearning, 3\\nLFs (labeling functions), 95\\nheuristics, 95\\nlogs, 51, 51\\nexperiment tracking, 162\\nmonitoring and, 256-257\\nstorage, 51\\nloop tiling, model optimization, 218\\nloss curve, 162\\nloss functions, 40\\n(see also objective functions)\\nlow-rank factorization, 206-208\\nM\\nmaintainability, 31\\nManning, Christopher, 44\\nMAR (missing at random) values, 124\\nMCAR (missing completely at random) values,\\n124\\nmerge conflicts, 164\\nmessage queue, dataflow and, 77\\nMetaflow, 318\\nmetrics\\nmonitoring and, 250\\naccuracy-related metrics, 252\\nfeatures, 253-255\\npredictions, 252-253\\nraw input, 255\\nperformance metrics, 162\\nIndex | 361', 'system performance, 163\\nmind versus data, 43-46\\nmissing at random (MAR), 124\\nmissing completely at random (MCAR), 124\\nmissing data, test split statistics and, 138\\nmissing not at random (MNAR), 124\\nML (machine learning)\\nbrowsers and, 222, 223\\ncloud computing, 212-223\\ncomplex patterns, 4\\ndeep learning and, 1\\nedge computing, 212-223\\nexisting data and, 5\\nlearning, 3\\nmodel optimization, 220-221\\npredictions and, 6\\nproduction and, 12-21\\nrepetition, 7\\nresearch and, 12-21\\nscale, 7\\nsmartphones and, 9\\nunseen data, 6\\nuse cases, 9-12\\nwhen to use, 3-12\\nML algorithms, 2, 149\\ndeep learning and, 150\\nlabels, 151\\nversus neural networks, 150\\nML model logic, 191\\nML models\\ncontinual learning, 35\\ndata iteration, 267\\ndebugging, 165\\ndeployment, 320\\nedge computing, optimization, 214-221\\nensembles, 156, 157\\nbagging, 158-159\\nbase learners, 156\\nboosting, 159-161\\nstacking, 161\\nevaluation, 150\\ntest in production, 281-291\\nexperiment tracking, 162-163\\nexporting, 193\\nfailures\\nbatches, overfitting, 167\\ncomponents, 167\\ndata problems, 166\\nfeature choice, 166\\nhyperparameters and, 166\\npoor model implementation, 166\\nrandom seeds, 167\\ntheoretical constraints, 166\\niteration, 267\\nmonitoring, 35\\noffline evaluation, 178\\nbaselines, 179-181\\nmethods, 181-188\\noptimization, 220-221\\nparameters, model store, 322\\nperformance metrics, 162\\nselection criteria, 151\\nhuman biases in, 153\\nmodel, 155\\nperformance now and later, 153\\nsimple models, 152\\nstate-of-the-art trap, 152\\ntrade-offs, 154\\nspeed, 163\\ntraining, 32-33\\ndata engineering, 34\\ndistributed, 168-172\\nupdate frequency, 279\\ndata freshness and, 279-280\\ndata iteration and, 281\\nmodel iteration and, 281\\nupdates, 267\\nversioning, 163-165\\nML platform, 319\\nmodel deployment, 320\\nmodel store, 321-325\\nML platform layer, infrastructure, 296\\nML system failures\\ndata distribution shifts, 237\\naddressing, 248-250\\nconcept drift, 238, 241\\ncovariate, 238-240\\ndetection, 242-247\\nfeature change, 241\\nlabel schema change, 241\\nlabel shifts, 238, 240\\nML-system specific\\ndegenerate feedback loops, 233-236\\nedge cases, 231\\nproduction data different from training\\ndata, 229-231\\noperational expectation violations, 227\\nsoftware\\n362 | Index', 'crashes, 228\\ndependency failure, 227\\ndeployment failure, 227\\ndowntime, 228\\nhardware failure, 228\\nML systems\\ndeclarative, 62\\nfailures, 226\\niterative processes, 32-35\\nrequirements\\nadaptability, 31\\nmaintainability, 31\\nreliability, 29\\nscalability, 30-31\\nversus traditional software, 22-23\\nMLOPs, ML systems design and, 2-3\\nMNAR (missing not at random) values, 124\\nmodel biases, AI ethics, 347-348\\nmodel calibration, 183-184\\nmodel cards, AI ethics, 351-353\\nmodel compression, 206\\nknowledge distillation, 208\\nlow-rank factorization, 206-208\\npruning, 208-209\\nquantization, 209-211\\nmodel development, 34\\nmodel implementation, failures and, 166\\nmodel parallelism, distributed training and,\\n170-172\\nmodel performance, business analysis, 35\\nmonitoring, 250, 263\\n(see also test in production)\\nalerts and, 259\\ndashboards and, 258\\nlogs and, 256-257\\nmetrics and, 250\\naccuracy-related metrics, 252\\nfeatures, 253-255\\npredictions, 252-253\\nraw input, 255\\nmulticlass classification, 37, 38\\nmultilabel classification, 38\\nmultimodal data, 53\\nN\\nn-grams, 120\\nNAS (neural architecture search), 174\\nnatural labels, 91\\nfeedback loop length, 92\\nrecommender systems, 91\\nnatural language processing (NLP) (see NLP)\\nneural architecture search (NAS), 174\\nneural networks, 150\\npositional embedding, 133\\nnewsfeeds\\nranking posts, 41\\nuser engagement and, 41\\nNLP (natural language processing), 114\\ndata augmentation and, 113\\nfeature engineering, 122\\nnonprobability sampling, 83\\nbiases, 83\\nNorvig, Peter, 44\\nNoSQL, 63\\ndocument model, 63\\ngraph model, 65\\nnotebooks, IDE and, 304\\nNSFW (not safe for work) content filtering, 41\\nNumPy, 56\\nO\\nobjective functions, 40-43\\nobservability, 250, 259-261\\noffline evaluation of models, 178\\nbaselines, 179\\nexisting solutions, 181\\nhuman, 180\\nrandom, 180\\nsimple heuristic, 180\\nzero rule, 180\\nOLAP (online analytical processing), 69\\nOLTP (online transaction processing) system,\\n69\\non-demand instances, 300\\non-demand prediction, 198\\nOne Billion Word Benchmark for Language\\nModeling, 45\\nonline features, 199\\nonline learning, 268\\nonline prediction, 197-201, 288\\nmoving to from batch prediction, 201-203\\nstreaming pipeline, 203-205\\noperation expectation violations, 227\\noperator fusion, model optimization and, 218\\norchestrators\\nHashiCorp Nomad, 314\\nKubernetes (K8s), 314\\noutliers, edge cases and, 232\\nIndex | 363', 'oversampling\\noverfitting, 110\\nSMOTE, 110\\nP\\npandas, 56\\nPapermill, 305\\nparallelization, model optimization and, 217\\nparameter values over time, 163\\nPareto optimization, 42\\nParquet, 54, 57\\nbinary files, 57\\npatterns\\nchanging, 8\\ncomplex, 4\\nPearl, Judea, 43\\nperformance metrics, 162\\nsystem performance, 163\\nperturbation, 114-116\\nperturbation method of semi-supervision, 99\\nperturbation tests, 181-182\\npositional embedding, 133-135\\nfixed, 135\\nprecision metrics, 107\\nprediction, 6, 39\\nasynchronous, 198\\nbatch prediction, 197-201\\nmoving to online prediction, 201-203\\n“mostly correct, ” user experience, 332-334\\non-demand prediction, 198\\nonline, 197-201\\nstreaming pipeline, 203-205\\nsynchronous, 198\\npredictions, monitoring, 252-253\\npredictive power of features, 140\\nprice optimization service, 73\\nproblem framing, 35-43\\nprocessing\\nanalytical, 67\\nbatch processing, 78-79\\nETL (extract, transform, load), 70-72\\nstream processing, 78-79\\ntransactional, 67\\nACID and, 68\\nproduction environment, 192\\nproduction, ML and, 12-21\\nproject objectives, 26-28\\nproject scoping, 34\\nprototyping, batch prediction and, 201\\npruning, 208-209\\npublic cloud versus private data center, 300-302\\nQ\\nquantization, 209-211\\nquery languages, 60\\nquota sampling, 83\\nR\\nrandom baselines, 180\\nreal-time transport\\ndataflow and, 74-77\\nstreaming data and, 78\\nreasonable scale, 294\\nrecall metrics, 107\\nrecommender systems, labels, 91\\nregression\\nclass imbalance and, 102\\ntasks, 39\\nregression models, 36\\nrelational databases, 60\\nrelational models, 59-62\\ndata normalization, 59\\nNoSQL, 63\\ndocument model, 63\\ngraph model, 65\\ntables, 59\\nreliability, 29\\nrepetition, 7\\nrepetitive jobs, scheduling, 311\\nrequest-driven data passing, 75\\nresampling, 109\\ndynamic sampling, 110\\noversampling\\noverfitting and, 110\\nSMOTE, 110\\ntwo-phase learning, 110\\nundersampling, 109\\nreservoir sampling, 86-87\\nresource management, 311\\nresource management layer, infrastructure, 295\\nREST (representational state transfer), 74\\nride management service, 73\\nROC (receiver operating characteristics) curve,\\n108\\nRogati, Monica, 44\\nROI (return on investment), maturity stage of\\nadoption, 28\\nrow deletion, 125\\n364 | Index', 'row-major format, 54-56\\nCSV (comma-separated values), 54\\nNumPy, 56\\nRPC (remote procedure call), 74\\nS\\nsampling, 82\\nimportance sampling, 87\\nnonprobability, 83\\nbiases, 83\\nreservoir sampling, 86-87\\nsimple random sampling, 84\\nstratified sampling, 84\\nweighted sampling, 85\\nscalability, 30-31\\nautoscaling, 30\\nscale, 7\\ndeployment myths, 196\\nschedulers, 313-314\\nBorg, 314\\nSlurm, 314\\nschemas, document model, 64\\nscoping a project, 34\\nself-training, 98\\nsemi-supervision, 98-99\\nsentiment analysis classifier, 120\\nserialization, 193\\nservices\\ndataflow and, 73-74\\ndriver management, 73\\nprice optimization, 73\\nride management, 73\\nSGD (stochastic gradient descent), 169\\nshadow deployment, 282\\nSHAP (SHapley Additive exPlanations, 142\\nsimple heuristic, offline evaluation, 180\\nsimple label-preserving transformations, 114\\nsimple random sampling, 84\\nSimpson’s paradox, 186\\nskewed distribution, feature scaling and, 127\\nslice-based evaluation, 185-188\\nslicing\\nerror analysis, 188\\nheuristics based, 188\\nslice finders, 188\\nSlurm, 314\\nsmartphones\\ndata sources and, 52\\nML (machine learning) and, 9\\nsmooth failing, user experience, 334\\nSMOTE (synthetic minority oversampling\\ntechnique), 110\\nSnorkel, 95\\nsnowball sampling, 83\\nsoft AutoML, 173-174\\nsoftware system failure\\ncrashes, 228\\ndependency, 227\\ndeployment, 227\\nhardware, 228\\nspam filtering, 41\\nsplitting\\ndata duplication, 139\\ndata leakage and, 138\\nSQL, 60\\nSQL databases, 61\\nSSD (solid state disk), 297\\nstacking, ensembles, 161\\nstakeholders, research projects, 13-15\\nstate-of-the-art models, 152\\nstateful training, 265-268\\nautomated, 277-278\\nstateless retraining, 265-268\\nmanual, 275\\nstochastic gradient descent (SGD), 169\\nstorage and compute layer, infrastructure, 295,\\n296, 297\\ncompute resources, 297\\nFLOPS (floating-point operations per sec‐\\nond), 298\\nprivate data centers, 300-302\\npublic cloud, 300-302\\nunits, 297\\nstorage engines, 67\\nstratified sampling, 84\\nstream processing, 78-79\\nstreaming data, real-time transport, 78\\nstreaming features, 199\\nstreaming pipeline, 203-205\\nstructured data, 66-67\\nSutton, Richard, 44\\nsynchronous prediction, 198\\nsynthetic minority oversampling technique\\n(SMOTE), 110\\nsystem performance metrics, 163\\nsystem-generated data, 50\\nIndex | 365', 'T\\ntags, model store, 323\\ntasks\\nclassification, 36\\nbinary, 37\\nhigh cardinality, 37\\nmulticlass, 37, 38\\nmultilabel, 38\\nlabels, 91\\nregression, 36, 39\\nteams\\ncross-functional collaboration, 335\\ndata scientists, 336-339\\nproduction management, 336\\ntelemetry, 260\\ntest in production, 263, 281\\nA/B testing, 283-284\\nbandits, 287-291\\ncanary release, 285\\ninterleaving experiments, 285-287\\nshadow deployment and, 282\\ntext data, 57\\ntext file size, 57\\ntheoretical constraints, failures and, 166\\nthird-party data, 52\\ntime-correlated data, data leakage and, 137\\ntraining\\nautomated retraining, 275-277\\ndistributed, 168\\ndata parallelism and, 168-170\\nmodel parallelism and, 170-172\\nstateful, 265-268\\nautomated, 277-278\\nstateless retraining, 265-268\\nmanual, 275\\ntraining data, 81\\nclass imbalance, 102\\nalgorithm-level methods, 110-113\\nchallenges, 103-105\\nevaluation metrics, 106-108\\nresampling, 109-110\\ndata augmentation, 113\\nperturbation, 114-116\\nsimple label-preserving transformations,\\n114\\ndata distributions, 229\\ndata leakage, 135\\nlabeling, 88\\nhand labels, 88-90\\nlack of labels, 94-102\\nnatural labels, 91-94\\nuser feedback, 93\\nn-grams, 121\\nnoisy samples, 116\\nsampling, 82\\nimportance sampling, 87\\nnonprobability, 83-84\\nreservoir sampling, 86-87\\nsimple random sampling, 84\\nstratified sampling, 84\\nweighted sampling, 85\\ntraining the model, iteration and, 32-33\\ndata engineering, 34\\nproject scoping, 34\\ntransactional processing, 67\\nACID and, 68\\ntransfer learning, 99-101\\ntwo-phase learning, 110\\nU\\nundersampling, 109\\nunseen data, 6\\nunstructured data, 66-67\\nupdates, deployment myths, 196\\nuse cases, 9-12\\nuser experience, 331\\nconsistency, 332\\npredictions, mostly correct, 332-334\\nsmooth failing, 334\\nuser feedback, 93\\nuser input data, 50\\nV\\nvCPU (virtual CPU), 299\\nvectorization, model optimization, 217\\nversioning, 163-165\\ncode versioning, 164\\nW\\nW ASM (WebAssembly), 223\\nweak supervision, 95-98\\nSnorkel, 95\\nweighted sampling, 85\\nword embeddings, 133\\nworkflow management, 314\\nAirflow, 315-316\\nArgo, 316-318\\n366 | Index', 'DAG (directed acyclic graph), 312\\nKubeflow, 318\\nMetaflow, 318\\nX\\nXGBoost, 142\\nZ\\nzero rule baselines, 180\\nzero-shot learning, 100\\nIndex | 367', 'About the Author\\nChip Huyen (https://huyenchip.com) is co-founder and CEO of Claypot AI, develop‐\\ning infrastructure for real-time machine learning. Previously, she was at NVIDIA,\\nSnorkel AI, and Netflix, where she helped some of the world’s largest organizations\\ndevelop and deploy machine learning systems.\\nWhen a student at Stanford, she created and taught the course TensorFlow for Deep\\nLearning Research. She is currently teaching CS 329S: Machine Learning Systems\\nDesign at Stanford. This book is based on the course’s lecture notes.\\nShe is also the author of four bestselling Vietnamese books, including the series Xách\\nba lô lên và Đi (Quảng Văn 2012, 2013). The series was among FAHASA ’s Top 10\\nReaders Choice Books in 2014.\\nChip’s expertise is in the intersection of software engineering and machine learning.\\nLinkedIn included her among the 10 Top Voices in Software Development in 2019,\\nand Top Voices in Data Science & AI in 2020.\\nColophon\\nThe animal on the cover of Designing Machine Learning Systems  is a red-legged\\npartridge (Alectoris rufa), also known as a French partridge.\\nBred for centuries as a gamebird, this economically important, largely nonmigratory\\nmember of the pheasant family is native to western continental Europe, though\\npopulations have been introduced elsewhere, including England, Ireland, and New\\nZealand.\\nRelatively small but stout bodied, the red-legged partridge boasts ornate coloration\\nand feather patterning, with light brown to gray plumage along its back, a light pink\\nbelly, a cream-colored throat, a brilliant red bill, and rufous or black barring on its\\nflanks.\\nFeeding primarily on seeds, leaves, grasses, and roots, but also on insects, red-legged\\npartridges breed each year in dry lowland areas, such as farmland, laying their eggs in\\nground nests. Though they continue to be bred in large numbers, these birds are now\\nconsidered near threatened due to steep population declines attributed, in part, to\\noverhunting and disappearance of habitat. Like all animals on O’Reilly covers, they’re\\nvitally important to our world.\\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\\nfrom The Riverside Natural History. The cover fonts are Gilroy Semibold and Guard‐\\nian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.', 'Learn from experts.  \\nBecome one yourself.\\nBooks | Live online courses   \\nInstant Answers | Virtual events \\nVideos | Interactive learning\\nGet started at oreilly.com. \\n©2022 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc. | 175']\n",
      "Total chunks: 535\n"
     ]
    }
   ],
   "source": [
    "# Perform the split\n",
    "# pass the text not the document\n",
    "# full book chunking\n",
    "\n",
    "all_chunks= []\n",
    "for doc in docs:\n",
    "    chunks = splitter.split_text(doc.page_content) \n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "# print(len(chunks))\n",
    "# print(chunks[0])\n",
    "print(all_chunks)\n",
    "print(f\"Total chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ef9c8",
   "metadata": {},
   "source": [
    "### Embed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a010622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b21522",
   "metadata": {},
   "source": [
    "### Store in Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import chroma\n",
    "\n",
    "\n",
    "# store chunks + embeddings in chroma db\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=all_chunks,       # the list of chunks you created\n",
    "    embedding=embeddings,   # your embedding model\n",
    "    persist_directory=\"chroma_db\",  # folder name for saving\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"} # l2,ip\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d24076",
   "metadata": {},
   "source": [
    "### .persist() = save your embeddings+chunks into Chroma on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb12dca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored chunks into Chroma DB\n"
     ]
    }
   ],
   "source": [
    "# persist to disk\n",
    "vectorstore.persist()\n",
    "print(\"✅ Stored chunks into Chroma DB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "51a80da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reload the db w/o building\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"rag_chroma\",\n",
    "#     embedding_function=embeddings\n",
    "# )\n",
    "# print(vectorstore._collection.count())  # number of stored chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "888f36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'creating production grade ML systems?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If that describes your background, it might be a steep learning curve for you to\n",
      "understand the challenges of deploying ML systems in the wild and navigate an\n",
      "overwhelming set of solutions to these challenges. ML in production is very different\n",
      "from ML in research. Table 1-1 shows five of the major differences.\n",
      "12 | Chapter 1: Overview of Machine Learning Systems\n",
      "If that describes your background, it might be a steep learning curve for you to\n",
      "understand the challenges of deploying ML systems in the wild and navigate an\n",
      "overwhelming set of solutions to these challenges. ML in production is very different\n",
      "from ML in research. Table 1-1 shows five of the major differences.\n",
      "12 | Chapter 1: Overview of Machine Learning Systems\n"
     ]
    }
   ],
   "source": [
    "# similarity_search = direct call->immediate results\n",
    "\n",
    "# directly queries chroma for top-k similar chunks\n",
    "# Simple, quick way to test retrieval outside a chain.\n",
    "\n",
    "# ✅ Use when you just want raw search results (testing/debugging).\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "for r in results:\n",
    "    # print(r.page_ content[:200])\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d75da2",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36177b83",
   "metadata": {},
   "source": [
    "| Metric                   | Description                                              | Pros                                                            | Cons / Notes                                                  | Best Use Case for LLMs                                 |\n",
    "| ------------------------ | -------------------------------------------------------- | --------------------------------------------------------------- | ------------------------------------------------------------- | ------------------------------------------------------ |\n",
    "| **Cosine Similarity**    | Measures the angle between vectors (ignores magnitude)   | Most common for semantic search; good for normalized embeddings | Sensitive to direction only, not magnitude                    | Semantic search for QA, RAG, embedding-based retrieval |\n",
    "| **Euclidean (L2)**       | Straight-line distance between vectors                   | Simple geometric intuition; widely supported                    | Sensitive to vector magnitude; may require normalized vectors | Small-medium datasets, normalized embeddings           |\n",
    "| **Dot Product**          | Measures magnitude-weighted similarity                   | Captures magnitude info; fast with matrix multiplication        | Can be skewed by vector norms                                 | Recommendation systems or embeddings with scale info   |\n",
    "| **Manhattan (L1)**       | Sum of absolute differences between vector components    | Robust to outliers in some cases                                | Rarely used in NLP embeddings                                 | Rarely used in LLM pipelines                           |\n",
    "| **Hybrid Search**        | Combines keyword (BM25) + vector search                  | Handles rare/technical terms better                             | More complex to implement                                     | Large corpora where vectors alone miss rare terms      |\n",
    "| **Approximate NN (ANN)** | Algorithms like HNSW, IVF, PQ for fast similarity search | Very fast for huge datasets                                     | Approximate, may miss exact nearest neighbors                 | Large-scale RAG / multi-million embeddings             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662e56f",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afaab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If that describes your background, it might be a steep learning curve for you to\n",
      "understand the challenges of deploying ML systems in the wild and navigate an\n",
      "overwhelming set of solutions to these challenges. ML in production is very different\n",
      "from ML in research. Table 1-1 shows five of the major differences.\n",
      "12 | Chapter 1: Overview of Machine Learning Systems\n",
      "If that describes your background, it might be a steep learning curve for you to\n",
      "understand the challenges of deploying ML systems in the wild and navigate an\n",
      "overwhelming set of solutions to these challenges. ML in production is very different\n",
      "from ML in research. Table 1-1 shows five of the major differences.\n",
      "12 | Chapter 1: Overview of Machine Learning Systems\n",
      "in the industry is still nascent, tooling around ML production is limited and best\n",
      "practices are not yet well developed or standardized. However, as toolings and best\n",
      "practices for ML production mature, there are reasons to believe that the proportion\n",
      "of software system failures will decrease and the proportion of ML-specific failures\n",
      "will increase.\n",
      "228 | Chapter 8: Data Distribution Shifts and Monitoring\n"
     ]
    }
   ],
   "source": [
    "# r1 becomes a retriever object= creating a search tool not performing the search\n",
    "r1 = vectorstore.as_retriever(search_kwargs={ \"k\": 3}) # metric = euclidean .dot\n",
    "# runs the retriever for your query\n",
    "result_cosine = r1.invoke(query) \n",
    "\n",
    "for r in result_cosine:\n",
    "    print(r.page_content) # metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "84b38c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model B 87% (234/270) 69% (55/80) 83% (289/350)\n",
      "a Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\n",
      "“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave\n",
      "Lithotripsy,” British Medical Journal  (Clinical Research Edition) 292, no. 6524 (March 1986): 879–82, https://oreil.ly/X8oWr.\n",
      "186 | Chapter 6: Model Development and Offline Evaluation\n",
      "Model B 87% (234/270) 69% (55/80) 83% (289/350)\n",
      "a Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\n",
      "“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave\n",
      "Lithotripsy,” British Medical Journal  (Clinical Research Edition) 292, no. 6524 (March 1986): 879–82, https://oreil.ly/X8oWr.\n",
      "186 | Chapter 6: Model Development and Offline Evaluation\n"
     ]
    }
   ],
   "source": [
    "q = 'which hospital is closest to harrison,nj'\n",
    "\n",
    "r2 = vectorstore.as_retriever(search_kwargs={\"k\":2})\n",
    "\n",
    "res =r2.invoke(q)\n",
    "\n",
    "for r in res:\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "508a1855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model B 87% (234/270) 69% (55/80) 83% (289/350)\n",
      "a Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\n",
      "“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave\n",
      "Lithotripsy,” British Medical Journal  (Clinical Research Edition) 292, no. 6524 (March 1986): 879–82, https://oreil.ly/X8oWr.\n",
      "186 | Chapter 6: Model Development and Offline Evaluation\n",
      "Model B 87% (234/270) 69% (55/80) 83% (289/350)\n",
      "a Numbers from Charig et al.’s kidney stone treatment study in 1986: C. R. Charig, D. R. Webb, S. R. Payne, and J. E. Wickham,\n",
      "“Comparison of Treatment of Renal Calculi by Open Surgery, Percutaneous Nephrolithotomy, and Extracorporeal Shockwave\n",
      "Lithotripsy,” British Medical Journal  (Clinical Research Edition) 292, no. 6524 (March 1986): 879–82, https://oreil.ly/X8oWr.\n",
      "186 | Chapter 6: Model Development and Offline Evaluation\n"
     ]
    }
   ],
   "source": [
    "res1 = vectorstore.similarity_search(q, k=2)\n",
    "for r in res1:\n",
    "    # print(r.page_ content[:200])\n",
    "    print(r.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd8c13",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1950c2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"Production best practices\" can refer to various fields such as manufacturing, software production, film production, or general product development. Could you please specify the context or industry you\\'re interested in? This way, I can provide the most relevant and detailed best practices for your needs.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 10, 'total_tokens': 64, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_3f58d112f7', 'id': 'chatcmpl-C66OjAXVwi7vucsKJUAw1j301493P', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0602654e-c4f2-4c58-81f2-982902df3c6b-0', usage_metadata={'input_tokens': 10, 'output_tokens': 54, 'total_tokens': 64, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4.1-mini')\n",
    "\n",
    "llm.invoke('production best practices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d797b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. creating the retriever from your chroma vs\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dc89f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'production best practices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e126e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "in the industry is still nascent, tooling around ML production is limited and best\n",
      "practices are not yet well developed or standardized. However, as toolings and best\n",
      "practices for ML production mature, there are reasons to believe that the proportion\n",
      "of software system failures will decrease and the proportion of ML-specific failures\n",
      "will increase.\n",
      "228 | Chapter 8: Data Distribution Shifts and Monitoring\n",
      "--- Document 2 ---\n",
      "in the industry is still nascent, tooling around ML production is limited and best\n",
      "practices are not yet well developed or standardized. However, as toolings and best\n",
      "practices for ML production mature, there are reasons to believe that the proportion\n",
      "of software system failures will decrease and the proportion of ML-specific failures\n",
      "will increase.\n",
      "228 | Chapter 8: Data Distribution Shifts and Monitoring\n"
     ]
    }
   ],
   "source": [
    "# get the raw retrieved documents\n",
    "retrieved_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "# print content = exactly what is sent to context\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141a1260",
   "metadata": {},
   "source": [
    "at retrieval time retriever cannot know that the answer is not in the documents because the it doesn't know the meaning of the question, it just measures the vector similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "57175647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 1. definin gprompt style\n",
    "# prompt arranges retrieved info into context\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant.\n",
    "    Answer ONLY from the provided document context.\n",
    "    If the context is insufficient, just say you don't know or\n",
    "    answer not found in the database\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81e36df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. defining a parser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c0157dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the pipeline\n",
    "\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "# Runnablepassthrough() passes the question through the pipeline\n",
    "\n",
    "chain = ({\n",
    "    \"context\": retriever,\n",
    "    \"question\": RunnablePassthrough()} | prompt | llm | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061593a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer not found in the database\n"
     ]
    }
   ],
   "source": [
    "# run a sample query\n",
    "response = chain.invoke('which hospital is closest to harrison,nj')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "47e3c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Production best practices for ML are not yet well developed or standardized, '\n",
      " 'as tooling around ML production is still limited and the industry is '\n",
      " 'nascent.')\n"
     ]
    }
   ],
   "source": [
    "# run a query\n",
    "question = chain.invoke('production best practices')\n",
    "pprint(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ef12d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "Chip Huyen\n",
      "Designing Machine\n",
      "Learning Systems\n",
      "An Iterative Process for\n",
      "Production-Ready Applications\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing\n",
      "\n",
      "--- Document 2 ---\n",
      "Chip Huyen\n",
      "Designing Machine\n",
      "Learning Systems\n",
      "An Iterative Process for\n",
      "Production-Ready Applications\n",
      "Boston Farnham Sebastopol TokyoBeijing Boston Farnham Sebastopol TokyoBeijing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = retriever.get_relevant_documents(question)\n",
    "a\n",
    "for i, v in enumerate(retrieved_docs):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(v.page_content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3e657",
   "metadata": {},
   "source": [
    "\"\"\"Pros:\n",
    "\n",
    "Very simple to set up.\n",
    "\n",
    "Works with all common retrievers and embeddings.\n",
    "\n",
    "Well-documented, stable API.\n",
    "\n",
    "Cons:\n",
    "\n",
    "Less flexible if you want modular pipelines (like adding pre-processing, multiple retrievers, or post-processing parsers).\n",
    "\n",
    "Not composable in the new LCEL style.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca30d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pros:\\n\\nVery simple to set up.\\n\\nWorks with all common retrievers and embeddings.\\n\\nWell-documented, stable API.\\n\\nCons:\\n\\nLess flexible if you want modular pipelines (like adding pre-processing, multiple retrievers, or post-processing parsers).\\n\\nNot composable in the new LCEL style.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from langchain.chains import RetrievalQA # stitches it together\n",
    "# # classic chain\n",
    "# #RetrievalQA handles the glue between retriever → prompt → LLM\n",
    "# qa_chain =RetrievalQA.from_chain_type(\n",
    "#     llm=llm, # answers the question\n",
    "#     retriever=retriever, # fetches the top-k results\n",
    "#     chain_type='stuff', #chain combines them all into a single string for the prompt\n",
    "#     chain_type_kwargs = {'prompt': prompt}\n",
    "# )\n",
    "# classic_ra = qa_chain.invoke(question)\n",
    "# classic_ra\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
